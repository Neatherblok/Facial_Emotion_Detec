{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574f39d9-dab6-45c3-bfd3-78e3dfebfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# Specify where to find the data preparation class\n",
    "sys.path.append('../../Data_Preparation')\n",
    "from Preparation import CustomDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008ac5be-e4af-4ee9-bb44-2d8a137cf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3 training data (ImageNet) properties\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "DIMENSIONS = 3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LR = 0.1\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LR_STEP_SIZE = 30\n",
    "LR_GAMMA = 0.1\n",
    "\n",
    "EPOCHS = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7889fed-b144-4a60-852b-ef19a3d1fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders\n",
      "Train Data Loader:\n",
      "Batch Index: 0\n",
      "Inputs Shape: torch.Size([16, 3, 299, 299])\n",
      "Labels Shape: torch.Size([16])\n",
      "Labels: tensor([4, 4, 3, 2, 6])\n",
      "Batch Index: 1\n",
      "Inputs Shape: torch.Size([16, 3, 299, 299])\n",
      "Labels Shape: torch.Size([16])\n",
      "Labels: tensor([3, 0, 3, 0, 3])\n",
      "Batch Index: 2\n",
      "Inputs Shape: torch.Size([16, 3, 299, 299])\n",
      "Labels Shape: torch.Size([16])\n",
      "Labels: tensor([5, 6, 4, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating data loaders\")\n",
    "# Instantiate the CustomDataLoader class for training\n",
    "train_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=BATCH_SIZE, dataset_type=\"train\", mean=MEAN, std=STD, dimensions=3).data_loader\n",
    "test_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=BATCH_SIZE, dataset_type=\"test\", mean=MEAN, std=STD, dimensions=3).data_loader\n",
    "\n",
    "# Confirm correct data load\n",
    "print(\"Train Data Loader:\")\n",
    "for batch_idx, (inputs, labels) in enumerate(train_data_loader):\n",
    "    print(\"Batch Index:\", batch_idx)\n",
    "    print(\"Inputs Shape:\", inputs.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    # Print the first few labels in the batch\n",
    "    print(\"Labels:\", labels[:5])\n",
    "    # Break after printing a few batches\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab3e7ce-6b24-499b-b5b4-4708ffcce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the InceptionV3 model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer with a new one that outputs 7 classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 7)  # Output layer with 7 classes\n",
    "model.aux_logits = False\n",
    "model.AuxLogits = None\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "LR_SCHEDULER = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP_SIZE, gamma=LR_GAMMA)\n",
    "\n",
    "#torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3c195-5ac7-4b5c-ac35-5ff58399b50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start training\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/90], lter [1/1752] Loss: 24.5209\n",
      "Epoch [1/90], lter [11/1752] Loss: 20.5382\n",
      "Epoch [1/90], lter [21/1752] Loss: 29.8732\n",
      "Epoch [1/90], lter [31/1752] Loss: 27.7454\n",
      "Epoch [1/90], lter [41/1752] Loss: 26.2988\n",
      "Epoch [1/90], lter [51/1752] Loss: 20.3020\n",
      "Epoch [1/90], lter [61/1752] Loss: 13.5113\n",
      "Epoch [1/90], lter [71/1752] Loss: 34.0717\n",
      "Epoch [1/90], lter [81/1752] Loss: 26.3085\n",
      "Epoch [1/90], lter [91/1752] Loss: 35.8470\n",
      "Epoch [1/90], lter [101/1752] Loss: 19.8037\n",
      "Epoch [1/90], lter [111/1752] Loss: 26.9462\n",
      "Epoch [1/90], lter [121/1752] Loss: 20.1170\n",
      "Epoch [1/90], lter [131/1752] Loss: 18.9827\n",
      "Epoch [1/90], lter [141/1752] Loss: 26.6137\n",
      "Epoch [1/90], lter [151/1752] Loss: 29.6315\n",
      "Epoch [1/90], lter [161/1752] Loss: 20.1835\n",
      "Epoch [1/90], lter [171/1752] Loss: 27.0698\n",
      "Epoch [1/90], lter [181/1752] Loss: 25.5315\n",
      "Epoch [1/90], lter [191/1752] Loss: 28.0733\n",
      "Epoch [1/90], lter [201/1752] Loss: 23.2970\n",
      "Epoch [1/90], lter [211/1752] Loss: 30.7603\n",
      "Epoch [1/90], lter [221/1752] Loss: 32.9085\n",
      "Epoch [1/90], lter [231/1752] Loss: 21.0433\n",
      "Epoch [1/90], lter [241/1752] Loss: 24.9263\n",
      "Epoch [1/90], lter [251/1752] Loss: 48.9817\n",
      "Epoch [1/90], lter [261/1752] Loss: 26.2382\n",
      "Epoch [1/90], lter [271/1752] Loss: 28.2986\n",
      "Epoch [1/90], lter [281/1752] Loss: 23.4412\n",
      "Epoch [1/90], lter [291/1752] Loss: 17.7998\n",
      "Epoch [1/90], lter [301/1752] Loss: 30.5551\n",
      "Epoch [1/90], lter [311/1752] Loss: 21.0738\n",
      "Epoch [1/90], lter [321/1752] Loss: 20.1925\n",
      "Epoch [1/90], lter [331/1752] Loss: 20.7225\n",
      "Epoch [1/90], lter [341/1752] Loss: 30.7498\n",
      "Epoch [1/90], lter [351/1752] Loss: 34.0035\n",
      "Epoch [1/90], lter [361/1752] Loss: 15.0497\n",
      "Epoch [1/90], lter [371/1752] Loss: 13.9836\n",
      "Epoch [1/90], lter [381/1752] Loss: 25.6668\n",
      "Epoch [1/90], lter [391/1752] Loss: 22.3204\n",
      "Epoch [1/90], lter [401/1752] Loss: 19.4927\n",
      "Epoch [1/90], lter [411/1752] Loss: 28.6334\n",
      "Epoch [1/90], lter [421/1752] Loss: 22.8914\n",
      "Epoch [1/90], lter [431/1752] Loss: 25.7003\n",
      "Epoch [1/90], lter [441/1752] Loss: 27.1615\n",
      "Epoch [1/90], lter [451/1752] Loss: 29.0525\n",
      "Epoch [1/90], lter [461/1752] Loss: 17.2620\n",
      "Epoch [1/90], lter [471/1752] Loss: 23.7238\n",
      "Epoch [1/90], lter [481/1752] Loss: 16.5466\n",
      "Epoch [1/90], lter [491/1752] Loss: 29.5550\n",
      "Epoch [1/90], lter [501/1752] Loss: 23.9559\n",
      "Epoch [1/90], lter [511/1752] Loss: 16.6366\n",
      "Epoch [1/90], lter [521/1752] Loss: 26.2307\n",
      "Epoch [1/90], lter [531/1752] Loss: 27.9519\n",
      "Epoch [1/90], lter [541/1752] Loss: 29.4701\n",
      "Epoch [1/90], lter [551/1752] Loss: 24.2476\n",
      "Epoch [1/90], lter [561/1752] Loss: 20.9035\n",
      "Epoch [1/90], lter [571/1752] Loss: 26.3874\n",
      "Epoch [1/90], lter [581/1752] Loss: 21.7391\n",
      "Epoch [1/90], lter [591/1752] Loss: 22.2572\n",
      "Epoch [1/90], lter [601/1752] Loss: 23.5426\n",
      "Epoch [1/90], lter [611/1752] Loss: 11.7435\n",
      "Epoch [1/90], lter [621/1752] Loss: 21.3155\n",
      "Epoch [1/90], lter [631/1752] Loss: 22.8328\n",
      "Epoch [1/90], lter [641/1752] Loss: 23.7605\n",
      "Epoch [1/90], lter [651/1752] Loss: 37.0626\n",
      "Epoch [1/90], lter [661/1752] Loss: 20.7264\n",
      "Epoch [1/90], lter [671/1752] Loss: 14.2023\n",
      "Epoch [1/90], lter [681/1752] Loss: 14.0633\n",
      "Epoch [1/90], lter [691/1752] Loss: 7.2645\n",
      "Epoch [1/90], lter [701/1752] Loss: 22.4663\n",
      "Epoch [1/90], lter [711/1752] Loss: 25.4034\n",
      "Epoch [1/90], lter [721/1752] Loss: 19.7624\n",
      "Epoch [1/90], lter [731/1752] Loss: 23.2577\n",
      "Epoch [1/90], lter [741/1752] Loss: 19.9663\n",
      "Epoch [1/90], lter [751/1752] Loss: 21.5694\n",
      "Epoch [1/90], lter [761/1752] Loss: 25.0483\n",
      "Epoch [1/90], lter [771/1752] Loss: 26.2850\n",
      "Epoch [1/90], lter [781/1752] Loss: 20.5027\n",
      "Epoch [1/90], lter [791/1752] Loss: 30.7782\n",
      "Epoch [1/90], lter [801/1752] Loss: 18.9944\n",
      "Epoch [1/90], lter [811/1752] Loss: 27.8579\n",
      "Epoch [1/90], lter [821/1752] Loss: 29.2619\n",
      "Epoch [1/90], lter [831/1752] Loss: 21.5964\n",
      "Epoch [1/90], lter [841/1752] Loss: 11.4913\n",
      "Epoch [1/90], lter [851/1752] Loss: 24.6951\n",
      "Epoch [1/90], lter [861/1752] Loss: 27.7347\n",
      "Epoch [1/90], lter [871/1752] Loss: 11.7788\n",
      "Epoch [1/90], lter [881/1752] Loss: 10.5657\n",
      "Epoch [1/90], lter [891/1752] Loss: 15.6534\n",
      "Epoch [1/90], lter [901/1752] Loss: 36.2841\n",
      "Epoch [1/90], lter [911/1752] Loss: 27.3917\n",
      "Epoch [1/90], lter [921/1752] Loss: 19.0125\n",
      "Epoch [1/90], lter [931/1752] Loss: 21.4181\n",
      "Epoch [1/90], lter [941/1752] Loss: 20.4481\n",
      "Epoch [1/90], lter [951/1752] Loss: 41.3827\n",
      "Epoch [1/90], lter [961/1752] Loss: 16.2140\n",
      "Epoch [1/90], lter [971/1752] Loss: 38.5475\n",
      "Epoch [1/90], lter [981/1752] Loss: 26.3163\n",
      "Epoch [1/90], lter [991/1752] Loss: 14.5055\n",
      "Epoch [1/90], lter [1001/1752] Loss: 23.8384\n",
      "Epoch [1/90], lter [1011/1752] Loss: 30.9288\n",
      "Epoch [1/90], lter [1021/1752] Loss: 27.2903\n",
      "Epoch [1/90], lter [1031/1752] Loss: 27.2195\n",
      "Epoch [1/90], lter [1041/1752] Loss: 20.5979\n",
      "Epoch [1/90], lter [1051/1752] Loss: 14.2595\n",
      "Epoch [1/90], lter [1061/1752] Loss: 28.5175\n",
      "Epoch [1/90], lter [1071/1752] Loss: 26.4446\n",
      "Epoch [1/90], lter [1081/1752] Loss: 35.0070\n",
      "Epoch [1/90], lter [1091/1752] Loss: 28.5745\n",
      "Epoch [1/90], lter [1101/1752] Loss: 19.1420\n",
      "Epoch [1/90], lter [1111/1752] Loss: 28.0893\n",
      "Epoch [1/90], lter [1121/1752] Loss: 25.0907\n",
      "Epoch [1/90], lter [1131/1752] Loss: 35.8531\n",
      "Epoch [1/90], lter [1141/1752] Loss: 33.7199\n",
      "Epoch [1/90], lter [1151/1752] Loss: 24.8384\n",
      "Epoch [1/90], lter [1161/1752] Loss: 22.6775\n",
      "Epoch [1/90], lter [1171/1752] Loss: 21.7936\n",
      "Epoch [1/90], lter [1181/1752] Loss: 33.1847\n",
      "Epoch [1/90], lter [1191/1752] Loss: 17.9563\n",
      "Epoch [1/90], lter [1201/1752] Loss: 31.0894\n",
      "Epoch [1/90], lter [1211/1752] Loss: 17.7454\n",
      "Epoch [1/90], lter [1221/1752] Loss: 14.9254\n",
      "Epoch [1/90], lter [1231/1752] Loss: 26.4195\n",
      "Epoch [1/90], lter [1241/1752] Loss: 27.3273\n",
      "Epoch [1/90], lter [1251/1752] Loss: 26.4063\n",
      "Epoch [1/90], lter [1261/1752] Loss: 24.6906\n",
      "Epoch [1/90], lter [1271/1752] Loss: 20.9734\n",
      "Epoch [1/90], lter [1281/1752] Loss: 14.1493\n",
      "Epoch [1/90], lter [1291/1752] Loss: 27.6180\n",
      "Epoch [1/90], lter [1301/1752] Loss: 15.0550\n",
      "Epoch [1/90], lter [1311/1752] Loss: 29.3392\n",
      "Epoch [1/90], lter [1321/1752] Loss: 24.1690\n",
      "Epoch [1/90], lter [1331/1752] Loss: 32.8293\n",
      "Epoch [1/90], lter [1341/1752] Loss: 29.3406\n",
      "Epoch [1/90], lter [1351/1752] Loss: 21.4699\n",
      "Epoch [1/90], lter [1361/1752] Loss: 22.8572\n",
      "Epoch [1/90], lter [1371/1752] Loss: 19.6102\n",
      "Epoch [1/90], lter [1381/1752] Loss: 14.6702\n",
      "Epoch [1/90], lter [1391/1752] Loss: 17.8064\n",
      "Epoch [1/90], lter [1401/1752] Loss: 31.5864\n",
      "Epoch [1/90], lter [1411/1752] Loss: 17.7449\n",
      "Epoch [1/90], lter [1421/1752] Loss: 25.2862\n",
      "Epoch [1/90], lter [1431/1752] Loss: 19.0988\n",
      "Epoch [1/90], lter [1441/1752] Loss: 18.0487\n",
      "Epoch [1/90], lter [1451/1752] Loss: 20.2531\n",
      "Epoch [1/90], lter [1461/1752] Loss: 25.9018\n",
      "Epoch [1/90], lter [1471/1752] Loss: 33.9305\n",
      "Epoch [1/90], lter [1481/1752] Loss: 20.4898\n",
      "Epoch [1/90], lter [1491/1752] Loss: 18.5774\n",
      "Epoch [1/90], lter [1501/1752] Loss: 34.1499\n",
      "Epoch [1/90], lter [1511/1752] Loss: 34.1239\n",
      "Epoch [1/90], lter [1521/1752] Loss: 28.5306\n",
      "Epoch [1/90], lter [1531/1752] Loss: 25.7692\n",
      "Epoch [1/90], lter [1541/1752] Loss: 16.3409\n",
      "Epoch [1/90], lter [1551/1752] Loss: 19.0006\n",
      "Epoch [1/90], lter [1561/1752] Loss: 15.2311\n",
      "Epoch [1/90], lter [1571/1752] Loss: 17.3902\n",
      "Epoch [1/90], lter [1581/1752] Loss: 20.5210\n",
      "Epoch [1/90], lter [1591/1752] Loss: 23.2565\n",
      "Epoch [1/90], lter [1601/1752] Loss: 28.0586\n",
      "Epoch [1/90], lter [1611/1752] Loss: 14.6927\n",
      "Epoch [1/90], lter [1621/1752] Loss: 20.7668\n",
      "Epoch [1/90], lter [1631/1752] Loss: 29.2855\n",
      "Epoch [1/90], lter [1641/1752] Loss: 32.1386\n",
      "Epoch [1/90], lter [1651/1752] Loss: 18.5513\n",
      "Epoch [1/90], lter [1661/1752] Loss: 24.7161\n",
      "Epoch [1/90], lter [1671/1752] Loss: 23.9926\n",
      "Epoch [1/90], lter [1681/1752] Loss: 30.3413\n",
      "Epoch [1/90], lter [1691/1752] Loss: 21.1056\n",
      "Epoch [1/90], lter [1701/1752] Loss: 24.4780\n",
      "Epoch [1/90], lter [1711/1752] Loss: 20.2645\n",
      "Epoch [1/90], lter [1721/1752] Loss: 22.6070\n",
      "Epoch [1/90], lter [1731/1752] Loss: 26.9177\n",
      "Epoch [1/90], lter [1741/1752] Loss: 19.0371\n",
      "Epoch [1/90], lter [1751/1752] Loss: 30.3881\n",
      "Epoch:  0 | train loss : 24.4372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/90 [53:35<79:29:07, 3215.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | test loss : 19.7356\n",
      "Epoch [2/90], lter [1/1752] Loss: 37.5264\n",
      "Epoch [2/90], lter [11/1752] Loss: 15.1287\n",
      "Epoch [2/90], lter [21/1752] Loss: 36.8454\n",
      "Epoch [2/90], lter [31/1752] Loss: 12.7344\n",
      "Epoch [2/90], lter [41/1752] Loss: 26.2758\n",
      "Epoch [2/90], lter [51/1752] Loss: 26.8353\n",
      "Epoch [2/90], lter [61/1752] Loss: 38.3289\n",
      "Epoch [2/90], lter [71/1752] Loss: 29.4488\n",
      "Epoch [2/90], lter [81/1752] Loss: 24.5433\n",
      "Epoch [2/90], lter [91/1752] Loss: 27.0105\n",
      "Epoch [2/90], lter [101/1752] Loss: 17.0444\n",
      "Epoch [2/90], lter [111/1752] Loss: 35.8786\n",
      "Epoch [2/90], lter [121/1752] Loss: 28.0786\n",
      "Epoch [2/90], lter [131/1752] Loss: 21.9039\n",
      "Epoch [2/90], lter [141/1752] Loss: 20.4429\n",
      "Epoch [2/90], lter [151/1752] Loss: 33.6597\n",
      "Epoch [2/90], lter [161/1752] Loss: 26.4655\n",
      "Epoch [2/90], lter [171/1752] Loss: 36.5336\n",
      "Epoch [2/90], lter [181/1752] Loss: 29.4748\n",
      "Epoch [2/90], lter [191/1752] Loss: 24.1091\n",
      "Epoch [2/90], lter [201/1752] Loss: 28.1159\n",
      "Epoch [2/90], lter [211/1752] Loss: 24.0201\n",
      "Epoch [2/90], lter [221/1752] Loss: 12.0824\n",
      "Epoch [2/90], lter [231/1752] Loss: 32.3140\n",
      "Epoch [2/90], lter [241/1752] Loss: 24.7964\n",
      "Epoch [2/90], lter [251/1752] Loss: 18.3894\n",
      "Epoch [2/90], lter [261/1752] Loss: 28.4668\n",
      "Epoch [2/90], lter [271/1752] Loss: 29.1225\n",
      "Epoch [2/90], lter [281/1752] Loss: 29.6510\n",
      "Epoch [2/90], lter [291/1752] Loss: 16.6886\n",
      "Epoch [2/90], lter [301/1752] Loss: 21.0093\n",
      "Epoch [2/90], lter [311/1752] Loss: 13.8829\n",
      "Epoch [2/90], lter [321/1752] Loss: 26.2147\n",
      "Epoch [2/90], lter [331/1752] Loss: 21.8610\n",
      "Epoch [2/90], lter [341/1752] Loss: 24.3913\n",
      "Epoch [2/90], lter [351/1752] Loss: 25.7244\n",
      "Epoch [2/90], lter [361/1752] Loss: 28.0364\n",
      "Epoch [2/90], lter [371/1752] Loss: 32.2818\n",
      "Epoch [2/90], lter [381/1752] Loss: 19.2031\n",
      "Epoch [2/90], lter [391/1752] Loss: 12.9869\n",
      "Epoch [2/90], lter [401/1752] Loss: 25.0137\n",
      "Epoch [2/90], lter [411/1752] Loss: 24.4332\n",
      "Epoch [2/90], lter [421/1752] Loss: 31.2747\n",
      "Epoch [2/90], lter [431/1752] Loss: 22.6457\n",
      "Epoch [2/90], lter [441/1752] Loss: 14.5225\n",
      "Epoch [2/90], lter [451/1752] Loss: 20.6147\n",
      "Epoch [2/90], lter [461/1752] Loss: 18.6851\n",
      "Epoch [2/90], lter [471/1752] Loss: 26.0817\n",
      "Epoch [2/90], lter [481/1752] Loss: 29.7919\n",
      "Epoch [2/90], lter [491/1752] Loss: 35.8441\n",
      "Epoch [2/90], lter [501/1752] Loss: 28.9902\n",
      "Epoch [2/90], lter [511/1752] Loss: 17.6954\n",
      "Epoch [2/90], lter [521/1752] Loss: 20.1437\n",
      "Epoch [2/90], lter [531/1752] Loss: 27.3049\n",
      "Epoch [2/90], lter [541/1752] Loss: 23.4246\n",
      "Epoch [2/90], lter [551/1752] Loss: 26.3561\n",
      "Epoch [2/90], lter [561/1752] Loss: 30.1052\n",
      "Epoch [2/90], lter [571/1752] Loss: 32.9785\n",
      "Epoch [2/90], lter [581/1752] Loss: 16.7479\n",
      "Epoch [2/90], lter [591/1752] Loss: 18.7160\n",
      "Epoch [2/90], lter [601/1752] Loss: 33.8936\n",
      "Epoch [2/90], lter [611/1752] Loss: 28.3818\n",
      "Epoch [2/90], lter [621/1752] Loss: 42.7840\n",
      "Epoch [2/90], lter [631/1752] Loss: 27.1361\n",
      "Epoch [2/90], lter [641/1752] Loss: 26.7325\n",
      "Epoch [2/90], lter [651/1752] Loss: 32.5457\n",
      "Epoch [2/90], lter [661/1752] Loss: 25.7688\n",
      "Epoch [2/90], lter [671/1752] Loss: 16.5029\n",
      "Epoch [2/90], lter [681/1752] Loss: 19.7423\n",
      "Epoch [2/90], lter [691/1752] Loss: 33.3716\n",
      "Epoch [2/90], lter [701/1752] Loss: 22.8005\n",
      "Epoch [2/90], lter [711/1752] Loss: 26.4898\n",
      "Epoch [2/90], lter [721/1752] Loss: 20.7638\n",
      "Epoch [2/90], lter [731/1752] Loss: 28.8819\n",
      "Epoch [2/90], lter [741/1752] Loss: 18.2936\n",
      "Epoch [2/90], lter [751/1752] Loss: 32.5400\n",
      "Epoch [2/90], lter [761/1752] Loss: 34.1118\n",
      "Epoch [2/90], lter [771/1752] Loss: 21.3173\n",
      "Epoch [2/90], lter [781/1752] Loss: 41.5701\n",
      "Epoch [2/90], lter [791/1752] Loss: 22.3405\n",
      "Epoch [2/90], lter [801/1752] Loss: 26.8118\n",
      "Epoch [2/90], lter [811/1752] Loss: 29.7604\n",
      "Epoch [2/90], lter [821/1752] Loss: 28.4670\n",
      "Epoch [2/90], lter [831/1752] Loss: 27.0546\n",
      "Epoch [2/90], lter [841/1752] Loss: 34.6034\n",
      "Epoch [2/90], lter [851/1752] Loss: 24.8866\n",
      "Epoch [2/90], lter [861/1752] Loss: 29.1801\n",
      "Epoch [2/90], lter [871/1752] Loss: 27.4372\n",
      "Epoch [2/90], lter [881/1752] Loss: 18.0912\n",
      "Epoch [2/90], lter [891/1752] Loss: 28.1065\n",
      "Epoch [2/90], lter [901/1752] Loss: 17.4587\n",
      "Epoch [2/90], lter [911/1752] Loss: 17.1769\n",
      "Epoch [2/90], lter [921/1752] Loss: 14.1505\n",
      "Epoch [2/90], lter [931/1752] Loss: 24.2431\n",
      "Epoch [2/90], lter [941/1752] Loss: 19.5123\n",
      "Epoch [2/90], lter [951/1752] Loss: 21.9778\n",
      "Epoch [2/90], lter [961/1752] Loss: 25.5417\n",
      "Epoch [2/90], lter [971/1752] Loss: 31.1163\n",
      "Epoch [2/90], lter [981/1752] Loss: 21.9650\n",
      "Epoch [2/90], lter [991/1752] Loss: 35.6122\n",
      "Epoch [2/90], lter [1001/1752] Loss: 30.1547\n",
      "Epoch [2/90], lter [1011/1752] Loss: 26.4799\n",
      "Epoch [2/90], lter [1021/1752] Loss: 20.6613\n",
      "Epoch [2/90], lter [1031/1752] Loss: 28.6320\n",
      "Epoch [2/90], lter [1041/1752] Loss: 25.1952\n",
      "Epoch [2/90], lter [1051/1752] Loss: 23.8422\n",
      "Epoch [2/90], lter [1061/1752] Loss: 23.0518\n",
      "Epoch [2/90], lter [1071/1752] Loss: 19.7944\n",
      "Epoch [2/90], lter [1081/1752] Loss: 15.9627\n",
      "Epoch [2/90], lter [1091/1752] Loss: 16.7673\n",
      "Epoch [2/90], lter [1101/1752] Loss: 31.5748\n",
      "Epoch [2/90], lter [1111/1752] Loss: 26.1060\n",
      "Epoch [2/90], lter [1121/1752] Loss: 24.3211\n",
      "Epoch [2/90], lter [1131/1752] Loss: 16.2007\n",
      "Epoch [2/90], lter [1141/1752] Loss: 23.7633\n",
      "Epoch [2/90], lter [1151/1752] Loss: 24.2321\n",
      "Epoch [2/90], lter [1161/1752] Loss: 33.3519\n",
      "Epoch [2/90], lter [1171/1752] Loss: 18.5115\n",
      "Epoch [2/90], lter [1181/1752] Loss: 29.5661\n",
      "Epoch [2/90], lter [1191/1752] Loss: 30.7560\n",
      "Epoch [2/90], lter [1201/1752] Loss: 32.6004\n",
      "Epoch [2/90], lter [1211/1752] Loss: 32.0713\n",
      "Epoch [2/90], lter [1221/1752] Loss: 30.2948\n",
      "Epoch [2/90], lter [1231/1752] Loss: 17.7974\n",
      "Epoch [2/90], lter [1241/1752] Loss: 29.7496\n",
      "Epoch [2/90], lter [1251/1752] Loss: 24.9575\n",
      "Epoch [2/90], lter [1261/1752] Loss: 16.5424\n",
      "Epoch [2/90], lter [1271/1752] Loss: 40.5390\n",
      "Epoch [2/90], lter [1281/1752] Loss: 20.0180\n",
      "Epoch [2/90], lter [1291/1752] Loss: 17.4856\n",
      "Epoch [2/90], lter [1301/1752] Loss: 18.4760\n",
      "Epoch [2/90], lter [1311/1752] Loss: 13.9217\n",
      "Epoch [2/90], lter [1321/1752] Loss: 20.5869\n",
      "Epoch [2/90], lter [1331/1752] Loss: 22.6420\n",
      "Epoch [2/90], lter [1341/1752] Loss: 21.9493\n",
      "Epoch [2/90], lter [1351/1752] Loss: 22.1460\n",
      "Epoch [2/90], lter [1361/1752] Loss: 29.0395\n",
      "Epoch [2/90], lter [1371/1752] Loss: 21.2331\n",
      "Epoch [2/90], lter [1381/1752] Loss: 24.6846\n",
      "Epoch [2/90], lter [1391/1752] Loss: 24.3630\n",
      "Epoch [2/90], lter [1401/1752] Loss: 14.5391\n",
      "Epoch [2/90], lter [1411/1752] Loss: 20.1240\n",
      "Epoch [2/90], lter [1421/1752] Loss: 19.9543\n",
      "Epoch [2/90], lter [1431/1752] Loss: 22.0060\n",
      "Epoch [2/90], lter [1441/1752] Loss: 18.9003\n",
      "Epoch [2/90], lter [1451/1752] Loss: 20.3456\n",
      "Epoch [2/90], lter [1461/1752] Loss: 23.9995\n",
      "Epoch [2/90], lter [1471/1752] Loss: 22.3253\n",
      "Epoch [2/90], lter [1481/1752] Loss: 16.2701\n",
      "Epoch [2/90], lter [1491/1752] Loss: 15.1003\n",
      "Epoch [2/90], lter [1501/1752] Loss: 16.2013\n",
      "Epoch [2/90], lter [1511/1752] Loss: 21.2210\n",
      "Epoch [2/90], lter [1521/1752] Loss: 20.6765\n",
      "Epoch [2/90], lter [1531/1752] Loss: 14.5383\n",
      "Epoch [2/90], lter [1541/1752] Loss: 15.1624\n",
      "Epoch [2/90], lter [1551/1752] Loss: 26.0640\n",
      "Epoch [2/90], lter [1561/1752] Loss: 34.9334\n",
      "Epoch [2/90], lter [1571/1752] Loss: 31.1376\n",
      "Epoch [2/90], lter [1581/1752] Loss: 25.1389\n",
      "Epoch [2/90], lter [1591/1752] Loss: 21.5128\n",
      "Epoch [2/90], lter [1601/1752] Loss: 14.6298\n",
      "Epoch [2/90], lter [1611/1752] Loss: 30.4603\n",
      "Epoch [2/90], lter [1621/1752] Loss: 25.5705\n",
      "Epoch [2/90], lter [1631/1752] Loss: 14.9269\n",
      "Epoch [2/90], lter [1641/1752] Loss: 30.2809\n",
      "Epoch [2/90], lter [1651/1752] Loss: 14.0104\n",
      "Epoch [2/90], lter [1661/1752] Loss: 26.9356\n",
      "Epoch [2/90], lter [1671/1752] Loss: 28.7938\n",
      "Epoch [2/90], lter [1681/1752] Loss: 26.9836\n",
      "Epoch [2/90], lter [1691/1752] Loss: 29.1069\n",
      "Epoch [2/90], lter [1701/1752] Loss: 26.6736\n",
      "Epoch [2/90], lter [1711/1752] Loss: 29.8623\n",
      "Epoch [2/90], lter [1721/1752] Loss: 24.0151\n",
      "Epoch [2/90], lter [1731/1752] Loss: 30.7654\n",
      "Epoch [2/90], lter [1741/1752] Loss: 29.9479\n",
      "Epoch [2/90], lter [1751/1752] Loss: 15.9625\n",
      "Epoch:  1 | train loss : 24.2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/90 [1:47:24<78:48:13, 3223.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | test loss : 21.0203\n",
      "Epoch [3/90], lter [1/1752] Loss: 26.4743\n",
      "Epoch [3/90], lter [11/1752] Loss: 22.4584\n",
      "Epoch [3/90], lter [21/1752] Loss: 13.2419\n",
      "Epoch [3/90], lter [31/1752] Loss: 24.3960\n",
      "Epoch [3/90], lter [41/1752] Loss: 40.9223\n",
      "Epoch [3/90], lter [51/1752] Loss: 26.5597\n",
      "Epoch [3/90], lter [61/1752] Loss: 21.4749\n",
      "Epoch [3/90], lter [71/1752] Loss: 19.0224\n",
      "Epoch [3/90], lter [81/1752] Loss: 24.0370\n",
      "Epoch [3/90], lter [91/1752] Loss: 31.8756\n",
      "Epoch [3/90], lter [101/1752] Loss: 13.6559\n",
      "Epoch [3/90], lter [111/1752] Loss: 24.5176\n",
      "Epoch [3/90], lter [121/1752] Loss: 24.2451\n",
      "Epoch [3/90], lter [131/1752] Loss: 32.2007\n",
      "Epoch [3/90], lter [141/1752] Loss: 23.5167\n",
      "Epoch [3/90], lter [151/1752] Loss: 17.9500\n",
      "Epoch [3/90], lter [161/1752] Loss: 19.5583\n",
      "Epoch [3/90], lter [171/1752] Loss: 29.4886\n",
      "Epoch [3/90], lter [181/1752] Loss: 21.7500\n",
      "Epoch [3/90], lter [191/1752] Loss: 16.2226\n",
      "Epoch [3/90], lter [201/1752] Loss: 25.2413\n",
      "Epoch [3/90], lter [211/1752] Loss: 19.7464\n",
      "Epoch [3/90], lter [221/1752] Loss: 24.9782\n",
      "Epoch [3/90], lter [231/1752] Loss: 23.7862\n",
      "Epoch [3/90], lter [241/1752] Loss: 36.0788\n",
      "Epoch [3/90], lter [251/1752] Loss: 25.5321\n",
      "Epoch [3/90], lter [261/1752] Loss: 22.0726\n",
      "Epoch [3/90], lter [271/1752] Loss: 13.8025\n",
      "Epoch [3/90], lter [281/1752] Loss: 15.9799\n",
      "Epoch [3/90], lter [291/1752] Loss: 12.6489\n",
      "Epoch [3/90], lter [301/1752] Loss: 21.1551\n",
      "Epoch [3/90], lter [311/1752] Loss: 31.7016\n",
      "Epoch [3/90], lter [321/1752] Loss: 17.0162\n",
      "Epoch [3/90], lter [331/1752] Loss: 26.2993\n",
      "Epoch [3/90], lter [341/1752] Loss: 21.5385\n",
      "Epoch [3/90], lter [351/1752] Loss: 23.8715\n",
      "Epoch [3/90], lter [361/1752] Loss: 18.6695\n",
      "Epoch [3/90], lter [371/1752] Loss: 27.3651\n",
      "Epoch [3/90], lter [381/1752] Loss: 23.4981\n",
      "Epoch [3/90], lter [391/1752] Loss: 24.9282\n",
      "Epoch [3/90], lter [401/1752] Loss: 36.7208\n",
      "Epoch [3/90], lter [411/1752] Loss: 30.0664\n",
      "Epoch [3/90], lter [421/1752] Loss: 22.8787\n",
      "Epoch [3/90], lter [431/1752] Loss: 20.2097\n",
      "Epoch [3/90], lter [441/1752] Loss: 31.4804\n",
      "Epoch [3/90], lter [451/1752] Loss: 27.8707\n",
      "Epoch [3/90], lter [461/1752] Loss: 23.9827\n",
      "Epoch [3/90], lter [471/1752] Loss: 19.1254\n",
      "Epoch [3/90], lter [481/1752] Loss: 21.0207\n",
      "Epoch [3/90], lter [491/1752] Loss: 29.8058\n",
      "Epoch [3/90], lter [501/1752] Loss: 32.6546\n",
      "Epoch [3/90], lter [511/1752] Loss: 24.3389\n",
      "Epoch [3/90], lter [521/1752] Loss: 29.1165\n",
      "Epoch [3/90], lter [531/1752] Loss: 22.6457\n",
      "Epoch [3/90], lter [541/1752] Loss: 20.7220\n",
      "Epoch [3/90], lter [551/1752] Loss: 24.8318\n",
      "Epoch [3/90], lter [561/1752] Loss: 17.3690\n",
      "Epoch [3/90], lter [571/1752] Loss: 30.5233\n",
      "Epoch [3/90], lter [581/1752] Loss: 24.1805\n",
      "Epoch [3/90], lter [591/1752] Loss: 27.0260\n",
      "Epoch [3/90], lter [601/1752] Loss: 21.9581\n",
      "Epoch [3/90], lter [611/1752] Loss: 24.6211\n",
      "Epoch [3/90], lter [621/1752] Loss: 18.9950\n",
      "Epoch [3/90], lter [631/1752] Loss: 19.4861\n",
      "Epoch [3/90], lter [641/1752] Loss: 15.4118\n",
      "Epoch [3/90], lter [651/1752] Loss: 18.9289\n",
      "Epoch [3/90], lter [661/1752] Loss: 25.3223\n",
      "Epoch [3/90], lter [671/1752] Loss: 21.7141\n",
      "Epoch [3/90], lter [681/1752] Loss: 20.0309\n",
      "Epoch [3/90], lter [691/1752] Loss: 18.0391\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Start training\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    total_batch = len(train_data_loader.dataset)//BATCH_SIZE\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()  # Set the model to train mode\n",
    "\n",
    "    # Get statistics\n",
    "    epoch_loss = 0\n",
    "    len_dataset = 0\n",
    "    \n",
    "    for step, (batch_images, batch_labels) in enumerate(train_data_loader):\n",
    "        X, Y = batch_images.to(device), batch_labels.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += pred.shape[0] * loss.item()\n",
    "        len_dataset += pred.shape[0]\n",
    "        if (step) % 10 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, EPOCHS, step+1, total_batch, loss.item()))\n",
    "\n",
    "    epoch_loss = epoch_loss/ len_dataset\n",
    "    print('Epoch: ', epoch+1, '| train loss : %0.4f' % epoch_loss)\n",
    "\n",
    "    LR_SCHEDULER.step()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.inference_mode():  \n",
    "        running_loss = 0\n",
    "        for step, (images, labels) in enumerate(test_data_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    running_loss = running_loss / len(test_data_loader)\n",
    "    print('Epoch: ', epoch, '| test loss : %0.4f' % running_loss )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84f01a-84ff-4a51-a83f-a2735e83abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), 'trained_inception_v3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec2bd5-6592-493c-b47f-88d834fde95f",
   "metadata": {},
   "source": [
    "# GARBAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932f170-89db-4555-aac4-fa85b3f90732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(5)):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor (step, (x, y)) in enumerate(train_data_loader):\n",
    "\t\t# send the input to the device\n",
    "\t\t#(x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\t# calculate the gradients\n",
    "\t\tloss.backward()\n",
    "\t\t# check if we are updating the model parameters and if so\n",
    "\t\t# update them, and zero out the previously accumulated gradients\n",
    "\t\tif (step + 2) % 2 == 0:\n",
    "\t\t\topt.step()\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\n",
    "    \t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tmodel.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in test_data_loader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t#(x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = model(x)\n",
    "\t\t\ttotalValLoss += lossFunc(pred, y)\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\tvalCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\t\ttorch.float).sum().item()\n",
    "\n",
    "    \t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# calculate the training and validation accuracy\n",
    "\ttrainCorrect = trainCorrect / len(trainDS)\n",
    "\tvalCorrect = valCorrect / len(valDS)\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(trainCorrect)\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\tH[\"val_acc\"].append(valCorrect)\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, 5))\n",
    "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, trainCorrect))\n",
    "\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\".format(\n",
    "\t\tavgValLoss, valCorrect))\n",
    "\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))\n",
    "# plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "# plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "# plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(config.WARMUP_PLOT)\n",
    "# serialize the model to disk\n",
    "torch.save(model, config.WARMUP_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f22e0-565c-4aa5-8526-b97fcb30e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in train_data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss / dataset_sizes\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            # deep copy the model\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f227e47-2ac9-4ff3-95a2-25054dd201db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
