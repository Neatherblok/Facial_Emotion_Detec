{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574f39d9-dab6-45c3-bfd3-78e3dfebfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# Specify where to find the data preparation class\n",
    "sys.path.append('../../Data_Preparation')\n",
    "from Preparation import CustomDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008ac5be-e4af-4ee9-bb44-2d8a137cf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3 training data (ImageNet) properties\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "DIMENSIONS = 3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LR = 0.1\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LR_STEP_SIZE = 30\n",
    "LR_GAMMA = 0.1\n",
    "\n",
    "EPOCHS = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7889fed-b144-4a60-852b-ef19a3d1fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders\n",
      "Train Data Loader:\n",
      "Batch Index: 0\n",
      "Inputs Shape: torch.Size([16, 3, 299, 299])\n",
      "Labels Shape: torch.Size([16])\n",
      "Labels: tensor([3, 0, 3, 0, 0])\n",
      "Batch Index: 1\n",
      "Inputs Shape: torch.Size([16, 3, 299, 299])\n",
      "Labels Shape: torch.Size([16])\n",
      "Labels: tensor([4, 6, 3, 0, 4])\n",
      "Batch Index: 2\n",
      "Inputs Shape: torch.Size([16, 3, 299, 299])\n",
      "Labels Shape: torch.Size([16])\n",
      "Labels: tensor([0, 6, 2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating data loaders\")\n",
    "# Instantiate the CustomDataLoader class for training\n",
    "train_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=BATCH_SIZE, dataset_type=\"train\", mean=MEAN, std=STD, dimensions=3).data_loader\n",
    "test_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=BATCH_SIZE, dataset_type=\"test\", mean=MEAN, std=STD, dimensions=3).data_loader\n",
    "\n",
    "# Confirm correct data load\n",
    "print(\"Train Data Loader:\")\n",
    "for batch_idx, (inputs, labels) in enumerate(train_data_loader):\n",
    "    print(\"Batch Index:\", batch_idx)\n",
    "    print(\"Inputs Shape:\", inputs.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    # Print the first few labels in the batch\n",
    "    print(\"Labels:\", labels[:5])\n",
    "    # Break after printing a few batches\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab3e7ce-6b24-499b-b5b4-4708ffcce3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\kh2197vi\\downloads\\facial_emotion_detec-main\\facial_emotion_detec-main\\.env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\users\\kh2197vi\\downloads\\facial_emotion_detec-main\\facial_emotion_detec-main\\.env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load up the InceptionV3 model\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer with a new one that outputs 7 classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 7)  # Output layer with 7 classes\n",
    "model.aux_logits = False\n",
    "model.AuxLogits = None\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "LR_SCHEDULER = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP_SIZE, gamma=LR_GAMMA)\n",
    "\n",
    "#torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e3c195-5ac7-4b5c-ac35-5ff58399b50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start training\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/90], lter [1/1752] Loss: 1.9066\n",
      "Epoch [1/90], lter [11/1752] Loss: 9.6600\n",
      "Epoch [1/90], lter [21/1752] Loss: 15.6475\n",
      "Epoch [1/90], lter [31/1752] Loss: 22.2051\n",
      "Epoch [1/90], lter [41/1752] Loss: 17.0985\n",
      "Epoch [1/90], lter [51/1752] Loss: 15.4631\n",
      "Epoch [1/90], lter [61/1752] Loss: 14.0895\n",
      "Epoch [1/90], lter [71/1752] Loss: 15.2768\n",
      "Epoch [1/90], lter [81/1752] Loss: 5.2184\n",
      "Epoch [1/90], lter [91/1752] Loss: 12.1279\n",
      "Epoch [1/90], lter [101/1752] Loss: 19.4472\n",
      "Epoch [1/90], lter [111/1752] Loss: 11.7660\n",
      "Epoch [1/90], lter [121/1752] Loss: 12.4745\n",
      "Epoch [1/90], lter [131/1752] Loss: 19.8807\n",
      "Epoch [1/90], lter [141/1752] Loss: 19.9148\n",
      "Epoch [1/90], lter [151/1752] Loss: 10.7936\n",
      "Epoch [1/90], lter [161/1752] Loss: 15.7218\n",
      "Epoch [1/90], lter [171/1752] Loss: 15.2498\n",
      "Epoch [1/90], lter [181/1752] Loss: 28.2794\n",
      "Epoch [1/90], lter [191/1752] Loss: 19.8906\n",
      "Epoch [1/90], lter [201/1752] Loss: 20.7210\n",
      "Epoch [1/90], lter [211/1752] Loss: 8.7459\n",
      "Epoch [1/90], lter [221/1752] Loss: 16.9010\n",
      "Epoch [1/90], lter [231/1752] Loss: 18.3869\n",
      "Epoch [1/90], lter [241/1752] Loss: 18.6214\n",
      "Epoch [1/90], lter [251/1752] Loss: 11.9109\n",
      "Epoch [1/90], lter [261/1752] Loss: 27.2632\n",
      "Epoch [1/90], lter [271/1752] Loss: 29.5119\n",
      "Epoch [1/90], lter [281/1752] Loss: 26.4971\n",
      "Epoch [1/90], lter [291/1752] Loss: 16.9011\n",
      "Epoch [1/90], lter [301/1752] Loss: 24.0964\n",
      "Epoch [1/90], lter [311/1752] Loss: 20.3900\n",
      "Epoch [1/90], lter [321/1752] Loss: 18.6190\n",
      "Epoch [1/90], lter [331/1752] Loss: 10.4592\n",
      "Epoch [1/90], lter [341/1752] Loss: 17.9297\n",
      "Epoch [1/90], lter [351/1752] Loss: 24.2933\n",
      "Epoch [1/90], lter [361/1752] Loss: 27.6609\n",
      "Epoch [1/90], lter [371/1752] Loss: 30.0258\n",
      "Epoch [1/90], lter [381/1752] Loss: 26.4256\n",
      "Epoch [1/90], lter [391/1752] Loss: 19.4421\n",
      "Epoch [1/90], lter [401/1752] Loss: 21.3713\n",
      "Epoch [1/90], lter [411/1752] Loss: 25.4936\n",
      "Epoch [1/90], lter [421/1752] Loss: 24.6663\n",
      "Epoch [1/90], lter [431/1752] Loss: 23.7027\n",
      "Epoch [1/90], lter [441/1752] Loss: 27.6591\n",
      "Epoch [1/90], lter [451/1752] Loss: 32.7677\n",
      "Epoch [1/90], lter [461/1752] Loss: 28.1584\n",
      "Epoch [1/90], lter [471/1752] Loss: 24.4792\n",
      "Epoch [1/90], lter [481/1752] Loss: 14.9702\n",
      "Epoch [1/90], lter [491/1752] Loss: 21.9521\n",
      "Epoch [1/90], lter [501/1752] Loss: 23.3372\n",
      "Epoch [1/90], lter [511/1752] Loss: 24.7999\n",
      "Epoch [1/90], lter [521/1752] Loss: 20.3952\n",
      "Epoch [1/90], lter [531/1752] Loss: 23.9379\n",
      "Epoch [1/90], lter [541/1752] Loss: 28.8144\n",
      "Epoch [1/90], lter [551/1752] Loss: 26.3616\n",
      "Epoch [1/90], lter [561/1752] Loss: 24.1890\n",
      "Epoch [1/90], lter [571/1752] Loss: 26.0463\n",
      "Epoch [1/90], lter [581/1752] Loss: 29.3200\n",
      "Epoch [1/90], lter [591/1752] Loss: 29.6975\n",
      "Epoch [1/90], lter [601/1752] Loss: 20.9597\n",
      "Epoch [1/90], lter [611/1752] Loss: 21.7439\n",
      "Epoch [1/90], lter [621/1752] Loss: 10.2700\n",
      "Epoch [1/90], lter [631/1752] Loss: 28.4159\n",
      "Epoch [1/90], lter [641/1752] Loss: 20.0164\n",
      "Epoch [1/90], lter [651/1752] Loss: 18.7201\n",
      "Epoch [1/90], lter [661/1752] Loss: 25.2528\n",
      "Epoch [1/90], lter [671/1752] Loss: 14.1762\n",
      "Epoch [1/90], lter [681/1752] Loss: 23.8514\n",
      "Epoch [1/90], lter [691/1752] Loss: 19.4463\n",
      "Epoch [1/90], lter [701/1752] Loss: 26.7614\n",
      "Epoch [1/90], lter [711/1752] Loss: 27.0788\n",
      "Epoch [1/90], lter [721/1752] Loss: 35.6470\n",
      "Epoch [1/90], lter [731/1752] Loss: 22.9596\n",
      "Epoch [1/90], lter [741/1752] Loss: 31.5948\n",
      "Epoch [1/90], lter [751/1752] Loss: 20.4224\n",
      "Epoch [1/90], lter [761/1752] Loss: 16.6503\n",
      "Epoch [1/90], lter [771/1752] Loss: 17.0434\n",
      "Epoch [1/90], lter [781/1752] Loss: 25.3639\n",
      "Epoch [1/90], lter [791/1752] Loss: 30.0556\n",
      "Epoch [1/90], lter [801/1752] Loss: 18.2036\n",
      "Epoch [1/90], lter [811/1752] Loss: 21.7057\n",
      "Epoch [1/90], lter [821/1752] Loss: 29.9871\n",
      "Epoch [1/90], lter [831/1752] Loss: 24.8360\n",
      "Epoch [1/90], lter [841/1752] Loss: 30.7527\n",
      "Epoch [1/90], lter [851/1752] Loss: 21.9945\n",
      "Epoch [1/90], lter [861/1752] Loss: 24.0844\n",
      "Epoch [1/90], lter [871/1752] Loss: 22.9542\n",
      "Epoch [1/90], lter [881/1752] Loss: 27.3613\n",
      "Epoch [1/90], lter [891/1752] Loss: 13.2256\n",
      "Epoch [1/90], lter [901/1752] Loss: 22.7059\n",
      "Epoch [1/90], lter [911/1752] Loss: 18.7502\n",
      "Epoch [1/90], lter [921/1752] Loss: 21.1431\n",
      "Epoch [1/90], lter [931/1752] Loss: 21.5277\n",
      "Epoch [1/90], lter [941/1752] Loss: 16.4074\n",
      "Epoch [1/90], lter [951/1752] Loss: 27.9706\n",
      "Epoch [1/90], lter [961/1752] Loss: 13.8528\n",
      "Epoch [1/90], lter [971/1752] Loss: 32.2680\n",
      "Epoch [1/90], lter [981/1752] Loss: 17.6374\n",
      "Epoch [1/90], lter [991/1752] Loss: 21.7203\n",
      "Epoch [1/90], lter [1001/1752] Loss: 25.0850\n",
      "Epoch [1/90], lter [1011/1752] Loss: 17.4153\n",
      "Epoch [1/90], lter [1021/1752] Loss: 22.0180\n",
      "Epoch [1/90], lter [1031/1752] Loss: 18.7790\n",
      "Epoch [1/90], lter [1041/1752] Loss: 22.9104\n",
      "Epoch [1/90], lter [1051/1752] Loss: 31.2425\n",
      "Epoch [1/90], lter [1061/1752] Loss: 21.3882\n",
      "Epoch [1/90], lter [1071/1752] Loss: 17.5742\n",
      "Epoch [1/90], lter [1081/1752] Loss: 20.8986\n",
      "Epoch [1/90], lter [1091/1752] Loss: 30.7508\n",
      "Epoch [1/90], lter [1101/1752] Loss: 28.0758\n",
      "Epoch [1/90], lter [1111/1752] Loss: 27.3659\n",
      "Epoch [1/90], lter [1121/1752] Loss: 32.8332\n",
      "Epoch [1/90], lter [1131/1752] Loss: 24.4726\n",
      "Epoch [1/90], lter [1141/1752] Loss: 34.0014\n",
      "Epoch [1/90], lter [1151/1752] Loss: 22.5214\n",
      "Epoch [1/90], lter [1161/1752] Loss: 15.6787\n",
      "Epoch [1/90], lter [1171/1752] Loss: 16.9805\n",
      "Epoch [1/90], lter [1181/1752] Loss: 18.3384\n",
      "Epoch [1/90], lter [1191/1752] Loss: 27.6857\n",
      "Epoch [1/90], lter [1201/1752] Loss: 12.6970\n",
      "Epoch [1/90], lter [1211/1752] Loss: 15.8037\n",
      "Epoch [1/90], lter [1221/1752] Loss: 24.9512\n",
      "Epoch [1/90], lter [1231/1752] Loss: 26.2386\n",
      "Epoch [1/90], lter [1241/1752] Loss: 22.6165\n",
      "Epoch [1/90], lter [1251/1752] Loss: 13.2605\n",
      "Epoch [1/90], lter [1261/1752] Loss: 15.9534\n",
      "Epoch [1/90], lter [1271/1752] Loss: 21.1096\n",
      "Epoch [1/90], lter [1281/1752] Loss: 21.4453\n",
      "Epoch [1/90], lter [1291/1752] Loss: 39.3051\n",
      "Epoch [1/90], lter [1301/1752] Loss: 27.4072\n",
      "Epoch [1/90], lter [1311/1752] Loss: 27.7572\n",
      "Epoch [1/90], lter [1321/1752] Loss: 29.2090\n",
      "Epoch [1/90], lter [1331/1752] Loss: 20.9702\n",
      "Epoch [1/90], lter [1341/1752] Loss: 23.9400\n",
      "Epoch [1/90], lter [1351/1752] Loss: 29.8362\n",
      "Epoch [1/90], lter [1361/1752] Loss: 29.1818\n",
      "Epoch [1/90], lter [1371/1752] Loss: 35.7828\n",
      "Epoch [1/90], lter [1381/1752] Loss: 18.2726\n",
      "Epoch [1/90], lter [1391/1752] Loss: 24.1610\n",
      "Epoch [1/90], lter [1401/1752] Loss: 22.7521\n",
      "Epoch [1/90], lter [1411/1752] Loss: 36.4170\n",
      "Epoch [1/90], lter [1421/1752] Loss: 30.5588\n",
      "Epoch [1/90], lter [1431/1752] Loss: 15.0690\n",
      "Epoch [1/90], lter [1441/1752] Loss: 24.1821\n",
      "Epoch [1/90], lter [1451/1752] Loss: 26.8125\n",
      "Epoch [1/90], lter [1461/1752] Loss: 29.1287\n",
      "Epoch [1/90], lter [1471/1752] Loss: 24.7196\n",
      "Epoch [1/90], lter [1481/1752] Loss: 31.7968\n",
      "Epoch [1/90], lter [1491/1752] Loss: 23.0711\n",
      "Epoch [1/90], lter [1501/1752] Loss: 17.4518\n",
      "Epoch [1/90], lter [1511/1752] Loss: 31.6137\n",
      "Epoch [1/90], lter [1521/1752] Loss: 18.6963\n",
      "Epoch [1/90], lter [1531/1752] Loss: 28.0350\n",
      "Epoch [1/90], lter [1541/1752] Loss: 22.9814\n",
      "Epoch [1/90], lter [1551/1752] Loss: 20.1976\n",
      "Epoch [1/90], lter [1561/1752] Loss: 31.6477\n",
      "Epoch [1/90], lter [1571/1752] Loss: 27.9053\n",
      "Epoch [1/90], lter [1581/1752] Loss: 26.8315\n",
      "Epoch [1/90], lter [1591/1752] Loss: 21.7577\n",
      "Epoch [1/90], lter [1601/1752] Loss: 21.5121\n",
      "Epoch [1/90], lter [1611/1752] Loss: 25.6651\n",
      "Epoch [1/90], lter [1621/1752] Loss: 24.9026\n",
      "Epoch [1/90], lter [1631/1752] Loss: 21.1669\n",
      "Epoch [1/90], lter [1641/1752] Loss: 37.5408\n",
      "Epoch [1/90], lter [1651/1752] Loss: 27.9744\n",
      "Epoch [1/90], lter [1661/1752] Loss: 22.9107\n",
      "Epoch [1/90], lter [1671/1752] Loss: 17.1299\n",
      "Epoch [1/90], lter [1681/1752] Loss: 34.1938\n",
      "Epoch [1/90], lter [1691/1752] Loss: 19.6068\n",
      "Epoch [1/90], lter [1701/1752] Loss: 31.6614\n",
      "Epoch [1/90], lter [1711/1752] Loss: 25.6130\n",
      "Epoch [1/90], lter [1721/1752] Loss: 23.8465\n",
      "Epoch [1/90], lter [1731/1752] Loss: 22.2633\n",
      "Epoch [1/90], lter [1741/1752] Loss: 19.7167\n",
      "Epoch [1/90], lter [1751/1752] Loss: 16.3642\n",
      "Epoch:  1 | train loss : 22.8619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/90 [62:48:38<5590:09:47, 226118.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | test loss : 20.6425\n",
      "Epoch [2/90], lter [1/1752] Loss: 25.6945\n",
      "Epoch [2/90], lter [11/1752] Loss: 26.5892\n",
      "Epoch [2/90], lter [21/1752] Loss: 27.3370\n",
      "Epoch [2/90], lter [31/1752] Loss: 17.6685\n",
      "Epoch [2/90], lter [41/1752] Loss: 27.7771\n",
      "Epoch [2/90], lter [51/1752] Loss: 13.9643\n",
      "Epoch [2/90], lter [61/1752] Loss: 23.7278\n",
      "Epoch [2/90], lter [71/1752] Loss: 28.5910\n",
      "Epoch [2/90], lter [81/1752] Loss: 20.5958\n",
      "Epoch [2/90], lter [91/1752] Loss: 19.4352\n",
      "Epoch [2/90], lter [101/1752] Loss: 24.2826\n",
      "Epoch [2/90], lter [111/1752] Loss: 26.2649\n",
      "Epoch [2/90], lter [121/1752] Loss: 26.7614\n",
      "Epoch [2/90], lter [131/1752] Loss: 22.5411\n",
      "Epoch [2/90], lter [141/1752] Loss: 12.3127\n",
      "Epoch [2/90], lter [151/1752] Loss: 22.5944\n",
      "Epoch [2/90], lter [161/1752] Loss: 24.9117\n",
      "Epoch [2/90], lter [171/1752] Loss: 14.9085\n",
      "Epoch [2/90], lter [181/1752] Loss: 26.0958\n",
      "Epoch [2/90], lter [191/1752] Loss: 21.8663\n",
      "Epoch [2/90], lter [201/1752] Loss: 28.5280\n",
      "Epoch [2/90], lter [211/1752] Loss: 20.4405\n",
      "Epoch [2/90], lter [221/1752] Loss: 17.7292\n",
      "Epoch [2/90], lter [231/1752] Loss: 21.7301\n",
      "Epoch [2/90], lter [241/1752] Loss: 34.7851\n",
      "Epoch [2/90], lter [251/1752] Loss: 26.5845\n",
      "Epoch [2/90], lter [261/1752] Loss: 15.8665\n",
      "Epoch [2/90], lter [271/1752] Loss: 25.7811\n",
      "Epoch [2/90], lter [281/1752] Loss: 28.0549\n",
      "Epoch [2/90], lter [291/1752] Loss: 19.6951\n",
      "Epoch [2/90], lter [301/1752] Loss: 32.5936\n",
      "Epoch [2/90], lter [311/1752] Loss: 19.5305\n",
      "Epoch [2/90], lter [321/1752] Loss: 27.7885\n",
      "Epoch [2/90], lter [331/1752] Loss: 22.5921\n",
      "Epoch [2/90], lter [341/1752] Loss: 28.7703\n",
      "Epoch [2/90], lter [351/1752] Loss: 32.8317\n",
      "Epoch [2/90], lter [361/1752] Loss: 23.5910\n",
      "Epoch [2/90], lter [371/1752] Loss: 30.5156\n",
      "Epoch [2/90], lter [381/1752] Loss: 24.6361\n",
      "Epoch [2/90], lter [391/1752] Loss: 18.0363\n",
      "Epoch [2/90], lter [401/1752] Loss: 15.5502\n",
      "Epoch [2/90], lter [411/1752] Loss: 25.7366\n",
      "Epoch [2/90], lter [421/1752] Loss: 24.4044\n",
      "Epoch [2/90], lter [431/1752] Loss: 19.1234\n",
      "Epoch [2/90], lter [441/1752] Loss: 18.6518\n",
      "Epoch [2/90], lter [451/1752] Loss: 21.7664\n",
      "Epoch [2/90], lter [461/1752] Loss: 22.4390\n",
      "Epoch [2/90], lter [471/1752] Loss: 27.5896\n",
      "Epoch [2/90], lter [481/1752] Loss: 18.8536\n",
      "Epoch [2/90], lter [491/1752] Loss: 21.0273\n",
      "Epoch [2/90], lter [501/1752] Loss: 32.6005\n",
      "Epoch [2/90], lter [511/1752] Loss: 20.3523\n",
      "Epoch [2/90], lter [521/1752] Loss: 26.8317\n",
      "Epoch [2/90], lter [531/1752] Loss: 30.9168\n",
      "Epoch [2/90], lter [541/1752] Loss: 18.8013\n",
      "Epoch [2/90], lter [551/1752] Loss: 25.6894\n",
      "Epoch [2/90], lter [561/1752] Loss: 36.4007\n",
      "Epoch [2/90], lter [571/1752] Loss: 21.9614\n",
      "Epoch [2/90], lter [581/1752] Loss: 20.5200\n",
      "Epoch [2/90], lter [591/1752] Loss: 33.0060\n",
      "Epoch [2/90], lter [601/1752] Loss: 29.3335\n",
      "Epoch [2/90], lter [611/1752] Loss: 13.3608\n",
      "Epoch [2/90], lter [621/1752] Loss: 18.4573\n",
      "Epoch [2/90], lter [631/1752] Loss: 24.8972\n",
      "Epoch [2/90], lter [641/1752] Loss: 18.2835\n",
      "Epoch [2/90], lter [651/1752] Loss: 23.4467\n",
      "Epoch [2/90], lter [661/1752] Loss: 33.8507\n",
      "Epoch [2/90], lter [671/1752] Loss: 22.1201\n",
      "Epoch [2/90], lter [681/1752] Loss: 35.5217\n",
      "Epoch [2/90], lter [691/1752] Loss: 41.8089\n",
      "Epoch [2/90], lter [701/1752] Loss: 29.5192\n",
      "Epoch [2/90], lter [711/1752] Loss: 32.5109\n",
      "Epoch [2/90], lter [721/1752] Loss: 41.2613\n",
      "Epoch [2/90], lter [731/1752] Loss: 26.2556\n",
      "Epoch [2/90], lter [741/1752] Loss: 36.7594\n",
      "Epoch [2/90], lter [751/1752] Loss: 26.1689\n",
      "Epoch [2/90], lter [761/1752] Loss: 13.7534\n",
      "Epoch [2/90], lter [771/1752] Loss: 13.3551\n",
      "Epoch [2/90], lter [781/1752] Loss: 23.9821\n",
      "Epoch [2/90], lter [791/1752] Loss: 23.2615\n",
      "Epoch [2/90], lter [801/1752] Loss: 21.9989\n",
      "Epoch [2/90], lter [811/1752] Loss: 25.9458\n",
      "Epoch [2/90], lter [821/1752] Loss: 24.7663\n",
      "Epoch [2/90], lter [831/1752] Loss: 17.6298\n",
      "Epoch [2/90], lter [841/1752] Loss: 16.5501\n",
      "Epoch [2/90], lter [851/1752] Loss: 22.1997\n",
      "Epoch [2/90], lter [861/1752] Loss: 26.4797\n",
      "Epoch [2/90], lter [871/1752] Loss: 24.3859\n",
      "Epoch [2/90], lter [881/1752] Loss: 21.4353\n",
      "Epoch [2/90], lter [891/1752] Loss: 41.0872\n",
      "Epoch [2/90], lter [901/1752] Loss: 47.0256\n",
      "Epoch [2/90], lter [911/1752] Loss: 25.2000\n",
      "Epoch [2/90], lter [921/1752] Loss: 30.0683\n",
      "Epoch [2/90], lter [931/1752] Loss: 33.4456\n",
      "Epoch [2/90], lter [941/1752] Loss: 20.3010\n",
      "Epoch [2/90], lter [951/1752] Loss: 21.8608\n",
      "Epoch [2/90], lter [961/1752] Loss: 28.3284\n",
      "Epoch [2/90], lter [971/1752] Loss: 29.2430\n",
      "Epoch [2/90], lter [981/1752] Loss: 20.8582\n",
      "Epoch [2/90], lter [991/1752] Loss: 17.5375\n",
      "Epoch [2/90], lter [1001/1752] Loss: 34.5328\n",
      "Epoch [2/90], lter [1011/1752] Loss: 13.5489\n",
      "Epoch [2/90], lter [1021/1752] Loss: 35.4263\n",
      "Epoch [2/90], lter [1031/1752] Loss: 34.3737\n",
      "Epoch [2/90], lter [1041/1752] Loss: 25.4708\n",
      "Epoch [2/90], lter [1051/1752] Loss: 21.5128\n",
      "Epoch [2/90], lter [1061/1752] Loss: 19.4177\n",
      "Epoch [2/90], lter [1071/1752] Loss: 22.4120\n",
      "Epoch [2/90], lter [1081/1752] Loss: 18.1885\n",
      "Epoch [2/90], lter [1091/1752] Loss: 22.0244\n",
      "Epoch [2/90], lter [1101/1752] Loss: 20.5437\n",
      "Epoch [2/90], lter [1111/1752] Loss: 25.4787\n",
      "Epoch [2/90], lter [1121/1752] Loss: 23.1535\n",
      "Epoch [2/90], lter [1131/1752] Loss: 34.6644\n",
      "Epoch [2/90], lter [1141/1752] Loss: 24.1398\n",
      "Epoch [2/90], lter [1151/1752] Loss: 19.0392\n",
      "Epoch [2/90], lter [1161/1752] Loss: 25.8320\n",
      "Epoch [2/90], lter [1171/1752] Loss: 19.9577\n",
      "Epoch [2/90], lter [1181/1752] Loss: 14.7832\n",
      "Epoch [2/90], lter [1191/1752] Loss: 21.8711\n",
      "Epoch [2/90], lter [1201/1752] Loss: 30.5686\n",
      "Epoch [2/90], lter [1211/1752] Loss: 23.2791\n",
      "Epoch [2/90], lter [1221/1752] Loss: 19.8873\n",
      "Epoch [2/90], lter [1231/1752] Loss: 21.4382\n",
      "Epoch [2/90], lter [1241/1752] Loss: 20.1375\n",
      "Epoch [2/90], lter [1251/1752] Loss: 18.8047\n",
      "Epoch [2/90], lter [1261/1752] Loss: 23.5777\n",
      "Epoch [2/90], lter [1271/1752] Loss: 25.2817\n",
      "Epoch [2/90], lter [1281/1752] Loss: 23.5775\n",
      "Epoch [2/90], lter [1291/1752] Loss: 15.5272\n",
      "Epoch [2/90], lter [1301/1752] Loss: 25.0027\n",
      "Epoch [2/90], lter [1311/1752] Loss: 22.9931\n",
      "Epoch [2/90], lter [1321/1752] Loss: 26.4149\n",
      "Epoch [2/90], lter [1331/1752] Loss: 34.3262\n",
      "Epoch [2/90], lter [1341/1752] Loss: 22.4345\n",
      "Epoch [2/90], lter [1351/1752] Loss: 25.8147\n",
      "Epoch [2/90], lter [1361/1752] Loss: 15.7225\n",
      "Epoch [2/90], lter [1371/1752] Loss: 11.9020\n",
      "Epoch [2/90], lter [1381/1752] Loss: 23.1178\n",
      "Epoch [2/90], lter [1391/1752] Loss: 36.2763\n",
      "Epoch [2/90], lter [1401/1752] Loss: 33.0192\n",
      "Epoch [2/90], lter [1411/1752] Loss: 18.0927\n",
      "Epoch [2/90], lter [1421/1752] Loss: 33.0074\n",
      "Epoch [2/90], lter [1431/1752] Loss: 21.5158\n",
      "Epoch [2/90], lter [1441/1752] Loss: 25.4937\n",
      "Epoch [2/90], lter [1451/1752] Loss: 38.4364\n",
      "Epoch [2/90], lter [1461/1752] Loss: 27.3732\n",
      "Epoch [2/90], lter [1471/1752] Loss: 27.1923\n",
      "Epoch [2/90], lter [1481/1752] Loss: 36.0215\n",
      "Epoch [2/90], lter [1491/1752] Loss: 27.6608\n",
      "Epoch [2/90], lter [1501/1752] Loss: 24.2473\n",
      "Epoch [2/90], lter [1511/1752] Loss: 24.5768\n",
      "Epoch [2/90], lter [1521/1752] Loss: 24.0625\n",
      "Epoch [2/90], lter [1531/1752] Loss: 20.5459\n",
      "Epoch [2/90], lter [1541/1752] Loss: 22.5065\n",
      "Epoch [2/90], lter [1551/1752] Loss: 31.2149\n",
      "Epoch [2/90], lter [1561/1752] Loss: 26.9342\n",
      "Epoch [2/90], lter [1571/1752] Loss: 28.4367\n",
      "Epoch [2/90], lter [1581/1752] Loss: 30.0828\n",
      "Epoch [2/90], lter [1591/1752] Loss: 22.6156\n",
      "Epoch [2/90], lter [1601/1752] Loss: 31.9747\n",
      "Epoch [2/90], lter [1611/1752] Loss: 23.3676\n",
      "Epoch [2/90], lter [1621/1752] Loss: 30.5699\n",
      "Epoch [2/90], lter [1631/1752] Loss: 20.8326\n",
      "Epoch [2/90], lter [1641/1752] Loss: 18.4766\n",
      "Epoch [2/90], lter [1651/1752] Loss: 22.9150\n",
      "Epoch [2/90], lter [1661/1752] Loss: 22.0811\n",
      "Epoch [2/90], lter [1671/1752] Loss: 18.7037\n",
      "Epoch [2/90], lter [1681/1752] Loss: 33.5310\n",
      "Epoch [2/90], lter [1691/1752] Loss: 23.8140\n",
      "Epoch [2/90], lter [1701/1752] Loss: 34.0609\n",
      "Epoch [2/90], lter [1711/1752] Loss: 30.2882\n",
      "Epoch [2/90], lter [1721/1752] Loss: 18.6442\n",
      "Epoch [2/90], lter [1731/1752] Loss: 19.5286\n",
      "Epoch [2/90], lter [1741/1752] Loss: 31.3586\n",
      "Epoch [2/90], lter [1751/1752] Loss: 26.8178\n",
      "Epoch:  2 | train loss : 24.2881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/90 [63:03:04<2288:25:09, 93617.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | test loss : 18.6842\n",
      "Epoch [3/90], lter [1/1752] Loss: 30.9921\n",
      "Epoch [3/90], lter [11/1752] Loss: 13.0244\n",
      "Epoch [3/90], lter [21/1752] Loss: 17.8884\n",
      "Epoch [3/90], lter [31/1752] Loss: 20.1954\n",
      "Epoch [3/90], lter [41/1752] Loss: 27.9051\n",
      "Epoch [3/90], lter [51/1752] Loss: 28.2653\n",
      "Epoch [3/90], lter [61/1752] Loss: 22.2407\n",
      "Epoch [3/90], lter [71/1752] Loss: 22.1005\n",
      "Epoch [3/90], lter [81/1752] Loss: 20.8150\n",
      "Epoch [3/90], lter [91/1752] Loss: 30.7468\n",
      "Epoch [3/90], lter [101/1752] Loss: 23.1507\n",
      "Epoch [3/90], lter [111/1752] Loss: 22.5039\n",
      "Epoch [3/90], lter [121/1752] Loss: 24.1299\n",
      "Epoch [3/90], lter [131/1752] Loss: 24.6771\n",
      "Epoch [3/90], lter [141/1752] Loss: 33.5244\n",
      "Epoch [3/90], lter [151/1752] Loss: 12.5061\n",
      "Epoch [3/90], lter [161/1752] Loss: 25.1685\n",
      "Epoch [3/90], lter [171/1752] Loss: 32.3801\n",
      "Epoch [3/90], lter [181/1752] Loss: 30.1345\n",
      "Epoch [3/90], lter [191/1752] Loss: 24.9132\n",
      "Epoch [3/90], lter [201/1752] Loss: 16.2483\n",
      "Epoch [3/90], lter [211/1752] Loss: 25.0299\n",
      "Epoch [3/90], lter [221/1752] Loss: 24.6346\n",
      "Epoch [3/90], lter [231/1752] Loss: 17.2305\n",
      "Epoch [3/90], lter [241/1752] Loss: 23.4093\n",
      "Epoch [3/90], lter [251/1752] Loss: 29.6592\n",
      "Epoch [3/90], lter [261/1752] Loss: 15.8047\n",
      "Epoch [3/90], lter [271/1752] Loss: 17.4075\n",
      "Epoch [3/90], lter [281/1752] Loss: 27.2905\n",
      "Epoch [3/90], lter [291/1752] Loss: 19.6675\n",
      "Epoch [3/90], lter [301/1752] Loss: 19.4179\n",
      "Epoch [3/90], lter [311/1752] Loss: 22.3198\n",
      "Epoch [3/90], lter [321/1752] Loss: 25.6998\n",
      "Epoch [3/90], lter [331/1752] Loss: 33.8381\n",
      "Epoch [3/90], lter [341/1752] Loss: 25.1022\n",
      "Epoch [3/90], lter [351/1752] Loss: 25.8287\n",
      "Epoch [3/90], lter [361/1752] Loss: 34.2117\n",
      "Epoch [3/90], lter [371/1752] Loss: 20.4071\n",
      "Epoch [3/90], lter [381/1752] Loss: 26.1120\n",
      "Epoch [3/90], lter [391/1752] Loss: 30.3643\n",
      "Epoch [3/90], lter [401/1752] Loss: 28.5314\n",
      "Epoch [3/90], lter [411/1752] Loss: 27.3683\n",
      "Epoch [3/90], lter [421/1752] Loss: 40.4843\n",
      "Epoch [3/90], lter [431/1752] Loss: 22.8513\n",
      "Epoch [3/90], lter [441/1752] Loss: 18.0842\n",
      "Epoch [3/90], lter [451/1752] Loss: 25.6685\n",
      "Epoch [3/90], lter [461/1752] Loss: 28.1192\n",
      "Epoch [3/90], lter [471/1752] Loss: 25.3331\n",
      "Epoch [3/90], lter [481/1752] Loss: 31.6482\n",
      "Epoch [3/90], lter [491/1752] Loss: 37.9859\n",
      "Epoch [3/90], lter [501/1752] Loss: 26.5912\n",
      "Epoch [3/90], lter [511/1752] Loss: 18.1515\n",
      "Epoch [3/90], lter [521/1752] Loss: 26.7678\n",
      "Epoch [3/90], lter [531/1752] Loss: 15.7439\n",
      "Epoch [3/90], lter [541/1752] Loss: 29.9099\n",
      "Epoch [3/90], lter [551/1752] Loss: 28.5703\n",
      "Epoch [3/90], lter [561/1752] Loss: 28.1237\n",
      "Epoch [3/90], lter [571/1752] Loss: 33.9530\n",
      "Epoch [3/90], lter [581/1752] Loss: 16.7646\n",
      "Epoch [3/90], lter [591/1752] Loss: 31.3667\n",
      "Epoch [3/90], lter [601/1752] Loss: 25.5630\n",
      "Epoch [3/90], lter [611/1752] Loss: 18.4254\n",
      "Epoch [3/90], lter [621/1752] Loss: 22.2441\n",
      "Epoch [3/90], lter [631/1752] Loss: 24.4142\n",
      "Epoch [3/90], lter [641/1752] Loss: 18.9971\n",
      "Epoch [3/90], lter [651/1752] Loss: 34.3542\n",
      "Epoch [3/90], lter [661/1752] Loss: 37.9687\n",
      "Epoch [3/90], lter [671/1752] Loss: 31.7179\n",
      "Epoch [3/90], lter [681/1752] Loss: 24.1038\n",
      "Epoch [3/90], lter [691/1752] Loss: 35.3188\n",
      "Epoch [3/90], lter [701/1752] Loss: 25.9064\n",
      "Epoch [3/90], lter [711/1752] Loss: 19.6390\n",
      "Epoch [3/90], lter [721/1752] Loss: 21.7677\n",
      "Epoch [3/90], lter [731/1752] Loss: 34.0764\n",
      "Epoch [3/90], lter [741/1752] Loss: 37.5449\n",
      "Epoch [3/90], lter [751/1752] Loss: 38.9177\n",
      "Epoch [3/90], lter [761/1752] Loss: 16.7678\n",
      "Epoch [3/90], lter [771/1752] Loss: 22.3266\n",
      "Epoch [3/90], lter [781/1752] Loss: 16.3380\n",
      "Epoch [3/90], lter [791/1752] Loss: 26.7321\n",
      "Epoch [3/90], lter [801/1752] Loss: 30.7132\n",
      "Epoch [3/90], lter [811/1752] Loss: 26.1029\n",
      "Epoch [3/90], lter [821/1752] Loss: 20.2886\n",
      "Epoch [3/90], lter [831/1752] Loss: 29.6192\n",
      "Epoch [3/90], lter [841/1752] Loss: 32.0144\n",
      "Epoch [3/90], lter [851/1752] Loss: 33.4096\n",
      "Epoch [3/90], lter [861/1752] Loss: 22.7075\n",
      "Epoch [3/90], lter [871/1752] Loss: 25.2265\n",
      "Epoch [3/90], lter [881/1752] Loss: 12.4134\n",
      "Epoch [3/90], lter [891/1752] Loss: 30.1625\n",
      "Epoch [3/90], lter [901/1752] Loss: 17.5922\n",
      "Epoch [3/90], lter [911/1752] Loss: 26.0645\n",
      "Epoch [3/90], lter [921/1752] Loss: 38.7518\n",
      "Epoch [3/90], lter [931/1752] Loss: 42.5794\n",
      "Epoch [3/90], lter [941/1752] Loss: 26.8408\n",
      "Epoch [3/90], lter [951/1752] Loss: 16.9989\n",
      "Epoch [3/90], lter [961/1752] Loss: 25.6356\n",
      "Epoch [3/90], lter [971/1752] Loss: 26.6710\n",
      "Epoch [3/90], lter [981/1752] Loss: 16.8079\n",
      "Epoch [3/90], lter [991/1752] Loss: 20.0919\n",
      "Epoch [3/90], lter [1001/1752] Loss: 32.4135\n",
      "Epoch [3/90], lter [1011/1752] Loss: 27.1342\n",
      "Epoch [3/90], lter [1021/1752] Loss: 20.7902\n",
      "Epoch [3/90], lter [1031/1752] Loss: 23.7346\n",
      "Epoch [3/90], lter [1041/1752] Loss: 31.3456\n",
      "Epoch [3/90], lter [1051/1752] Loss: 22.5974\n",
      "Epoch [3/90], lter [1061/1752] Loss: 23.6804\n",
      "Epoch [3/90], lter [1071/1752] Loss: 21.8561\n",
      "Epoch [3/90], lter [1081/1752] Loss: 22.4761\n",
      "Epoch [3/90], lter [1091/1752] Loss: 34.4412\n",
      "Epoch [3/90], lter [1101/1752] Loss: 26.0851\n",
      "Epoch [3/90], lter [1111/1752] Loss: 38.9175\n",
      "Epoch [3/90], lter [1121/1752] Loss: 25.9314\n",
      "Epoch [3/90], lter [1131/1752] Loss: 29.6584\n",
      "Epoch [3/90], lter [1141/1752] Loss: 34.4299\n",
      "Epoch [3/90], lter [1151/1752] Loss: 31.1645\n",
      "Epoch [3/90], lter [1161/1752] Loss: 38.0608\n",
      "Epoch [3/90], lter [1171/1752] Loss: 13.9215\n",
      "Epoch [3/90], lter [1181/1752] Loss: 19.6156\n",
      "Epoch [3/90], lter [1191/1752] Loss: 16.8546\n",
      "Epoch [3/90], lter [1201/1752] Loss: 28.5135\n",
      "Epoch [3/90], lter [1211/1752] Loss: 21.6239\n",
      "Epoch [3/90], lter [1221/1752] Loss: 22.5683\n",
      "Epoch [3/90], lter [1231/1752] Loss: 27.6512\n",
      "Epoch [3/90], lter [1241/1752] Loss: 21.6390\n",
      "Epoch [3/90], lter [1251/1752] Loss: 17.4375\n",
      "Epoch [3/90], lter [1261/1752] Loss: 20.4895\n",
      "Epoch [3/90], lter [1271/1752] Loss: 32.1785\n",
      "Epoch [3/90], lter [1281/1752] Loss: 33.6588\n",
      "Epoch [3/90], lter [1291/1752] Loss: 15.8939\n",
      "Epoch [3/90], lter [1301/1752] Loss: 28.8010\n",
      "Epoch [3/90], lter [1311/1752] Loss: 35.3998\n",
      "Epoch [3/90], lter [1321/1752] Loss: 35.0275\n",
      "Epoch [3/90], lter [1331/1752] Loss: 15.0250\n",
      "Epoch [3/90], lter [1341/1752] Loss: 20.3326\n",
      "Epoch [3/90], lter [1351/1752] Loss: 25.2408\n",
      "Epoch [3/90], lter [1361/1752] Loss: 26.9036\n",
      "Epoch [3/90], lter [1371/1752] Loss: 19.2643\n",
      "Epoch [3/90], lter [1381/1752] Loss: 33.7075\n",
      "Epoch [3/90], lter [1391/1752] Loss: 22.8709\n",
      "Epoch [3/90], lter [1401/1752] Loss: 17.2815\n",
      "Epoch [3/90], lter [1411/1752] Loss: 21.7799\n",
      "Epoch [3/90], lter [1421/1752] Loss: 20.7847\n",
      "Epoch [3/90], lter [1431/1752] Loss: 20.5462\n",
      "Epoch [3/90], lter [1441/1752] Loss: 33.3529\n",
      "Epoch [3/90], lter [1451/1752] Loss: 24.1400\n",
      "Epoch [3/90], lter [1461/1752] Loss: 23.6852\n",
      "Epoch [3/90], lter [1471/1752] Loss: 27.3606\n",
      "Epoch [3/90], lter [1481/1752] Loss: 28.8165\n",
      "Epoch [3/90], lter [1491/1752] Loss: 20.3958\n",
      "Epoch [3/90], lter [1501/1752] Loss: 26.3372\n",
      "Epoch [3/90], lter [1511/1752] Loss: 22.2614\n",
      "Epoch [3/90], lter [1521/1752] Loss: 31.1699\n",
      "Epoch [3/90], lter [1531/1752] Loss: 21.5421\n",
      "Epoch [3/90], lter [1541/1752] Loss: 43.3135\n",
      "Epoch [3/90], lter [1551/1752] Loss: 30.4694\n",
      "Epoch [3/90], lter [1561/1752] Loss: 23.8876\n",
      "Epoch [3/90], lter [1571/1752] Loss: 26.3604\n",
      "Epoch [3/90], lter [1581/1752] Loss: 19.4926\n",
      "Epoch [3/90], lter [1591/1752] Loss: 22.9325\n",
      "Epoch [3/90], lter [1601/1752] Loss: 25.1007\n",
      "Epoch [3/90], lter [1611/1752] Loss: 20.7872\n",
      "Epoch [3/90], lter [1621/1752] Loss: 32.1679\n",
      "Epoch [3/90], lter [1631/1752] Loss: 22.8736\n",
      "Epoch [3/90], lter [1641/1752] Loss: 47.4827\n",
      "Epoch [3/90], lter [1651/1752] Loss: 20.7728\n",
      "Epoch [3/90], lter [1661/1752] Loss: 29.5645\n",
      "Epoch [3/90], lter [1671/1752] Loss: 27.2076\n",
      "Epoch [3/90], lter [1681/1752] Loss: 32.9768\n",
      "Epoch [3/90], lter [1691/1752] Loss: 42.1581\n",
      "Epoch [3/90], lter [1701/1752] Loss: 29.7586\n",
      "Epoch [3/90], lter [1711/1752] Loss: 21.0536\n",
      "Epoch [3/90], lter [1721/1752] Loss: 27.3265\n",
      "Epoch [3/90], lter [1731/1752] Loss: 12.6599\n",
      "Epoch [3/90], lter [1741/1752] Loss: 24.8087\n",
      "Epoch [3/90], lter [1751/1752] Loss: 18.9307\n",
      "Epoch:  3 | train loss : 24.7855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/90 [63:17:12<1238:42:13, 51256.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 | test loss : 20.6165\n",
      "Epoch [4/90], lter [1/1752] Loss: 26.4278\n",
      "Epoch [4/90], lter [11/1752] Loss: 25.0309\n",
      "Epoch [4/90], lter [21/1752] Loss: 23.7912\n",
      "Epoch [4/90], lter [31/1752] Loss: 25.3325\n",
      "Epoch [4/90], lter [41/1752] Loss: 18.2146\n",
      "Epoch [4/90], lter [51/1752] Loss: 14.6777\n",
      "Epoch [4/90], lter [61/1752] Loss: 28.4314\n",
      "Epoch [4/90], lter [71/1752] Loss: 29.6847\n",
      "Epoch [4/90], lter [81/1752] Loss: 32.3684\n",
      "Epoch [4/90], lter [91/1752] Loss: 26.2000\n",
      "Epoch [4/90], lter [101/1752] Loss: 17.0259\n",
      "Epoch [4/90], lter [111/1752] Loss: 18.4322\n",
      "Epoch [4/90], lter [121/1752] Loss: 28.2356\n",
      "Epoch [4/90], lter [131/1752] Loss: 32.0105\n",
      "Epoch [4/90], lter [141/1752] Loss: 29.1251\n",
      "Epoch [4/90], lter [151/1752] Loss: 28.6870\n",
      "Epoch [4/90], lter [161/1752] Loss: 18.0427\n",
      "Epoch [4/90], lter [171/1752] Loss: 16.5183\n",
      "Epoch [4/90], lter [181/1752] Loss: 27.8936\n",
      "Epoch [4/90], lter [191/1752] Loss: 24.1293\n",
      "Epoch [4/90], lter [201/1752] Loss: 35.5847\n",
      "Epoch [4/90], lter [211/1752] Loss: 25.1418\n",
      "Epoch [4/90], lter [221/1752] Loss: 15.4594\n",
      "Epoch [4/90], lter [231/1752] Loss: 32.4419\n",
      "Epoch [4/90], lter [241/1752] Loss: 26.2222\n",
      "Epoch [4/90], lter [251/1752] Loss: 29.6940\n",
      "Epoch [4/90], lter [261/1752] Loss: 25.3995\n",
      "Epoch [4/90], lter [271/1752] Loss: 22.1583\n",
      "Epoch [4/90], lter [281/1752] Loss: 21.4788\n",
      "Epoch [4/90], lter [291/1752] Loss: 32.5907\n",
      "Epoch [4/90], lter [301/1752] Loss: 24.1292\n",
      "Epoch [4/90], lter [311/1752] Loss: 22.2415\n",
      "Epoch [4/90], lter [321/1752] Loss: 31.3409\n",
      "Epoch [4/90], lter [331/1752] Loss: 26.1995\n",
      "Epoch [4/90], lter [341/1752] Loss: 30.3699\n",
      "Epoch [4/90], lter [351/1752] Loss: 22.1927\n",
      "Epoch [4/90], lter [361/1752] Loss: 28.0818\n",
      "Epoch [4/90], lter [371/1752] Loss: 27.1951\n",
      "Epoch [4/90], lter [381/1752] Loss: 29.1498\n",
      "Epoch [4/90], lter [391/1752] Loss: 25.5233\n",
      "Epoch [4/90], lter [401/1752] Loss: 20.9013\n",
      "Epoch [4/90], lter [411/1752] Loss: 37.7515\n",
      "Epoch [4/90], lter [421/1752] Loss: 13.7037\n",
      "Epoch [4/90], lter [431/1752] Loss: 17.3997\n",
      "Epoch [4/90], lter [441/1752] Loss: 30.5686\n",
      "Epoch [4/90], lter [451/1752] Loss: 16.5454\n",
      "Epoch [4/90], lter [461/1752] Loss: 22.9500\n",
      "Epoch [4/90], lter [471/1752] Loss: 23.1221\n",
      "Epoch [4/90], lter [481/1752] Loss: 29.2790\n",
      "Epoch [4/90], lter [491/1752] Loss: 28.5977\n",
      "Epoch [4/90], lter [501/1752] Loss: 18.2080\n",
      "Epoch [4/90], lter [511/1752] Loss: 18.4793\n",
      "Epoch [4/90], lter [521/1752] Loss: 34.1174\n",
      "Epoch [4/90], lter [531/1752] Loss: 27.5201\n",
      "Epoch [4/90], lter [541/1752] Loss: 24.1195\n",
      "Epoch [4/90], lter [551/1752] Loss: 20.5993\n",
      "Epoch [4/90], lter [561/1752] Loss: 18.4834\n",
      "Epoch [4/90], lter [571/1752] Loss: 23.0753\n",
      "Epoch [4/90], lter [581/1752] Loss: 21.0732\n",
      "Epoch [4/90], lter [591/1752] Loss: 23.2050\n",
      "Epoch [4/90], lter [601/1752] Loss: 19.7390\n",
      "Epoch [4/90], lter [611/1752] Loss: 22.3969\n",
      "Epoch [4/90], lter [621/1752] Loss: 34.7413\n",
      "Epoch [4/90], lter [631/1752] Loss: 25.2192\n",
      "Epoch [4/90], lter [641/1752] Loss: 18.2620\n",
      "Epoch [4/90], lter [651/1752] Loss: 22.7305\n",
      "Epoch [4/90], lter [661/1752] Loss: 35.4795\n",
      "Epoch [4/90], lter [671/1752] Loss: 19.0277\n",
      "Epoch [4/90], lter [681/1752] Loss: 31.5641\n",
      "Epoch [4/90], lter [691/1752] Loss: 39.6006\n",
      "Epoch [4/90], lter [701/1752] Loss: 18.6223\n",
      "Epoch [4/90], lter [711/1752] Loss: 20.4722\n",
      "Epoch [4/90], lter [721/1752] Loss: 34.3712\n",
      "Epoch [4/90], lter [731/1752] Loss: 15.4972\n",
      "Epoch [4/90], lter [741/1752] Loss: 26.3545\n",
      "Epoch [4/90], lter [751/1752] Loss: 30.1664\n",
      "Epoch [4/90], lter [761/1752] Loss: 23.8265\n",
      "Epoch [4/90], lter [771/1752] Loss: 33.3380\n",
      "Epoch [4/90], lter [781/1752] Loss: 10.6877\n",
      "Epoch [4/90], lter [791/1752] Loss: 36.0141\n",
      "Epoch [4/90], lter [801/1752] Loss: 20.1523\n",
      "Epoch [4/90], lter [811/1752] Loss: 26.2666\n",
      "Epoch [4/90], lter [821/1752] Loss: 21.2940\n",
      "Epoch [4/90], lter [831/1752] Loss: 23.9067\n",
      "Epoch [4/90], lter [841/1752] Loss: 15.1838\n",
      "Epoch [4/90], lter [851/1752] Loss: 16.3390\n",
      "Epoch [4/90], lter [861/1752] Loss: 23.8786\n",
      "Epoch [4/90], lter [871/1752] Loss: 27.3006\n",
      "Epoch [4/90], lter [881/1752] Loss: 23.3441\n",
      "Epoch [4/90], lter [891/1752] Loss: 38.7007\n",
      "Epoch [4/90], lter [901/1752] Loss: 16.6360\n",
      "Epoch [4/90], lter [911/1752] Loss: 15.6370\n",
      "Epoch [4/90], lter [921/1752] Loss: 26.8586\n",
      "Epoch [4/90], lter [931/1752] Loss: 19.2689\n",
      "Epoch [4/90], lter [941/1752] Loss: 26.5983\n",
      "Epoch [4/90], lter [951/1752] Loss: 20.4310\n",
      "Epoch [4/90], lter [961/1752] Loss: 25.8185\n",
      "Epoch [4/90], lter [971/1752] Loss: 24.8235\n",
      "Epoch [4/90], lter [981/1752] Loss: 29.5796\n",
      "Epoch [4/90], lter [991/1752] Loss: 21.5777\n",
      "Epoch [4/90], lter [1001/1752] Loss: 25.5476\n",
      "Epoch [4/90], lter [1011/1752] Loss: 30.6930\n",
      "Epoch [4/90], lter [1021/1752] Loss: 22.5818\n",
      "Epoch [4/90], lter [1031/1752] Loss: 20.9752\n",
      "Epoch [4/90], lter [1041/1752] Loss: 11.3783\n",
      "Epoch [4/90], lter [1051/1752] Loss: 25.7443\n",
      "Epoch [4/90], lter [1061/1752] Loss: 36.8205\n",
      "Epoch [4/90], lter [1071/1752] Loss: 26.3242\n",
      "Epoch [4/90], lter [1081/1752] Loss: 22.2763\n",
      "Epoch [4/90], lter [1091/1752] Loss: 24.9703\n",
      "Epoch [4/90], lter [1101/1752] Loss: 15.2489\n",
      "Epoch [4/90], lter [1111/1752] Loss: 27.0102\n",
      "Epoch [4/90], lter [1121/1752] Loss: 20.9692\n",
      "Epoch [4/90], lter [1131/1752] Loss: 31.9129\n",
      "Epoch [4/90], lter [1141/1752] Loss: 33.2321\n",
      "Epoch [4/90], lter [1151/1752] Loss: 14.8605\n",
      "Epoch [4/90], lter [1161/1752] Loss: 18.3164\n",
      "Epoch [4/90], lter [1171/1752] Loss: 36.3083\n",
      "Epoch [4/90], lter [1181/1752] Loss: 27.7832\n",
      "Epoch [4/90], lter [1191/1752] Loss: 36.9829\n",
      "Epoch [4/90], lter [1201/1752] Loss: 31.0632\n",
      "Epoch [4/90], lter [1211/1752] Loss: 19.1579\n",
      "Epoch [4/90], lter [1221/1752] Loss: 17.7264\n",
      "Epoch [4/90], lter [1231/1752] Loss: 25.1009\n",
      "Epoch [4/90], lter [1241/1752] Loss: 31.0010\n",
      "Epoch [4/90], lter [1251/1752] Loss: 30.3171\n",
      "Epoch [4/90], lter [1261/1752] Loss: 18.2858\n",
      "Epoch [4/90], lter [1271/1752] Loss: 27.3940\n",
      "Epoch [4/90], lter [1281/1752] Loss: 16.4146\n",
      "Epoch [4/90], lter [1291/1752] Loss: 19.7437\n",
      "Epoch [4/90], lter [1301/1752] Loss: 22.1381\n",
      "Epoch [4/90], lter [1311/1752] Loss: 20.7804\n",
      "Epoch [4/90], lter [1321/1752] Loss: 21.1087\n",
      "Epoch [4/90], lter [1331/1752] Loss: 19.7691\n",
      "Epoch [4/90], lter [1341/1752] Loss: 16.3663\n",
      "Epoch [4/90], lter [1351/1752] Loss: 20.7838\n",
      "Epoch [4/90], lter [1361/1752] Loss: 16.4829\n",
      "Epoch [4/90], lter [1371/1752] Loss: 44.0935\n",
      "Epoch [4/90], lter [1381/1752] Loss: 28.6773\n",
      "Epoch [4/90], lter [1391/1752] Loss: 29.6655\n",
      "Epoch [4/90], lter [1401/1752] Loss: 22.4245\n",
      "Epoch [4/90], lter [1411/1752] Loss: 23.4140\n",
      "Epoch [4/90], lter [1421/1752] Loss: 17.1188\n",
      "Epoch [4/90], lter [1431/1752] Loss: 30.9581\n",
      "Epoch [4/90], lter [1441/1752] Loss: 25.8966\n",
      "Epoch [4/90], lter [1451/1752] Loss: 18.8531\n",
      "Epoch [4/90], lter [1461/1752] Loss: 23.5217\n",
      "Epoch [4/90], lter [1471/1752] Loss: 30.0339\n",
      "Epoch [4/90], lter [1481/1752] Loss: 20.8010\n",
      "Epoch [4/90], lter [1491/1752] Loss: 13.8980\n",
      "Epoch [4/90], lter [1501/1752] Loss: 16.6890\n",
      "Epoch [4/90], lter [1511/1752] Loss: 22.3163\n",
      "Epoch [4/90], lter [1521/1752] Loss: 24.5846\n",
      "Epoch [4/90], lter [1531/1752] Loss: 14.3360\n",
      "Epoch [4/90], lter [1541/1752] Loss: 12.3179\n",
      "Epoch [4/90], lter [1551/1752] Loss: 20.2297\n",
      "Epoch [4/90], lter [1561/1752] Loss: 21.6925\n",
      "Epoch [4/90], lter [1571/1752] Loss: 25.0647\n",
      "Epoch [4/90], lter [1581/1752] Loss: 27.5125\n",
      "Epoch [4/90], lter [1591/1752] Loss: 21.0951\n",
      "Epoch [4/90], lter [1601/1752] Loss: 35.7618\n",
      "Epoch [4/90], lter [1611/1752] Loss: 10.5341\n",
      "Epoch [4/90], lter [1621/1752] Loss: 16.9415\n",
      "Epoch [4/90], lter [1631/1752] Loss: 21.9458\n",
      "Epoch [4/90], lter [1641/1752] Loss: 24.2852\n",
      "Epoch [4/90], lter [1651/1752] Loss: 21.5509\n",
      "Epoch [4/90], lter [1661/1752] Loss: 14.3758\n",
      "Epoch [4/90], lter [1671/1752] Loss: 19.5235\n",
      "Epoch [4/90], lter [1681/1752] Loss: 16.8312\n",
      "Epoch [4/90], lter [1691/1752] Loss: 31.1854\n",
      "Epoch [4/90], lter [1701/1752] Loss: 34.6062\n",
      "Epoch [4/90], lter [1711/1752] Loss: 23.4154\n",
      "Epoch [4/90], lter [1721/1752] Loss: 18.9331\n",
      "Epoch [4/90], lter [1731/1752] Loss: 26.2828\n",
      "Epoch [4/90], lter [1741/1752] Loss: 31.4924\n",
      "Epoch [4/90], lter [1751/1752] Loss: 22.2746\n",
      "Epoch:  4 | train loss : 24.5238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/90 [63:31:19<749:03:03, 31355.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 | test loss : 15.5317\n",
      "Epoch [5/90], lter [1/1752] Loss: 25.4480\n",
      "Epoch [5/90], lter [11/1752] Loss: 58.0671\n",
      "Epoch [5/90], lter [21/1752] Loss: 19.4602\n",
      "Epoch [5/90], lter [31/1752] Loss: 32.4469\n",
      "Epoch [5/90], lter [41/1752] Loss: 16.2123\n",
      "Epoch [5/90], lter [51/1752] Loss: 20.0197\n",
      "Epoch [5/90], lter [61/1752] Loss: 19.4273\n",
      "Epoch [5/90], lter [71/1752] Loss: 13.8857\n",
      "Epoch [5/90], lter [81/1752] Loss: 18.7677\n",
      "Epoch [5/90], lter [91/1752] Loss: 17.4479\n",
      "Epoch [5/90], lter [101/1752] Loss: 28.5417\n",
      "Epoch [5/90], lter [111/1752] Loss: 20.7776\n",
      "Epoch [5/90], lter [121/1752] Loss: 22.1363\n",
      "Epoch [5/90], lter [131/1752] Loss: 28.0989\n",
      "Epoch [5/90], lter [141/1752] Loss: 31.9987\n",
      "Epoch [5/90], lter [151/1752] Loss: 18.5135\n",
      "Epoch [5/90], lter [161/1752] Loss: 26.9769\n",
      "Epoch [5/90], lter [171/1752] Loss: 21.2462\n",
      "Epoch [5/90], lter [181/1752] Loss: 12.9461\n",
      "Epoch [5/90], lter [191/1752] Loss: 26.3629\n",
      "Epoch [5/90], lter [201/1752] Loss: 18.5683\n",
      "Epoch [5/90], lter [211/1752] Loss: 11.7748\n",
      "Epoch [5/90], lter [221/1752] Loss: 37.6080\n",
      "Epoch [5/90], lter [231/1752] Loss: 23.2986\n",
      "Epoch [5/90], lter [241/1752] Loss: 40.4690\n",
      "Epoch [5/90], lter [251/1752] Loss: 35.6285\n",
      "Epoch [5/90], lter [261/1752] Loss: 19.5320\n",
      "Epoch [5/90], lter [271/1752] Loss: 32.4738\n",
      "Epoch [5/90], lter [281/1752] Loss: 16.4570\n",
      "Epoch [5/90], lter [291/1752] Loss: 18.6158\n",
      "Epoch [5/90], lter [301/1752] Loss: 31.7487\n",
      "Epoch [5/90], lter [311/1752] Loss: 23.9135\n",
      "Epoch [5/90], lter [321/1752] Loss: 20.2024\n",
      "Epoch [5/90], lter [331/1752] Loss: 28.9540\n",
      "Epoch [5/90], lter [341/1752] Loss: 31.4162\n",
      "Epoch [5/90], lter [351/1752] Loss: 20.0633\n",
      "Epoch [5/90], lter [361/1752] Loss: 19.1883\n",
      "Epoch [5/90], lter [371/1752] Loss: 27.8633\n",
      "Epoch [5/90], lter [381/1752] Loss: 9.4264\n",
      "Epoch [5/90], lter [391/1752] Loss: 20.6794\n",
      "Epoch [5/90], lter [401/1752] Loss: 28.1976\n",
      "Epoch [5/90], lter [411/1752] Loss: 23.4584\n",
      "Epoch [5/90], lter [421/1752] Loss: 26.0755\n",
      "Epoch [5/90], lter [431/1752] Loss: 37.9327\n",
      "Epoch [5/90], lter [441/1752] Loss: 11.6228\n",
      "Epoch [5/90], lter [451/1752] Loss: 22.2873\n",
      "Epoch [5/90], lter [461/1752] Loss: 34.0645\n",
      "Epoch [5/90], lter [471/1752] Loss: 20.5389\n",
      "Epoch [5/90], lter [481/1752] Loss: 23.0536\n",
      "Epoch [5/90], lter [491/1752] Loss: 19.3136\n",
      "Epoch [5/90], lter [501/1752] Loss: 12.3262\n",
      "Epoch [5/90], lter [511/1752] Loss: 24.4439\n",
      "Epoch [5/90], lter [521/1752] Loss: 28.1128\n",
      "Epoch [5/90], lter [531/1752] Loss: 17.5750\n",
      "Epoch [5/90], lter [541/1752] Loss: 21.5601\n",
      "Epoch [5/90], lter [551/1752] Loss: 27.6264\n",
      "Epoch [5/90], lter [561/1752] Loss: 30.0143\n",
      "Epoch [5/90], lter [571/1752] Loss: 31.4192\n",
      "Epoch [5/90], lter [581/1752] Loss: 27.3862\n",
      "Epoch [5/90], lter [591/1752] Loss: 16.2314\n",
      "Epoch [5/90], lter [601/1752] Loss: 21.0858\n",
      "Epoch [5/90], lter [611/1752] Loss: 27.1868\n",
      "Epoch [5/90], lter [621/1752] Loss: 22.1571\n",
      "Epoch [5/90], lter [631/1752] Loss: 23.1300\n",
      "Epoch [5/90], lter [641/1752] Loss: 14.2624\n",
      "Epoch [5/90], lter [651/1752] Loss: 28.4778\n",
      "Epoch [5/90], lter [661/1752] Loss: 17.7948\n",
      "Epoch [5/90], lter [671/1752] Loss: 12.9425\n",
      "Epoch [5/90], lter [681/1752] Loss: 17.5722\n",
      "Epoch [5/90], lter [691/1752] Loss: 17.8579\n",
      "Epoch [5/90], lter [701/1752] Loss: 31.6859\n",
      "Epoch [5/90], lter [711/1752] Loss: 21.1962\n",
      "Epoch [5/90], lter [721/1752] Loss: 47.1942\n",
      "Epoch [5/90], lter [731/1752] Loss: 13.6983\n",
      "Epoch [5/90], lter [741/1752] Loss: 33.3439\n",
      "Epoch [5/90], lter [751/1752] Loss: 27.8044\n",
      "Epoch [5/90], lter [761/1752] Loss: 23.3875\n",
      "Epoch [5/90], lter [771/1752] Loss: 34.3129\n",
      "Epoch [5/90], lter [781/1752] Loss: 21.4155\n",
      "Epoch [5/90], lter [791/1752] Loss: 16.0161\n",
      "Epoch [5/90], lter [801/1752] Loss: 23.0365\n",
      "Epoch [5/90], lter [811/1752] Loss: 29.0096\n",
      "Epoch [5/90], lter [821/1752] Loss: 16.6751\n",
      "Epoch [5/90], lter [831/1752] Loss: 12.4215\n",
      "Epoch [5/90], lter [841/1752] Loss: 13.3671\n",
      "Epoch [5/90], lter [851/1752] Loss: 21.2493\n",
      "Epoch [5/90], lter [861/1752] Loss: 34.0621\n",
      "Epoch [5/90], lter [871/1752] Loss: 19.9612\n",
      "Epoch [5/90], lter [881/1752] Loss: 30.4183\n",
      "Epoch [5/90], lter [891/1752] Loss: 15.9361\n",
      "Epoch [5/90], lter [901/1752] Loss: 17.2074\n",
      "Epoch [5/90], lter [911/1752] Loss: 24.1569\n",
      "Epoch [5/90], lter [921/1752] Loss: 40.3326\n",
      "Epoch [5/90], lter [931/1752] Loss: 32.1717\n",
      "Epoch [5/90], lter [941/1752] Loss: 38.3144\n",
      "Epoch [5/90], lter [951/1752] Loss: 14.2513\n",
      "Epoch [5/90], lter [961/1752] Loss: 21.0585\n",
      "Epoch [5/90], lter [971/1752] Loss: 24.1969\n",
      "Epoch [5/90], lter [981/1752] Loss: 21.5777\n",
      "Epoch [5/90], lter [991/1752] Loss: 28.0398\n",
      "Epoch [5/90], lter [1001/1752] Loss: 38.3458\n",
      "Epoch [5/90], lter [1011/1752] Loss: 24.7047\n",
      "Epoch [5/90], lter [1021/1752] Loss: 28.1562\n",
      "Epoch [5/90], lter [1031/1752] Loss: 21.1373\n",
      "Epoch [5/90], lter [1041/1752] Loss: 27.5560\n",
      "Epoch [5/90], lter [1051/1752] Loss: 30.8507\n",
      "Epoch [5/90], lter [1061/1752] Loss: 17.1468\n",
      "Epoch [5/90], lter [1071/1752] Loss: 16.3674\n",
      "Epoch [5/90], lter [1081/1752] Loss: 20.8269\n",
      "Epoch [5/90], lter [1091/1752] Loss: 25.7113\n",
      "Epoch [5/90], lter [1101/1752] Loss: 27.3314\n",
      "Epoch [5/90], lter [1111/1752] Loss: 20.8061\n",
      "Epoch [5/90], lter [1121/1752] Loss: 13.6809\n",
      "Epoch [5/90], lter [1131/1752] Loss: 27.0702\n",
      "Epoch [5/90], lter [1141/1752] Loss: 22.6938\n",
      "Epoch [5/90], lter [1151/1752] Loss: 28.1836\n",
      "Epoch [5/90], lter [1161/1752] Loss: 22.6822\n",
      "Epoch [5/90], lter [1171/1752] Loss: 21.8625\n",
      "Epoch [5/90], lter [1181/1752] Loss: 22.8873\n",
      "Epoch [5/90], lter [1191/1752] Loss: 37.3592\n",
      "Epoch [5/90], lter [1201/1752] Loss: 20.8935\n",
      "Epoch [5/90], lter [1211/1752] Loss: 26.9583\n",
      "Epoch [5/90], lter [1221/1752] Loss: 19.2795\n",
      "Epoch [5/90], lter [1231/1752] Loss: 21.9866\n",
      "Epoch [5/90], lter [1241/1752] Loss: 20.0450\n",
      "Epoch [5/90], lter [1251/1752] Loss: 16.3639\n",
      "Epoch [5/90], lter [1261/1752] Loss: 15.2615\n",
      "Epoch [5/90], lter [1271/1752] Loss: 16.6786\n",
      "Epoch [5/90], lter [1281/1752] Loss: 32.2358\n",
      "Epoch [5/90], lter [1291/1752] Loss: 25.8639\n",
      "Epoch [5/90], lter [1301/1752] Loss: 24.2700\n",
      "Epoch [5/90], lter [1311/1752] Loss: 24.9826\n",
      "Epoch [5/90], lter [1321/1752] Loss: 29.4196\n",
      "Epoch [5/90], lter [1331/1752] Loss: 17.1526\n",
      "Epoch [5/90], lter [1341/1752] Loss: 32.6028\n",
      "Epoch [5/90], lter [1351/1752] Loss: 29.9120\n",
      "Epoch [5/90], lter [1361/1752] Loss: 21.1596\n",
      "Epoch [5/90], lter [1371/1752] Loss: 17.1151\n",
      "Epoch [5/90], lter [1381/1752] Loss: 17.3416\n",
      "Epoch [5/90], lter [1391/1752] Loss: 25.9882\n",
      "Epoch [5/90], lter [1401/1752] Loss: 23.7460\n",
      "Epoch [5/90], lter [1411/1752] Loss: 20.6132\n",
      "Epoch [5/90], lter [1421/1752] Loss: 21.5251\n",
      "Epoch [5/90], lter [1431/1752] Loss: 51.5258\n",
      "Epoch [5/90], lter [1441/1752] Loss: 19.4334\n",
      "Epoch [5/90], lter [1451/1752] Loss: 14.2478\n",
      "Epoch [5/90], lter [1461/1752] Loss: 18.1102\n",
      "Epoch [5/90], lter [1471/1752] Loss: 17.0388\n",
      "Epoch [5/90], lter [1481/1752] Loss: 20.0543\n",
      "Epoch [5/90], lter [1491/1752] Loss: 32.7203\n",
      "Epoch [5/90], lter [1501/1752] Loss: 23.0378\n",
      "Epoch [5/90], lter [1511/1752] Loss: 27.0745\n",
      "Epoch [5/90], lter [1521/1752] Loss: 28.2237\n",
      "Epoch [5/90], lter [1531/1752] Loss: 19.0629\n",
      "Epoch [5/90], lter [1541/1752] Loss: 25.0073\n",
      "Epoch [5/90], lter [1551/1752] Loss: 32.1222\n",
      "Epoch [5/90], lter [1561/1752] Loss: 30.9576\n",
      "Epoch [5/90], lter [1571/1752] Loss: 24.2870\n",
      "Epoch [5/90], lter [1581/1752] Loss: 25.1729\n",
      "Epoch [5/90], lter [1591/1752] Loss: 20.9003\n",
      "Epoch [5/90], lter [1601/1752] Loss: 21.5388\n",
      "Epoch [5/90], lter [1611/1752] Loss: 27.4293\n",
      "Epoch [5/90], lter [1621/1752] Loss: 25.4802\n",
      "Epoch [5/90], lter [1631/1752] Loss: 26.5512\n",
      "Epoch [5/90], lter [1641/1752] Loss: 21.4531\n",
      "Epoch [5/90], lter [1651/1752] Loss: 20.8635\n",
      "Epoch [5/90], lter [1661/1752] Loss: 23.1679\n",
      "Epoch [5/90], lter [1671/1752] Loss: 24.6199\n",
      "Epoch [5/90], lter [1681/1752] Loss: 30.8380\n",
      "Epoch [5/90], lter [1691/1752] Loss: 20.8929\n",
      "Epoch [5/90], lter [1701/1752] Loss: 20.9834\n",
      "Epoch [5/90], lter [1711/1752] Loss: 24.4551\n",
      "Epoch [5/90], lter [1721/1752] Loss: 26.0235\n",
      "Epoch [5/90], lter [1731/1752] Loss: 22.5934\n",
      "Epoch [5/90], lter [1741/1752] Loss: 12.1203\n",
      "Epoch [5/90], lter [1751/1752] Loss: 27.2014\n",
      "Epoch:  5 | train loss : 24.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/90 [63:45:26<480:34:50, 20354.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 | test loss : 12.9112\n",
      "Epoch [6/90], lter [1/1752] Loss: 23.8563\n",
      "Epoch [6/90], lter [11/1752] Loss: 33.2844\n",
      "Epoch [6/90], lter [21/1752] Loss: 33.5816\n",
      "Epoch [6/90], lter [31/1752] Loss: 29.6207\n",
      "Epoch [6/90], lter [41/1752] Loss: 28.6036\n",
      "Epoch [6/90], lter [51/1752] Loss: 32.7378\n",
      "Epoch [6/90], lter [61/1752] Loss: 22.1302\n",
      "Epoch [6/90], lter [71/1752] Loss: 34.0496\n",
      "Epoch [6/90], lter [81/1752] Loss: 33.7976\n",
      "Epoch [6/90], lter [91/1752] Loss: 28.9813\n",
      "Epoch [6/90], lter [101/1752] Loss: 22.1138\n",
      "Epoch [6/90], lter [111/1752] Loss: 26.9622\n",
      "Epoch [6/90], lter [121/1752] Loss: 29.3331\n",
      "Epoch [6/90], lter [131/1752] Loss: 18.5533\n",
      "Epoch [6/90], lter [141/1752] Loss: 33.9267\n",
      "Epoch [6/90], lter [151/1752] Loss: 28.5964\n",
      "Epoch [6/90], lter [161/1752] Loss: 22.7055\n",
      "Epoch [6/90], lter [171/1752] Loss: 14.5250\n",
      "Epoch [6/90], lter [181/1752] Loss: 23.7865\n",
      "Epoch [6/90], lter [191/1752] Loss: 31.8457\n",
      "Epoch [6/90], lter [201/1752] Loss: 29.5798\n",
      "Epoch [6/90], lter [211/1752] Loss: 14.8172\n",
      "Epoch [6/90], lter [221/1752] Loss: 26.6534\n",
      "Epoch [6/90], lter [231/1752] Loss: 29.1636\n",
      "Epoch [6/90], lter [241/1752] Loss: 23.6766\n",
      "Epoch [6/90], lter [251/1752] Loss: 29.9302\n",
      "Epoch [6/90], lter [261/1752] Loss: 26.2048\n",
      "Epoch [6/90], lter [271/1752] Loss: 25.5781\n",
      "Epoch [6/90], lter [281/1752] Loss: 32.0132\n",
      "Epoch [6/90], lter [291/1752] Loss: 31.6987\n",
      "Epoch [6/90], lter [301/1752] Loss: 20.5266\n",
      "Epoch [6/90], lter [311/1752] Loss: 17.0602\n",
      "Epoch [6/90], lter [321/1752] Loss: 21.6843\n",
      "Epoch [6/90], lter [331/1752] Loss: 24.4454\n",
      "Epoch [6/90], lter [341/1752] Loss: 17.7649\n",
      "Epoch [6/90], lter [351/1752] Loss: 22.4365\n",
      "Epoch [6/90], lter [361/1752] Loss: 26.4747\n",
      "Epoch [6/90], lter [371/1752] Loss: 28.0848\n",
      "Epoch [6/90], lter [381/1752] Loss: 12.2219\n",
      "Epoch [6/90], lter [391/1752] Loss: 15.8696\n",
      "Epoch [6/90], lter [401/1752] Loss: 18.8976\n",
      "Epoch [6/90], lter [411/1752] Loss: 28.4049\n",
      "Epoch [6/90], lter [421/1752] Loss: 23.8044\n",
      "Epoch [6/90], lter [431/1752] Loss: 20.3096\n",
      "Epoch [6/90], lter [441/1752] Loss: 20.0614\n",
      "Epoch [6/90], lter [451/1752] Loss: 13.8364\n",
      "Epoch [6/90], lter [461/1752] Loss: 33.2997\n",
      "Epoch [6/90], lter [471/1752] Loss: 25.9827\n",
      "Epoch [6/90], lter [481/1752] Loss: 22.1449\n",
      "Epoch [6/90], lter [491/1752] Loss: 22.4257\n",
      "Epoch [6/90], lter [501/1752] Loss: 11.6319\n",
      "Epoch [6/90], lter [511/1752] Loss: 33.4901\n",
      "Epoch [6/90], lter [521/1752] Loss: 26.2658\n",
      "Epoch [6/90], lter [531/1752] Loss: 16.5724\n",
      "Epoch [6/90], lter [541/1752] Loss: 17.8047\n",
      "Epoch [6/90], lter [551/1752] Loss: 18.4294\n",
      "Epoch [6/90], lter [561/1752] Loss: 41.4452\n",
      "Epoch [6/90], lter [571/1752] Loss: 22.2357\n",
      "Epoch [6/90], lter [581/1752] Loss: 25.9311\n",
      "Epoch [6/90], lter [591/1752] Loss: 23.3098\n",
      "Epoch [6/90], lter [601/1752] Loss: 29.8502\n",
      "Epoch [6/90], lter [611/1752] Loss: 20.6245\n",
      "Epoch [6/90], lter [621/1752] Loss: 13.6254\n",
      "Epoch [6/90], lter [631/1752] Loss: 17.2854\n",
      "Epoch [6/90], lter [641/1752] Loss: 27.9272\n",
      "Epoch [6/90], lter [651/1752] Loss: 8.0116\n",
      "Epoch [6/90], lter [661/1752] Loss: 26.2421\n",
      "Epoch [6/90], lter [671/1752] Loss: 20.7312\n",
      "Epoch [6/90], lter [681/1752] Loss: 20.5151\n",
      "Epoch [6/90], lter [691/1752] Loss: 30.9657\n",
      "Epoch [6/90], lter [701/1752] Loss: 33.8165\n",
      "Epoch [6/90], lter [711/1752] Loss: 26.8477\n",
      "Epoch [6/90], lter [721/1752] Loss: 17.4854\n",
      "Epoch [6/90], lter [731/1752] Loss: 19.4803\n",
      "Epoch [6/90], lter [741/1752] Loss: 24.2005\n",
      "Epoch [6/90], lter [751/1752] Loss: 12.0049\n",
      "Epoch [6/90], lter [761/1752] Loss: 24.0315\n",
      "Epoch [6/90], lter [771/1752] Loss: 28.8557\n",
      "Epoch [6/90], lter [781/1752] Loss: 39.9218\n",
      "Epoch [6/90], lter [791/1752] Loss: 23.2732\n",
      "Epoch [6/90], lter [801/1752] Loss: 9.9682\n",
      "Epoch [6/90], lter [811/1752] Loss: 33.8503\n",
      "Epoch [6/90], lter [821/1752] Loss: 20.4571\n",
      "Epoch [6/90], lter [831/1752] Loss: 17.3584\n",
      "Epoch [6/90], lter [841/1752] Loss: 24.0137\n",
      "Epoch [6/90], lter [851/1752] Loss: 22.8492\n",
      "Epoch [6/90], lter [861/1752] Loss: 22.4330\n",
      "Epoch [6/90], lter [871/1752] Loss: 27.0272\n",
      "Epoch [6/90], lter [881/1752] Loss: 18.4810\n",
      "Epoch [6/90], lter [891/1752] Loss: 31.4268\n",
      "Epoch [6/90], lter [901/1752] Loss: 23.6368\n",
      "Epoch [6/90], lter [911/1752] Loss: 23.0652\n",
      "Epoch [6/90], lter [921/1752] Loss: 28.4601\n",
      "Epoch [6/90], lter [931/1752] Loss: 23.3903\n",
      "Epoch [6/90], lter [941/1752] Loss: 23.5660\n",
      "Epoch [6/90], lter [951/1752] Loss: 21.5272\n",
      "Epoch [6/90], lter [961/1752] Loss: 20.0942\n",
      "Epoch [6/90], lter [971/1752] Loss: 21.5989\n",
      "Epoch [6/90], lter [981/1752] Loss: 31.2521\n",
      "Epoch [6/90], lter [991/1752] Loss: 28.0993\n",
      "Epoch [6/90], lter [1001/1752] Loss: 18.7140\n",
      "Epoch [6/90], lter [1011/1752] Loss: 37.9210\n",
      "Epoch [6/90], lter [1021/1752] Loss: 40.8412\n",
      "Epoch [6/90], lter [1031/1752] Loss: 32.6854\n",
      "Epoch [6/90], lter [1041/1752] Loss: 33.5528\n",
      "Epoch [6/90], lter [1051/1752] Loss: 23.6498\n",
      "Epoch [6/90], lter [1061/1752] Loss: 37.9568\n",
      "Epoch [6/90], lter [1071/1752] Loss: 26.3221\n",
      "Epoch [6/90], lter [1081/1752] Loss: 22.0455\n",
      "Epoch [6/90], lter [1091/1752] Loss: 20.3891\n",
      "Epoch [6/90], lter [1101/1752] Loss: 36.8119\n",
      "Epoch [6/90], lter [1111/1752] Loss: 20.0309\n",
      "Epoch [6/90], lter [1121/1752] Loss: 21.2067\n",
      "Epoch [6/90], lter [1131/1752] Loss: 15.3463\n",
      "Epoch [6/90], lter [1141/1752] Loss: 30.0019\n",
      "Epoch [6/90], lter [1151/1752] Loss: 24.6967\n",
      "Epoch [6/90], lter [1161/1752] Loss: 26.8888\n",
      "Epoch [6/90], lter [1171/1752] Loss: 23.3670\n",
      "Epoch [6/90], lter [1181/1752] Loss: 26.3288\n",
      "Epoch [6/90], lter [1191/1752] Loss: 19.2647\n",
      "Epoch [6/90], lter [1201/1752] Loss: 17.0334\n",
      "Epoch [6/90], lter [1211/1752] Loss: 27.0726\n",
      "Epoch [6/90], lter [1221/1752] Loss: 29.3490\n",
      "Epoch [6/90], lter [1231/1752] Loss: 35.9346\n",
      "Epoch [6/90], lter [1241/1752] Loss: 22.3263\n",
      "Epoch [6/90], lter [1251/1752] Loss: 23.1076\n",
      "Epoch [6/90], lter [1261/1752] Loss: 16.9011\n",
      "Epoch [6/90], lter [1271/1752] Loss: 28.9267\n",
      "Epoch [6/90], lter [1281/1752] Loss: 31.4821\n",
      "Epoch [6/90], lter [1291/1752] Loss: 26.7015\n",
      "Epoch [6/90], lter [1301/1752] Loss: 18.7589\n",
      "Epoch [6/90], lter [1311/1752] Loss: 16.0064\n",
      "Epoch [6/90], lter [1321/1752] Loss: 22.2829\n",
      "Epoch [6/90], lter [1331/1752] Loss: 24.1539\n",
      "Epoch [6/90], lter [1341/1752] Loss: 19.5461\n",
      "Epoch [6/90], lter [1351/1752] Loss: 24.8512\n",
      "Epoch [6/90], lter [1361/1752] Loss: 26.4448\n",
      "Epoch [6/90], lter [1371/1752] Loss: 28.6079\n",
      "Epoch [6/90], lter [1381/1752] Loss: 29.6748\n",
      "Epoch [6/90], lter [1391/1752] Loss: 13.6725\n",
      "Epoch [6/90], lter [1401/1752] Loss: 10.3720\n",
      "Epoch [6/90], lter [1411/1752] Loss: 29.6982\n",
      "Epoch [6/90], lter [1421/1752] Loss: 20.8025\n",
      "Epoch [6/90], lter [1431/1752] Loss: 27.0252\n",
      "Epoch [6/90], lter [1441/1752] Loss: 20.5388\n",
      "Epoch [6/90], lter [1451/1752] Loss: 32.1984\n",
      "Epoch [6/90], lter [1461/1752] Loss: 25.9224\n",
      "Epoch [6/90], lter [1471/1752] Loss: 26.2716\n",
      "Epoch [6/90], lter [1481/1752] Loss: 26.0213\n",
      "Epoch [6/90], lter [1491/1752] Loss: 22.6236\n",
      "Epoch [6/90], lter [1501/1752] Loss: 43.7515\n",
      "Epoch [6/90], lter [1511/1752] Loss: 21.5810\n",
      "Epoch [6/90], lter [1521/1752] Loss: 22.3142\n",
      "Epoch [6/90], lter [1531/1752] Loss: 37.6307\n",
      "Epoch [6/90], lter [1541/1752] Loss: 15.3957\n",
      "Epoch [6/90], lter [1551/1752] Loss: 22.5832\n",
      "Epoch [6/90], lter [1561/1752] Loss: 21.6328\n",
      "Epoch [6/90], lter [1571/1752] Loss: 27.8386\n",
      "Epoch [6/90], lter [1581/1752] Loss: 16.0469\n",
      "Epoch [6/90], lter [1591/1752] Loss: 23.5912\n",
      "Epoch [6/90], lter [1601/1752] Loss: 27.7316\n",
      "Epoch [6/90], lter [1611/1752] Loss: 18.6167\n",
      "Epoch [6/90], lter [1621/1752] Loss: 27.8033\n",
      "Epoch [6/90], lter [1631/1752] Loss: 18.5559\n",
      "Epoch [6/90], lter [1641/1752] Loss: 25.5305\n",
      "Epoch [6/90], lter [1651/1752] Loss: 22.3872\n",
      "Epoch [6/90], lter [1661/1752] Loss: 15.4103\n",
      "Epoch [6/90], lter [1671/1752] Loss: 23.6531\n",
      "Epoch [6/90], lter [1681/1752] Loss: 24.8899\n",
      "Epoch [6/90], lter [1691/1752] Loss: 24.9785\n",
      "Epoch [6/90], lter [1701/1752] Loss: 18.1892\n",
      "Epoch [6/90], lter [1711/1752] Loss: 35.6338\n",
      "Epoch [6/90], lter [1721/1752] Loss: 23.6244\n",
      "Epoch [6/90], lter [1731/1752] Loss: 32.4151\n",
      "Epoch [6/90], lter [1741/1752] Loss: 26.1452\n",
      "Epoch [6/90], lter [1751/1752] Loss: 27.6806\n",
      "Epoch:  6 | train loss : 24.3959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/90 [63:59:33<320:10:16, 13721.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5 | test loss : 15.0533\n",
      "Epoch [7/90], lter [1/1752] Loss: 25.3170\n",
      "Epoch [7/90], lter [11/1752] Loss: 28.3433\n",
      "Epoch [7/90], lter [21/1752] Loss: 31.7258\n",
      "Epoch [7/90], lter [31/1752] Loss: 25.5570\n",
      "Epoch [7/90], lter [41/1752] Loss: 20.9049\n",
      "Epoch [7/90], lter [51/1752] Loss: 19.5210\n",
      "Epoch [7/90], lter [61/1752] Loss: 37.4359\n",
      "Epoch [7/90], lter [71/1752] Loss: 29.9196\n",
      "Epoch [7/90], lter [81/1752] Loss: 20.9519\n",
      "Epoch [7/90], lter [91/1752] Loss: 21.2747\n",
      "Epoch [7/90], lter [101/1752] Loss: 24.4028\n",
      "Epoch [7/90], lter [111/1752] Loss: 20.3582\n",
      "Epoch [7/90], lter [121/1752] Loss: 32.2940\n",
      "Epoch [7/90], lter [131/1752] Loss: 19.3523\n",
      "Epoch [7/90], lter [141/1752] Loss: 21.7510\n",
      "Epoch [7/90], lter [151/1752] Loss: 23.8393\n",
      "Epoch [7/90], lter [161/1752] Loss: 31.3689\n",
      "Epoch [7/90], lter [171/1752] Loss: 25.7497\n",
      "Epoch [7/90], lter [181/1752] Loss: 22.5232\n",
      "Epoch [7/90], lter [191/1752] Loss: 25.7543\n",
      "Epoch [7/90], lter [201/1752] Loss: 23.3422\n",
      "Epoch [7/90], lter [211/1752] Loss: 9.1711\n",
      "Epoch [7/90], lter [221/1752] Loss: 24.5736\n",
      "Epoch [7/90], lter [231/1752] Loss: 27.6154\n",
      "Epoch [7/90], lter [241/1752] Loss: 14.5612\n",
      "Epoch [7/90], lter [251/1752] Loss: 27.9076\n",
      "Epoch [7/90], lter [261/1752] Loss: 30.5890\n",
      "Epoch [7/90], lter [271/1752] Loss: 25.0020\n",
      "Epoch [7/90], lter [281/1752] Loss: 10.0486\n",
      "Epoch [7/90], lter [291/1752] Loss: 19.1279\n",
      "Epoch [7/90], lter [301/1752] Loss: 13.0750\n",
      "Epoch [7/90], lter [311/1752] Loss: 33.1140\n",
      "Epoch [7/90], lter [321/1752] Loss: 28.1640\n",
      "Epoch [7/90], lter [331/1752] Loss: 21.8130\n",
      "Epoch [7/90], lter [341/1752] Loss: 18.5773\n",
      "Epoch [7/90], lter [351/1752] Loss: 11.2795\n",
      "Epoch [7/90], lter [361/1752] Loss: 21.6026\n",
      "Epoch [7/90], lter [371/1752] Loss: 17.7734\n",
      "Epoch [7/90], lter [381/1752] Loss: 14.7787\n",
      "Epoch [7/90], lter [391/1752] Loss: 17.5873\n",
      "Epoch [7/90], lter [401/1752] Loss: 18.5283\n",
      "Epoch [7/90], lter [411/1752] Loss: 23.0994\n",
      "Epoch [7/90], lter [421/1752] Loss: 29.3260\n",
      "Epoch [7/90], lter [431/1752] Loss: 15.4068\n",
      "Epoch [7/90], lter [441/1752] Loss: 30.1029\n",
      "Epoch [7/90], lter [451/1752] Loss: 20.9330\n",
      "Epoch [7/90], lter [461/1752] Loss: 32.1196\n",
      "Epoch [7/90], lter [471/1752] Loss: 19.6855\n",
      "Epoch [7/90], lter [481/1752] Loss: 27.5585\n",
      "Epoch [7/90], lter [491/1752] Loss: 14.1292\n",
      "Epoch [7/90], lter [501/1752] Loss: 31.4229\n",
      "Epoch [7/90], lter [511/1752] Loss: 26.5312\n",
      "Epoch [7/90], lter [521/1752] Loss: 29.5025\n",
      "Epoch [7/90], lter [531/1752] Loss: 19.0608\n",
      "Epoch [7/90], lter [541/1752] Loss: 28.7401\n",
      "Epoch [7/90], lter [551/1752] Loss: 20.2639\n",
      "Epoch [7/90], lter [561/1752] Loss: 21.7812\n",
      "Epoch [7/90], lter [571/1752] Loss: 24.6342\n",
      "Epoch [7/90], lter [581/1752] Loss: 18.6324\n",
      "Epoch [7/90], lter [591/1752] Loss: 50.9796\n",
      "Epoch [7/90], lter [601/1752] Loss: 17.7078\n",
      "Epoch [7/90], lter [611/1752] Loss: 28.2949\n",
      "Epoch [7/90], lter [621/1752] Loss: 29.2974\n",
      "Epoch [7/90], lter [631/1752] Loss: 29.9734\n",
      "Epoch [7/90], lter [641/1752] Loss: 23.1315\n",
      "Epoch [7/90], lter [651/1752] Loss: 26.7953\n",
      "Epoch [7/90], lter [661/1752] Loss: 30.2410\n",
      "Epoch [7/90], lter [671/1752] Loss: 26.0343\n",
      "Epoch [7/90], lter [681/1752] Loss: 23.1157\n",
      "Epoch [7/90], lter [691/1752] Loss: 28.6890\n",
      "Epoch [7/90], lter [701/1752] Loss: 34.4290\n",
      "Epoch [7/90], lter [711/1752] Loss: 34.3346\n",
      "Epoch [7/90], lter [721/1752] Loss: 32.3077\n",
      "Epoch [7/90], lter [731/1752] Loss: 34.2671\n",
      "Epoch [7/90], lter [741/1752] Loss: 35.5613\n",
      "Epoch [7/90], lter [751/1752] Loss: 28.7551\n",
      "Epoch [7/90], lter [761/1752] Loss: 15.1683\n",
      "Epoch [7/90], lter [771/1752] Loss: 23.6364\n",
      "Epoch [7/90], lter [781/1752] Loss: 8.4365\n",
      "Epoch [7/90], lter [791/1752] Loss: 32.2617\n",
      "Epoch [7/90], lter [801/1752] Loss: 25.0512\n",
      "Epoch [7/90], lter [811/1752] Loss: 34.8693\n",
      "Epoch [7/90], lter [821/1752] Loss: 14.6386\n",
      "Epoch [7/90], lter [831/1752] Loss: 33.5101\n",
      "Epoch [7/90], lter [841/1752] Loss: 22.0373\n",
      "Epoch [7/90], lter [851/1752] Loss: 29.1386\n",
      "Epoch [7/90], lter [861/1752] Loss: 26.1148\n",
      "Epoch [7/90], lter [871/1752] Loss: 33.4791\n",
      "Epoch [7/90], lter [881/1752] Loss: 31.3414\n",
      "Epoch [7/90], lter [891/1752] Loss: 28.6970\n",
      "Epoch [7/90], lter [901/1752] Loss: 23.5122\n",
      "Epoch [7/90], lter [911/1752] Loss: 22.6696\n",
      "Epoch [7/90], lter [921/1752] Loss: 19.5095\n",
      "Epoch [7/90], lter [931/1752] Loss: 25.0525\n",
      "Epoch [7/90], lter [941/1752] Loss: 16.4663\n",
      "Epoch [7/90], lter [951/1752] Loss: 29.0159\n",
      "Epoch [7/90], lter [961/1752] Loss: 14.4398\n",
      "Epoch [7/90], lter [971/1752] Loss: 26.2924\n",
      "Epoch [7/90], lter [981/1752] Loss: 14.5072\n",
      "Epoch [7/90], lter [991/1752] Loss: 27.9712\n",
      "Epoch [7/90], lter [1001/1752] Loss: 16.6389\n",
      "Epoch [7/90], lter [1011/1752] Loss: 11.9237\n",
      "Epoch [7/90], lter [1021/1752] Loss: 25.5203\n",
      "Epoch [7/90], lter [1031/1752] Loss: 17.6124\n",
      "Epoch [7/90], lter [1041/1752] Loss: 25.9990\n",
      "Epoch [7/90], lter [1051/1752] Loss: 16.1164\n",
      "Epoch [7/90], lter [1061/1752] Loss: 28.0140\n",
      "Epoch [7/90], lter [1071/1752] Loss: 32.3689\n",
      "Epoch [7/90], lter [1081/1752] Loss: 21.1486\n",
      "Epoch [7/90], lter [1091/1752] Loss: 33.1570\n",
      "Epoch [7/90], lter [1101/1752] Loss: 24.2848\n",
      "Epoch [7/90], lter [1111/1752] Loss: 26.7930\n",
      "Epoch [7/90], lter [1121/1752] Loss: 26.4215\n",
      "Epoch [7/90], lter [1131/1752] Loss: 23.1618\n",
      "Epoch [7/90], lter [1141/1752] Loss: 33.6868\n",
      "Epoch [7/90], lter [1151/1752] Loss: 25.0821\n",
      "Epoch [7/90], lter [1161/1752] Loss: 25.3607\n",
      "Epoch [7/90], lter [1171/1752] Loss: 19.6181\n",
      "Epoch [7/90], lter [1181/1752] Loss: 23.5510\n",
      "Epoch [7/90], lter [1191/1752] Loss: 18.6899\n",
      "Epoch [7/90], lter [1201/1752] Loss: 24.2620\n",
      "Epoch [7/90], lter [1211/1752] Loss: 33.2437\n",
      "Epoch [7/90], lter [1221/1752] Loss: 40.2585\n",
      "Epoch [7/90], lter [1231/1752] Loss: 30.0216\n",
      "Epoch [7/90], lter [1241/1752] Loss: 12.4024\n",
      "Epoch [7/90], lter [1251/1752] Loss: 27.3312\n",
      "Epoch [7/90], lter [1261/1752] Loss: 43.0044\n",
      "Epoch [7/90], lter [1271/1752] Loss: 26.6146\n",
      "Epoch [7/90], lter [1281/1752] Loss: 30.3691\n",
      "Epoch [7/90], lter [1291/1752] Loss: 18.4197\n",
      "Epoch [7/90], lter [1301/1752] Loss: 20.2129\n",
      "Epoch [7/90], lter [1311/1752] Loss: 17.2362\n",
      "Epoch [7/90], lter [1321/1752] Loss: 35.1029\n",
      "Epoch [7/90], lter [1331/1752] Loss: 24.5798\n",
      "Epoch [7/90], lter [1341/1752] Loss: 31.7329\n",
      "Epoch [7/90], lter [1351/1752] Loss: 30.0724\n",
      "Epoch [7/90], lter [1361/1752] Loss: 25.0824\n",
      "Epoch [7/90], lter [1371/1752] Loss: 21.9603\n",
      "Epoch [7/90], lter [1381/1752] Loss: 28.6164\n",
      "Epoch [7/90], lter [1391/1752] Loss: 16.1106\n",
      "Epoch [7/90], lter [1401/1752] Loss: 35.9953\n",
      "Epoch [7/90], lter [1411/1752] Loss: 24.0662\n",
      "Epoch [7/90], lter [1421/1752] Loss: 24.9340\n",
      "Epoch [7/90], lter [1431/1752] Loss: 27.7032\n",
      "Epoch [7/90], lter [1441/1752] Loss: 25.9315\n",
      "Epoch [7/90], lter [1451/1752] Loss: 20.4636\n",
      "Epoch [7/90], lter [1461/1752] Loss: 32.3569\n",
      "Epoch [7/90], lter [1471/1752] Loss: 30.2491\n",
      "Epoch [7/90], lter [1481/1752] Loss: 24.7603\n",
      "Epoch [7/90], lter [1491/1752] Loss: 26.2143\n",
      "Epoch [7/90], lter [1501/1752] Loss: 18.6103\n",
      "Epoch [7/90], lter [1511/1752] Loss: 24.2044\n",
      "Epoch [7/90], lter [1521/1752] Loss: 20.5529\n",
      "Epoch [7/90], lter [1531/1752] Loss: 13.2169\n",
      "Epoch [7/90], lter [1541/1752] Loss: 39.3670\n",
      "Epoch [7/90], lter [1551/1752] Loss: 25.2582\n",
      "Epoch [7/90], lter [1561/1752] Loss: 23.6530\n",
      "Epoch [7/90], lter [1571/1752] Loss: 23.7879\n",
      "Epoch [7/90], lter [1581/1752] Loss: 15.4870\n",
      "Epoch [7/90], lter [1591/1752] Loss: 20.6248\n",
      "Epoch [7/90], lter [1601/1752] Loss: 21.9326\n",
      "Epoch [7/90], lter [1611/1752] Loss: 26.9217\n",
      "Epoch [7/90], lter [1621/1752] Loss: 22.3764\n",
      "Epoch [7/90], lter [1631/1752] Loss: 24.9848\n",
      "Epoch [7/90], lter [1641/1752] Loss: 29.3081\n",
      "Epoch [7/90], lter [1651/1752] Loss: 28.2565\n",
      "Epoch [7/90], lter [1661/1752] Loss: 20.4559\n",
      "Epoch [7/90], lter [1671/1752] Loss: 36.7603\n",
      "Epoch [7/90], lter [1681/1752] Loss: 33.4506\n",
      "Epoch [7/90], lter [1691/1752] Loss: 33.3268\n",
      "Epoch [7/90], lter [1701/1752] Loss: 30.7651\n",
      "Epoch [7/90], lter [1711/1752] Loss: 24.8381\n",
      "Epoch [7/90], lter [1721/1752] Loss: 24.9853\n",
      "Epoch [7/90], lter [1731/1752] Loss: 23.8389\n",
      "Epoch [7/90], lter [1741/1752] Loss: 27.2956\n",
      "Epoch [7/90], lter [1751/1752] Loss: 23.9960\n",
      "Epoch:  7 | train loss : 25.0920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7/90 [64:13:33<219:15:50, 9510.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6 | test loss : 15.8969\n",
      "Epoch [8/90], lter [1/1752] Loss: 20.6540\n",
      "Epoch [8/90], lter [11/1752] Loss: 21.9800\n",
      "Epoch [8/90], lter [21/1752] Loss: 21.2082\n",
      "Epoch [8/90], lter [31/1752] Loss: 25.1518\n",
      "Epoch [8/90], lter [41/1752] Loss: 21.5137\n",
      "Epoch [8/90], lter [51/1752] Loss: 18.5792\n",
      "Epoch [8/90], lter [61/1752] Loss: 11.3900\n",
      "Epoch [8/90], lter [71/1752] Loss: 18.2760\n",
      "Epoch [8/90], lter [81/1752] Loss: 13.6372\n",
      "Epoch [8/90], lter [91/1752] Loss: 33.0610\n",
      "Epoch [8/90], lter [101/1752] Loss: 23.0824\n",
      "Epoch [8/90], lter [111/1752] Loss: 29.1217\n",
      "Epoch [8/90], lter [121/1752] Loss: 19.6571\n",
      "Epoch [8/90], lter [131/1752] Loss: 22.4892\n",
      "Epoch [8/90], lter [141/1752] Loss: 28.2859\n",
      "Epoch [8/90], lter [151/1752] Loss: 20.0537\n",
      "Epoch [8/90], lter [161/1752] Loss: 30.0450\n",
      "Epoch [8/90], lter [171/1752] Loss: 26.2589\n",
      "Epoch [8/90], lter [181/1752] Loss: 44.1307\n",
      "Epoch [8/90], lter [191/1752] Loss: 18.9821\n",
      "Epoch [8/90], lter [201/1752] Loss: 16.7831\n",
      "Epoch [8/90], lter [211/1752] Loss: 29.6584\n",
      "Epoch [8/90], lter [221/1752] Loss: 19.6561\n",
      "Epoch [8/90], lter [231/1752] Loss: 16.6759\n",
      "Epoch [8/90], lter [241/1752] Loss: 21.7944\n",
      "Epoch [8/90], lter [251/1752] Loss: 36.4587\n",
      "Epoch [8/90], lter [261/1752] Loss: 19.4120\n",
      "Epoch [8/90], lter [271/1752] Loss: 25.6025\n",
      "Epoch [8/90], lter [281/1752] Loss: 23.8149\n",
      "Epoch [8/90], lter [291/1752] Loss: 32.4518\n",
      "Epoch [8/90], lter [301/1752] Loss: 30.9580\n",
      "Epoch [8/90], lter [311/1752] Loss: 23.0293\n",
      "Epoch [8/90], lter [321/1752] Loss: 22.1730\n",
      "Epoch [8/90], lter [331/1752] Loss: 19.0642\n",
      "Epoch [8/90], lter [341/1752] Loss: 27.5306\n",
      "Epoch [8/90], lter [351/1752] Loss: 23.4945\n",
      "Epoch [8/90], lter [361/1752] Loss: 24.3399\n",
      "Epoch [8/90], lter [371/1752] Loss: 20.8247\n",
      "Epoch [8/90], lter [381/1752] Loss: 29.9024\n",
      "Epoch [8/90], lter [391/1752] Loss: 18.1502\n",
      "Epoch [8/90], lter [401/1752] Loss: 24.2024\n",
      "Epoch [8/90], lter [411/1752] Loss: 18.0773\n",
      "Epoch [8/90], lter [421/1752] Loss: 33.7727\n",
      "Epoch [8/90], lter [431/1752] Loss: 14.4766\n",
      "Epoch [8/90], lter [441/1752] Loss: 13.7076\n",
      "Epoch [8/90], lter [451/1752] Loss: 18.4530\n",
      "Epoch [8/90], lter [461/1752] Loss: 21.6790\n",
      "Epoch [8/90], lter [471/1752] Loss: 28.4002\n",
      "Epoch [8/90], lter [481/1752] Loss: 33.7711\n",
      "Epoch [8/90], lter [491/1752] Loss: 17.3109\n",
      "Epoch [8/90], lter [501/1752] Loss: 27.7656\n",
      "Epoch [8/90], lter [511/1752] Loss: 23.8512\n",
      "Epoch [8/90], lter [521/1752] Loss: 11.2403\n",
      "Epoch [8/90], lter [531/1752] Loss: 16.0200\n",
      "Epoch [8/90], lter [541/1752] Loss: 22.1833\n",
      "Epoch [8/90], lter [551/1752] Loss: 15.1094\n",
      "Epoch [8/90], lter [561/1752] Loss: 20.5860\n",
      "Epoch [8/90], lter [571/1752] Loss: 28.5403\n",
      "Epoch [8/90], lter [581/1752] Loss: 31.8187\n",
      "Epoch [8/90], lter [591/1752] Loss: 25.5200\n",
      "Epoch [8/90], lter [601/1752] Loss: 19.2146\n",
      "Epoch [8/90], lter [611/1752] Loss: 24.9393\n",
      "Epoch [8/90], lter [621/1752] Loss: 24.4523\n",
      "Epoch [8/90], lter [631/1752] Loss: 9.9648\n",
      "Epoch [8/90], lter [641/1752] Loss: 12.7895\n",
      "Epoch [8/90], lter [651/1752] Loss: 16.1116\n",
      "Epoch [8/90], lter [661/1752] Loss: 23.0121\n",
      "Epoch [8/90], lter [671/1752] Loss: 19.8052\n",
      "Epoch [8/90], lter [681/1752] Loss: 37.5398\n",
      "Epoch [8/90], lter [691/1752] Loss: 27.5103\n",
      "Epoch [8/90], lter [701/1752] Loss: 20.8119\n",
      "Epoch [8/90], lter [711/1752] Loss: 18.0853\n",
      "Epoch [8/90], lter [721/1752] Loss: 22.3646\n",
      "Epoch [8/90], lter [731/1752] Loss: 38.4510\n",
      "Epoch [8/90], lter [741/1752] Loss: 34.1902\n",
      "Epoch [8/90], lter [751/1752] Loss: 16.4341\n",
      "Epoch [8/90], lter [761/1752] Loss: 20.0053\n",
      "Epoch [8/90], lter [771/1752] Loss: 18.2073\n",
      "Epoch [8/90], lter [781/1752] Loss: 31.0102\n",
      "Epoch [8/90], lter [791/1752] Loss: 26.5144\n",
      "Epoch [8/90], lter [801/1752] Loss: 22.9262\n",
      "Epoch [8/90], lter [811/1752] Loss: 24.5804\n",
      "Epoch [8/90], lter [821/1752] Loss: 30.5172\n",
      "Epoch [8/90], lter [831/1752] Loss: 25.5233\n",
      "Epoch [8/90], lter [841/1752] Loss: 26.6254\n",
      "Epoch [8/90], lter [851/1752] Loss: 26.5061\n",
      "Epoch [8/90], lter [861/1752] Loss: 20.0403\n",
      "Epoch [8/90], lter [871/1752] Loss: 25.1805\n",
      "Epoch [8/90], lter [881/1752] Loss: 27.0571\n",
      "Epoch [8/90], lter [891/1752] Loss: 32.6934\n",
      "Epoch [8/90], lter [901/1752] Loss: 22.7694\n",
      "Epoch [8/90], lter [911/1752] Loss: 29.6307\n",
      "Epoch [8/90], lter [921/1752] Loss: 24.0888\n",
      "Epoch [8/90], lter [931/1752] Loss: 22.4276\n",
      "Epoch [8/90], lter [941/1752] Loss: 26.5285\n",
      "Epoch [8/90], lter [951/1752] Loss: 30.1362\n",
      "Epoch [8/90], lter [961/1752] Loss: 24.4225\n",
      "Epoch [8/90], lter [971/1752] Loss: 32.5784\n",
      "Epoch [8/90], lter [981/1752] Loss: 20.3703\n",
      "Epoch [8/90], lter [991/1752] Loss: 32.7384\n",
      "Epoch [8/90], lter [1001/1752] Loss: 30.1082\n",
      "Epoch [8/90], lter [1011/1752] Loss: 27.5491\n",
      "Epoch [8/90], lter [1021/1752] Loss: 25.5063\n",
      "Epoch [8/90], lter [1031/1752] Loss: 26.7283\n",
      "Epoch [8/90], lter [1041/1752] Loss: 25.0069\n",
      "Epoch [8/90], lter [1051/1752] Loss: 25.4786\n",
      "Epoch [8/90], lter [1061/1752] Loss: 23.3492\n",
      "Epoch [8/90], lter [1071/1752] Loss: 35.5366\n",
      "Epoch [8/90], lter [1081/1752] Loss: 28.2736\n",
      "Epoch [8/90], lter [1091/1752] Loss: 36.8683\n",
      "Epoch [8/90], lter [1101/1752] Loss: 43.1644\n",
      "Epoch [8/90], lter [1111/1752] Loss: 21.8867\n",
      "Epoch [8/90], lter [1121/1752] Loss: 16.5516\n",
      "Epoch [8/90], lter [1131/1752] Loss: 21.1383\n",
      "Epoch [8/90], lter [1141/1752] Loss: 18.9025\n",
      "Epoch [8/90], lter [1151/1752] Loss: 23.0068\n",
      "Epoch [8/90], lter [1161/1752] Loss: 29.2216\n",
      "Epoch [8/90], lter [1171/1752] Loss: 22.6902\n",
      "Epoch [8/90], lter [1181/1752] Loss: 16.0829\n",
      "Epoch [8/90], lter [1191/1752] Loss: 27.7806\n",
      "Epoch [8/90], lter [1201/1752] Loss: 23.5262\n",
      "Epoch [8/90], lter [1211/1752] Loss: 54.5365\n",
      "Epoch [8/90], lter [1221/1752] Loss: 30.7388\n",
      "Epoch [8/90], lter [1231/1752] Loss: 22.7542\n",
      "Epoch [8/90], lter [1241/1752] Loss: 30.4048\n",
      "Epoch [8/90], lter [1251/1752] Loss: 28.1585\n",
      "Epoch [8/90], lter [1261/1752] Loss: 24.4104\n",
      "Epoch [8/90], lter [1271/1752] Loss: 19.8825\n",
      "Epoch [8/90], lter [1281/1752] Loss: 18.4726\n",
      "Epoch [8/90], lter [1291/1752] Loss: 22.4606\n",
      "Epoch [8/90], lter [1301/1752] Loss: 20.2787\n",
      "Epoch [8/90], lter [1311/1752] Loss: 24.6065\n",
      "Epoch [8/90], lter [1321/1752] Loss: 32.5817\n",
      "Epoch [8/90], lter [1331/1752] Loss: 22.4247\n",
      "Epoch [8/90], lter [1341/1752] Loss: 34.2221\n",
      "Epoch [8/90], lter [1351/1752] Loss: 28.4628\n",
      "Epoch [8/90], lter [1361/1752] Loss: 32.9547\n",
      "Epoch [8/90], lter [1371/1752] Loss: 23.9111\n",
      "Epoch [8/90], lter [1381/1752] Loss: 16.3487\n",
      "Epoch [8/90], lter [1391/1752] Loss: 27.9030\n",
      "Epoch [8/90], lter [1401/1752] Loss: 32.2826\n",
      "Epoch [8/90], lter [1411/1752] Loss: 21.8814\n",
      "Epoch [8/90], lter [1421/1752] Loss: 29.3865\n",
      "Epoch [8/90], lter [1431/1752] Loss: 34.7755\n",
      "Epoch [8/90], lter [1441/1752] Loss: 16.7213\n",
      "Epoch [8/90], lter [1451/1752] Loss: 32.4452\n",
      "Epoch [8/90], lter [1461/1752] Loss: 23.5523\n",
      "Epoch [8/90], lter [1471/1752] Loss: 14.9715\n",
      "Epoch [8/90], lter [1481/1752] Loss: 20.4200\n",
      "Epoch [8/90], lter [1491/1752] Loss: 16.4266\n",
      "Epoch [8/90], lter [1501/1752] Loss: 20.8038\n",
      "Epoch [8/90], lter [1511/1752] Loss: 24.8531\n",
      "Epoch [8/90], lter [1521/1752] Loss: 26.5765\n",
      "Epoch [8/90], lter [1531/1752] Loss: 40.5284\n",
      "Epoch [8/90], lter [1541/1752] Loss: 29.5696\n",
      "Epoch [8/90], lter [1551/1752] Loss: 15.2821\n",
      "Epoch [8/90], lter [1561/1752] Loss: 26.0231\n",
      "Epoch [8/90], lter [1571/1752] Loss: 18.8818\n",
      "Epoch [8/90], lter [1581/1752] Loss: 20.9676\n",
      "Epoch [8/90], lter [1591/1752] Loss: 17.9960\n",
      "Epoch [8/90], lter [1601/1752] Loss: 18.4492\n",
      "Epoch [8/90], lter [1611/1752] Loss: 25.5799\n",
      "Epoch [8/90], lter [1621/1752] Loss: 21.9512\n",
      "Epoch [8/90], lter [1631/1752] Loss: 25.2581\n",
      "Epoch [8/90], lter [1641/1752] Loss: 19.9146\n",
      "Epoch [8/90], lter [1651/1752] Loss: 30.8469\n",
      "Epoch [8/90], lter [1661/1752] Loss: 20.2020\n",
      "Epoch [8/90], lter [1671/1752] Loss: 29.1673\n",
      "Epoch [8/90], lter [1681/1752] Loss: 17.7759\n",
      "Epoch [8/90], lter [1691/1752] Loss: 26.5912\n",
      "Epoch [8/90], lter [1701/1752] Loss: 29.8487\n",
      "Epoch [8/90], lter [1711/1752] Loss: 30.7819\n",
      "Epoch [8/90], lter [1721/1752] Loss: 19.6133\n",
      "Epoch [8/90], lter [1731/1752] Loss: 27.2240\n",
      "Epoch [8/90], lter [1741/1752] Loss: 28.0309\n",
      "Epoch [8/90], lter [1751/1752] Loss: 22.6947\n",
      "Epoch:  8 | train loss : 24.9005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 8/90 [64:27:30<153:43:35, 6748.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7 | test loss : 24.6857\n",
      "Epoch [9/90], lter [1/1752] Loss: 35.0716\n",
      "Epoch [9/90], lter [11/1752] Loss: 14.4115\n",
      "Epoch [9/90], lter [21/1752] Loss: 28.9812\n",
      "Epoch [9/90], lter [31/1752] Loss: 32.2687\n",
      "Epoch [9/90], lter [41/1752] Loss: 15.6168\n",
      "Epoch [9/90], lter [51/1752] Loss: 28.6674\n",
      "Epoch [9/90], lter [61/1752] Loss: 25.0349\n",
      "Epoch [9/90], lter [71/1752] Loss: 25.5322\n",
      "Epoch [9/90], lter [81/1752] Loss: 17.1357\n",
      "Epoch [9/90], lter [91/1752] Loss: 23.4464\n",
      "Epoch [9/90], lter [101/1752] Loss: 21.4180\n",
      "Epoch [9/90], lter [111/1752] Loss: 17.6231\n",
      "Epoch [9/90], lter [121/1752] Loss: 20.1036\n",
      "Epoch [9/90], lter [131/1752] Loss: 18.8700\n",
      "Epoch [9/90], lter [141/1752] Loss: 16.0766\n",
      "Epoch [9/90], lter [151/1752] Loss: 29.9478\n",
      "Epoch [9/90], lter [161/1752] Loss: 19.0222\n",
      "Epoch [9/90], lter [171/1752] Loss: 21.9459\n",
      "Epoch [9/90], lter [181/1752] Loss: 28.2262\n",
      "Epoch [9/90], lter [191/1752] Loss: 20.1103\n",
      "Epoch [9/90], lter [201/1752] Loss: 16.0180\n",
      "Epoch [9/90], lter [211/1752] Loss: 26.0217\n",
      "Epoch [9/90], lter [221/1752] Loss: 28.4156\n",
      "Epoch [9/90], lter [231/1752] Loss: 19.9481\n",
      "Epoch [9/90], lter [241/1752] Loss: 15.6165\n",
      "Epoch [9/90], lter [251/1752] Loss: 22.4725\n",
      "Epoch [9/90], lter [261/1752] Loss: 21.0177\n",
      "Epoch [9/90], lter [271/1752] Loss: 31.7418\n",
      "Epoch [9/90], lter [281/1752] Loss: 28.4175\n",
      "Epoch [9/90], lter [291/1752] Loss: 30.0319\n",
      "Epoch [9/90], lter [501/1752] Loss: 36.8401\n",
      "Epoch [9/90], lter [511/1752] Loss: 38.3916\n",
      "Epoch [9/90], lter [521/1752] Loss: 17.0926\n",
      "Epoch [9/90], lter [531/1752] Loss: 33.4482\n",
      "Epoch [9/90], lter [541/1752] Loss: 23.1058\n",
      "Epoch [9/90], lter [551/1752] Loss: 28.5887\n",
      "Epoch [9/90], lter [561/1752] Loss: 13.7900\n",
      "Epoch [9/90], lter [571/1752] Loss: 40.4750\n",
      "Epoch [9/90], lter [581/1752] Loss: 17.9086\n",
      "Epoch [9/90], lter [591/1752] Loss: 25.7058\n",
      "Epoch [9/90], lter [601/1752] Loss: 31.7438\n",
      "Epoch [9/90], lter [611/1752] Loss: 12.9087\n",
      "Epoch [9/90], lter [621/1752] Loss: 21.0961\n",
      "Epoch [9/90], lter [631/1752] Loss: 25.8753\n",
      "Epoch [9/90], lter [641/1752] Loss: 21.5163\n",
      "Epoch [9/90], lter [651/1752] Loss: 17.6530\n",
      "Epoch [9/90], lter [661/1752] Loss: 23.3247\n",
      "Epoch [9/90], lter [671/1752] Loss: 19.7195\n",
      "Epoch [9/90], lter [681/1752] Loss: 23.1149\n",
      "Epoch [9/90], lter [691/1752] Loss: 15.7881\n",
      "Epoch [9/90], lter [701/1752] Loss: 26.2417\n",
      "Epoch [9/90], lter [711/1752] Loss: 24.1098\n",
      "Epoch [9/90], lter [721/1752] Loss: 19.8856\n",
      "Epoch [9/90], lter [731/1752] Loss: 18.4524\n",
      "Epoch [9/90], lter [741/1752] Loss: 18.7163\n",
      "Epoch [9/90], lter [751/1752] Loss: 39.6712\n",
      "Epoch [9/90], lter [761/1752] Loss: 31.0040\n",
      "Epoch [9/90], lter [771/1752] Loss: 28.2466\n",
      "Epoch [9/90], lter [781/1752] Loss: 17.1646\n",
      "Epoch [9/90], lter [791/1752] Loss: 27.8404\n",
      "Epoch [9/90], lter [801/1752] Loss: 21.7882\n",
      "Epoch [9/90], lter [811/1752] Loss: 24.0610\n",
      "Epoch [9/90], lter [821/1752] Loss: 15.9556\n",
      "Epoch [9/90], lter [831/1752] Loss: 17.7820\n",
      "Epoch [9/90], lter [841/1752] Loss: 29.0763\n",
      "Epoch [9/90], lter [851/1752] Loss: 25.9678\n",
      "Epoch [9/90], lter [861/1752] Loss: 35.1684\n",
      "Epoch [9/90], lter [871/1752] Loss: 20.7462\n",
      "Epoch [9/90], lter [881/1752] Loss: 20.5064\n",
      "Epoch [9/90], lter [891/1752] Loss: 20.1146\n",
      "Epoch [9/90], lter [901/1752] Loss: 27.9428\n",
      "Epoch [9/90], lter [911/1752] Loss: 15.8590\n",
      "Epoch [9/90], lter [921/1752] Loss: 23.7535\n",
      "Epoch [9/90], lter [931/1752] Loss: 25.7802\n",
      "Epoch [9/90], lter [941/1752] Loss: 19.4538\n",
      "Epoch [9/90], lter [951/1752] Loss: 23.3555\n",
      "Epoch [9/90], lter [961/1752] Loss: 23.9460\n",
      "Epoch [9/90], lter [971/1752] Loss: 29.0059\n",
      "Epoch [9/90], lter [981/1752] Loss: 23.1491\n",
      "Epoch [9/90], lter [991/1752] Loss: 25.3946\n",
      "Epoch [9/90], lter [1001/1752] Loss: 18.1219\n",
      "Epoch [9/90], lter [1011/1752] Loss: 14.5493\n",
      "Epoch [9/90], lter [1021/1752] Loss: 28.2119\n",
      "Epoch [9/90], lter [1031/1752] Loss: 16.4553\n",
      "Epoch [9/90], lter [1041/1752] Loss: 17.0555\n",
      "Epoch [9/90], lter [1051/1752] Loss: 16.9603\n",
      "Epoch [9/90], lter [1061/1752] Loss: 27.2019\n",
      "Epoch [9/90], lter [1071/1752] Loss: 22.3983\n",
      "Epoch [9/90], lter [1081/1752] Loss: 30.4199\n",
      "Epoch [9/90], lter [1091/1752] Loss: 15.2182\n",
      "Epoch [9/90], lter [1101/1752] Loss: 23.6042\n",
      "Epoch [9/90], lter [1111/1752] Loss: 27.2250\n",
      "Epoch [9/90], lter [1121/1752] Loss: 22.1345\n",
      "Epoch [9/90], lter [1131/1752] Loss: 33.4111\n",
      "Epoch [9/90], lter [1141/1752] Loss: 34.9839\n",
      "Epoch [9/90], lter [1151/1752] Loss: 33.0078\n",
      "Epoch [9/90], lter [1161/1752] Loss: 33.2266\n",
      "Epoch [9/90], lter [1171/1752] Loss: 22.6601\n",
      "Epoch [9/90], lter [1181/1752] Loss: 14.2139\n",
      "Epoch [9/90], lter [1191/1752] Loss: 27.6885\n",
      "Epoch [9/90], lter [1201/1752] Loss: 33.6466\n",
      "Epoch [9/90], lter [1211/1752] Loss: 19.5026\n",
      "Epoch [9/90], lter [1221/1752] Loss: 21.9721\n",
      "Epoch [9/90], lter [1231/1752] Loss: 19.3721\n",
      "Epoch [9/90], lter [1241/1752] Loss: 26.6080\n",
      "Epoch [9/90], lter [1251/1752] Loss: 23.7143\n",
      "Epoch [9/90], lter [1261/1752] Loss: 28.6460\n",
      "Epoch [9/90], lter [1271/1752] Loss: 31.2554\n",
      "Epoch [9/90], lter [1281/1752] Loss: 23.0432\n",
      "Epoch [9/90], lter [1291/1752] Loss: 23.6551\n",
      "Epoch [9/90], lter [1301/1752] Loss: 21.6224\n",
      "Epoch [9/90], lter [1311/1752] Loss: 28.4054\n",
      "Epoch [9/90], lter [1321/1752] Loss: 20.0191\n",
      "Epoch [9/90], lter [1331/1752] Loss: 30.8864\n",
      "Epoch [9/90], lter [1341/1752] Loss: 29.6821\n",
      "Epoch [9/90], lter [1351/1752] Loss: 14.5911\n",
      "Epoch [9/90], lter [1361/1752] Loss: 39.9413\n",
      "Epoch [9/90], lter [1371/1752] Loss: 36.4540\n",
      "Epoch [9/90], lter [1381/1752] Loss: 21.8084\n",
      "Epoch [9/90], lter [1391/1752] Loss: 22.3689\n",
      "Epoch [9/90], lter [1401/1752] Loss: 13.6697\n",
      "Epoch [9/90], lter [1411/1752] Loss: 36.5874\n",
      "Epoch [9/90], lter [1421/1752] Loss: 36.9034\n",
      "Epoch [9/90], lter [1431/1752] Loss: 24.7419\n",
      "Epoch [9/90], lter [1441/1752] Loss: 20.2907\n",
      "Epoch [9/90], lter [1451/1752] Loss: 17.2230\n",
      "Epoch [9/90], lter [1461/1752] Loss: 23.8008\n",
      "Epoch [9/90], lter [1471/1752] Loss: 26.5439\n",
      "Epoch [9/90], lter [1481/1752] Loss: 22.7631\n",
      "Epoch [9/90], lter [1491/1752] Loss: 23.5203\n",
      "Epoch [9/90], lter [1501/1752] Loss: 21.6122\n",
      "Epoch [9/90], lter [1511/1752] Loss: 22.9890\n",
      "Epoch [9/90], lter [1521/1752] Loss: 16.5145\n",
      "Epoch [9/90], lter [1531/1752] Loss: 24.9142\n",
      "Epoch [9/90], lter [1541/1752] Loss: 19.5071\n",
      "Epoch [9/90], lter [1551/1752] Loss: 28.3978\n",
      "Epoch [9/90], lter [1561/1752] Loss: 24.1635\n",
      "Epoch [9/90], lter [1571/1752] Loss: 18.3404\n",
      "Epoch [9/90], lter [1581/1752] Loss: 29.7500\n",
      "Epoch [9/90], lter [1591/1752] Loss: 22.3938\n",
      "Epoch [9/90], lter [1601/1752] Loss: 21.5023\n",
      "Epoch [9/90], lter [1611/1752] Loss: 13.8758\n",
      "Epoch [9/90], lter [1621/1752] Loss: 30.6619\n",
      "Epoch [9/90], lter [1631/1752] Loss: 11.8773\n",
      "Epoch [9/90], lter [1641/1752] Loss: 20.1618\n",
      "Epoch [9/90], lter [1651/1752] Loss: 19.2333\n",
      "Epoch [9/90], lter [1661/1752] Loss: 29.2008\n",
      "Epoch [9/90], lter [1671/1752] Loss: 26.4765\n",
      "Epoch [9/90], lter [1681/1752] Loss: 39.8689\n",
      "Epoch [9/90], lter [1691/1752] Loss: 27.7982\n",
      "Epoch [9/90], lter [1701/1752] Loss: 32.8263\n",
      "Epoch [9/90], lter [1711/1752] Loss: 38.8122\n",
      "Epoch [9/90], lter [1721/1752] Loss: 23.6219\n",
      "Epoch [9/90], lter [1731/1752] Loss: 15.8975\n",
      "Epoch [9/90], lter [1741/1752] Loss: 29.1678\n",
      "Epoch [9/90], lter [1751/1752] Loss: 17.8020\n",
      "Epoch:  9 | train loss : 24.7391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 9/90 [64:41:33<110:18:51, 4902.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8 | test loss : 15.4627\n",
      "Epoch [10/90], lter [1/1752] Loss: 38.1512\n",
      "Epoch [10/90], lter [11/1752] Loss: 21.5456\n",
      "Epoch [10/90], lter [21/1752] Loss: 25.4719\n",
      "Epoch [10/90], lter [31/1752] Loss: 25.4807\n",
      "Epoch [10/90], lter [41/1752] Loss: 10.8598\n",
      "Epoch [10/90], lter [51/1752] Loss: 14.3911\n",
      "Epoch [10/90], lter [61/1752] Loss: 26.9247\n",
      "Epoch [10/90], lter [71/1752] Loss: 28.0516\n",
      "Epoch [10/90], lter [81/1752] Loss: 17.6826\n",
      "Epoch [10/90], lter [91/1752] Loss: 28.9257\n",
      "Epoch [10/90], lter [101/1752] Loss: 20.7689\n",
      "Epoch [10/90], lter [111/1752] Loss: 10.5980\n",
      "Epoch [10/90], lter [121/1752] Loss: 19.3650\n",
      "Epoch [10/90], lter [131/1752] Loss: 25.0184\n",
      "Epoch [10/90], lter [141/1752] Loss: 29.7496\n",
      "Epoch [10/90], lter [151/1752] Loss: 32.0596\n",
      "Epoch [10/90], lter [161/1752] Loss: 29.5560\n",
      "Epoch [10/90], lter [171/1752] Loss: 40.6023\n",
      "Epoch [10/90], lter [181/1752] Loss: 17.7013\n",
      "Epoch [10/90], lter [191/1752] Loss: 25.2015\n",
      "Epoch [10/90], lter [201/1752] Loss: 26.7277\n",
      "Epoch [10/90], lter [211/1752] Loss: 19.6286\n",
      "Epoch [10/90], lter [221/1752] Loss: 21.3308\n",
      "Epoch [10/90], lter [231/1752] Loss: 14.6397\n",
      "Epoch [10/90], lter [241/1752] Loss: 19.3110\n",
      "Epoch [10/90], lter [251/1752] Loss: 31.6763\n",
      "Epoch [10/90], lter [261/1752] Loss: 38.2336\n",
      "Epoch [10/90], lter [271/1752] Loss: 28.4140\n",
      "Epoch [10/90], lter [281/1752] Loss: 21.0186\n",
      "Epoch [10/90], lter [291/1752] Loss: 26.6351\n",
      "Epoch [10/90], lter [301/1752] Loss: 23.9756\n",
      "Epoch [10/90], lter [311/1752] Loss: 32.1286\n",
      "Epoch [10/90], lter [321/1752] Loss: 29.2867\n",
      "Epoch [10/90], lter [331/1752] Loss: 32.9108\n",
      "Epoch [10/90], lter [341/1752] Loss: 32.6140\n",
      "Epoch [10/90], lter [351/1752] Loss: 15.3328\n",
      "Epoch [10/90], lter [361/1752] Loss: 23.1938\n",
      "Epoch [10/90], lter [371/1752] Loss: 20.1185\n",
      "Epoch [10/90], lter [381/1752] Loss: 17.1662\n",
      "Epoch [10/90], lter [391/1752] Loss: 20.3654\n",
      "Epoch [10/90], lter [401/1752] Loss: 19.0191\n",
      "Epoch [10/90], lter [411/1752] Loss: 18.5765\n",
      "Epoch [10/90], lter [421/1752] Loss: 21.7219\n",
      "Epoch [10/90], lter [431/1752] Loss: 15.1474\n",
      "Epoch [10/90], lter [441/1752] Loss: 20.9046\n",
      "Epoch [10/90], lter [451/1752] Loss: 19.9678\n",
      "Epoch [10/90], lter [461/1752] Loss: 34.5092\n",
      "Epoch [10/90], lter [471/1752] Loss: 26.2516\n",
      "Epoch [10/90], lter [481/1752] Loss: 27.7069\n",
      "Epoch [10/90], lter [491/1752] Loss: 15.8700\n",
      "Epoch [10/90], lter [501/1752] Loss: 33.9132\n",
      "Epoch [10/90], lter [511/1752] Loss: 26.1469\n",
      "Epoch [10/90], lter [521/1752] Loss: 19.6132\n",
      "Epoch [10/90], lter [531/1752] Loss: 13.8763\n",
      "Epoch [10/90], lter [541/1752] Loss: 22.9531\n",
      "Epoch [10/90], lter [551/1752] Loss: 16.0609\n",
      "Epoch [10/90], lter [561/1752] Loss: 28.4183\n",
      "Epoch [10/90], lter [571/1752] Loss: 30.6882\n",
      "Epoch [10/90], lter [581/1752] Loss: 28.8435\n",
      "Epoch [10/90], lter [591/1752] Loss: 32.1545\n",
      "Epoch [10/90], lter [601/1752] Loss: 22.7886\n",
      "Epoch [10/90], lter [611/1752] Loss: 37.3020\n",
      "Epoch [10/90], lter [621/1752] Loss: 47.7276\n",
      "Epoch [10/90], lter [631/1752] Loss: 20.5070\n",
      "Epoch [10/90], lter [641/1752] Loss: 35.2310\n",
      "Epoch [10/90], lter [651/1752] Loss: 22.8606\n",
      "Epoch [10/90], lter [661/1752] Loss: 35.4933\n",
      "Epoch [10/90], lter [671/1752] Loss: 26.3310\n",
      "Epoch [10/90], lter [681/1752] Loss: 22.7458\n",
      "Epoch [10/90], lter [691/1752] Loss: 17.5173\n",
      "Epoch [10/90], lter [701/1752] Loss: 30.2980\n",
      "Epoch [10/90], lter [711/1752] Loss: 29.2153\n",
      "Epoch [10/90], lter [721/1752] Loss: 14.5991\n",
      "Epoch [10/90], lter [731/1752] Loss: 21.4142\n",
      "Epoch [10/90], lter [741/1752] Loss: 33.1614\n",
      "Epoch [10/90], lter [751/1752] Loss: 28.3925\n",
      "Epoch [10/90], lter [761/1752] Loss: 25.9876\n",
      "Epoch [10/90], lter [771/1752] Loss: 20.3368\n",
      "Epoch [10/90], lter [781/1752] Loss: 23.0673\n",
      "Epoch [10/90], lter [791/1752] Loss: 13.5582\n",
      "Epoch [10/90], lter [801/1752] Loss: 23.7375\n",
      "Epoch [10/90], lter [811/1752] Loss: 30.8152\n",
      "Epoch [10/90], lter [821/1752] Loss: 24.5145\n",
      "Epoch [10/90], lter [831/1752] Loss: 30.2670\n",
      "Epoch [10/90], lter [841/1752] Loss: 28.7596\n",
      "Epoch [10/90], lter [851/1752] Loss: 27.7789\n",
      "Epoch [10/90], lter [861/1752] Loss: 39.5635\n",
      "Epoch [10/90], lter [871/1752] Loss: 40.9255\n",
      "Epoch [10/90], lter [881/1752] Loss: 28.2058\n",
      "Epoch [10/90], lter [891/1752] Loss: 23.4172\n",
      "Epoch [10/90], lter [901/1752] Loss: 30.7340\n",
      "Epoch [10/90], lter [911/1752] Loss: 27.1336\n",
      "Epoch [10/90], lter [921/1752] Loss: 23.3500\n",
      "Epoch [10/90], lter [931/1752] Loss: 20.2231\n",
      "Epoch [10/90], lter [941/1752] Loss: 27.5758\n",
      "Epoch [10/90], lter [951/1752] Loss: 21.9385\n",
      "Epoch [10/90], lter [961/1752] Loss: 15.8890\n",
      "Epoch [10/90], lter [971/1752] Loss: 28.6722\n",
      "Epoch [10/90], lter [981/1752] Loss: 15.7620\n",
      "Epoch [10/90], lter [991/1752] Loss: 15.2750\n",
      "Epoch [10/90], lter [1001/1752] Loss: 30.0837\n",
      "Epoch [10/90], lter [1011/1752] Loss: 20.5659\n",
      "Epoch [10/90], lter [1021/1752] Loss: 23.2716\n",
      "Epoch [10/90], lter [1031/1752] Loss: 20.4815\n",
      "Epoch [10/90], lter [1041/1752] Loss: 29.0208\n",
      "Epoch [10/90], lter [1051/1752] Loss: 32.0935\n",
      "Epoch [10/90], lter [1061/1752] Loss: 18.3660\n",
      "Epoch [10/90], lter [1071/1752] Loss: 21.7428\n",
      "Epoch [10/90], lter [1081/1752] Loss: 42.2647\n",
      "Epoch [10/90], lter [1091/1752] Loss: 28.8223\n",
      "Epoch [10/90], lter [1101/1752] Loss: 21.9536\n",
      "Epoch [10/90], lter [1111/1752] Loss: 23.9979\n",
      "Epoch [10/90], lter [1121/1752] Loss: 26.1644\n",
      "Epoch [10/90], lter [1131/1752] Loss: 36.5169\n",
      "Epoch [10/90], lter [1141/1752] Loss: 25.2347\n",
      "Epoch [10/90], lter [1151/1752] Loss: 15.1855\n",
      "Epoch [10/90], lter [1161/1752] Loss: 27.8627\n",
      "Epoch [10/90], lter [1171/1752] Loss: 39.0868\n",
      "Epoch [10/90], lter [1181/1752] Loss: 33.8878\n",
      "Epoch [10/90], lter [1191/1752] Loss: 21.9152\n",
      "Epoch [10/90], lter [1201/1752] Loss: 27.1941\n",
      "Epoch [10/90], lter [1211/1752] Loss: 20.6533\n",
      "Epoch [10/90], lter [1221/1752] Loss: 32.6755\n",
      "Epoch [10/90], lter [1231/1752] Loss: 20.0446\n",
      "Epoch [10/90], lter [1241/1752] Loss: 25.2387\n",
      "Epoch [10/90], lter [1251/1752] Loss: 14.2755\n",
      "Epoch [10/90], lter [1261/1752] Loss: 26.6262\n",
      "Epoch [10/90], lter [1271/1752] Loss: 30.4548\n",
      "Epoch [10/90], lter [1281/1752] Loss: 21.8134\n",
      "Epoch [10/90], lter [1291/1752] Loss: 20.3419\n",
      "Epoch [10/90], lter [1301/1752] Loss: 23.0909\n",
      "Epoch [10/90], lter [1311/1752] Loss: 30.3930\n",
      "Epoch [10/90], lter [1321/1752] Loss: 20.6971\n",
      "Epoch [10/90], lter [1331/1752] Loss: 16.5591\n",
      "Epoch [10/90], lter [1341/1752] Loss: 16.0980\n",
      "Epoch [10/90], lter [1351/1752] Loss: 21.0481\n",
      "Epoch [10/90], lter [1361/1752] Loss: 19.8914\n",
      "Epoch [10/90], lter [1371/1752] Loss: 31.7724\n",
      "Epoch [10/90], lter [1381/1752] Loss: 25.6663\n",
      "Epoch [10/90], lter [1391/1752] Loss: 26.8008\n",
      "Epoch [10/90], lter [1401/1752] Loss: 33.9157\n",
      "Epoch [10/90], lter [1411/1752] Loss: 26.1831\n",
      "Epoch [10/90], lter [1421/1752] Loss: 33.7418\n",
      "Epoch [10/90], lter [1431/1752] Loss: 17.3553\n",
      "Epoch [10/90], lter [1441/1752] Loss: 26.9593\n",
      "Epoch [10/90], lter [1451/1752] Loss: 23.1928\n",
      "Epoch [10/90], lter [1461/1752] Loss: 33.4292\n",
      "Epoch [10/90], lter [1471/1752] Loss: 10.9169\n",
      "Epoch [10/90], lter [1481/1752] Loss: 38.7236\n",
      "Epoch [10/90], lter [1491/1752] Loss: 28.8168\n",
      "Epoch [10/90], lter [1501/1752] Loss: 24.7833\n",
      "Epoch [10/90], lter [1511/1752] Loss: 14.4093\n",
      "Epoch [10/90], lter [1521/1752] Loss: 22.0937\n",
      "Epoch [10/90], lter [1531/1752] Loss: 24.5802\n",
      "Epoch [10/90], lter [1541/1752] Loss: 39.0547\n",
      "Epoch [10/90], lter [1551/1752] Loss: 25.6899\n",
      "Epoch [10/90], lter [1561/1752] Loss: 26.9688\n",
      "Epoch [10/90], lter [1571/1752] Loss: 18.7725\n",
      "Epoch [10/90], lter [1581/1752] Loss: 14.6961\n",
      "Epoch [10/90], lter [1591/1752] Loss: 31.1063\n",
      "Epoch [10/90], lter [1601/1752] Loss: 31.3894\n",
      "Epoch [10/90], lter [1611/1752] Loss: 27.8106\n",
      "Epoch [10/90], lter [1621/1752] Loss: 18.2049\n",
      "Epoch [10/90], lter [1631/1752] Loss: 25.5623\n",
      "Epoch [10/90], lter [1641/1752] Loss: 22.9832\n",
      "Epoch [10/90], lter [1651/1752] Loss: 17.8299\n",
      "Epoch [10/90], lter [1661/1752] Loss: 22.8066\n",
      "Epoch [10/90], lter [1671/1752] Loss: 29.1941\n",
      "Epoch [10/90], lter [1681/1752] Loss: 25.0734\n",
      "Epoch [10/90], lter [1691/1752] Loss: 19.8263\n",
      "Epoch [10/90], lter [1701/1752] Loss: 29.0330\n",
      "Epoch [10/90], lter [1711/1752] Loss: 13.5544\n",
      "Epoch [10/90], lter [1721/1752] Loss: 26.4365\n",
      "Epoch [10/90], lter [1731/1752] Loss: 27.1095\n",
      "Epoch [10/90], lter [1741/1752] Loss: 26.7848\n",
      "Epoch [10/90], lter [1751/1752] Loss: 28.6959\n",
      "Epoch:  10 | train loss : 24.8984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/90 [64:55:34<81:05:01, 3648.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9 | test loss : 16.6240\n",
      "Epoch [11/90], lter [1/1752] Loss: 32.3637\n",
      "Epoch [11/90], lter [11/1752] Loss: 21.5887\n",
      "Epoch [11/90], lter [21/1752] Loss: 21.6804\n",
      "Epoch [11/90], lter [31/1752] Loss: 24.7639\n",
      "Epoch [11/90], lter [41/1752] Loss: 17.4201\n",
      "Epoch [11/90], lter [51/1752] Loss: 20.1305\n",
      "Epoch [11/90], lter [61/1752] Loss: 31.0133\n",
      "Epoch [11/90], lter [71/1752] Loss: 33.2016\n",
      "Epoch [11/90], lter [81/1752] Loss: 20.4975\n",
      "Epoch [11/90], lter [91/1752] Loss: 22.9241\n",
      "Epoch [11/90], lter [101/1752] Loss: 19.8645\n",
      "Epoch [11/90], lter [111/1752] Loss: 25.9010\n",
      "Epoch [11/90], lter [121/1752] Loss: 29.7615\n",
      "Epoch [11/90], lter [131/1752] Loss: 16.1917\n",
      "Epoch [11/90], lter [141/1752] Loss: 29.7380\n",
      "Epoch [11/90], lter [151/1752] Loss: 30.4256\n",
      "Epoch [11/90], lter [161/1752] Loss: 23.0209\n",
      "Epoch [11/90], lter [171/1752] Loss: 28.1314\n",
      "Epoch [11/90], lter [181/1752] Loss: 14.0009\n",
      "Epoch [11/90], lter [191/1752] Loss: 23.1640\n",
      "Epoch [11/90], lter [201/1752] Loss: 30.6781\n",
      "Epoch [11/90], lter [211/1752] Loss: 18.0061\n",
      "Epoch [11/90], lter [221/1752] Loss: 28.9466\n",
      "Epoch [11/90], lter [231/1752] Loss: 19.5132\n",
      "Epoch [11/90], lter [241/1752] Loss: 19.3801\n",
      "Epoch [11/90], lter [251/1752] Loss: 23.5015\n",
      "Epoch [11/90], lter [261/1752] Loss: 28.7922\n",
      "Epoch [11/90], lter [271/1752] Loss: 16.5948\n",
      "Epoch [11/90], lter [281/1752] Loss: 36.9684\n",
      "Epoch [11/90], lter [291/1752] Loss: 35.3258\n",
      "Epoch [11/90], lter [301/1752] Loss: 17.5672\n",
      "Epoch [11/90], lter [311/1752] Loss: 27.6227\n",
      "Epoch [11/90], lter [321/1752] Loss: 20.7382\n",
      "Epoch [11/90], lter [331/1752] Loss: 27.2065\n",
      "Epoch [11/90], lter [341/1752] Loss: 27.1921\n",
      "Epoch [11/90], lter [351/1752] Loss: 32.6276\n",
      "Epoch [11/90], lter [361/1752] Loss: 34.6771\n",
      "Epoch [11/90], lter [371/1752] Loss: 22.2609\n",
      "Epoch [11/90], lter [381/1752] Loss: 13.7349\n",
      "Epoch [11/90], lter [391/1752] Loss: 27.0824\n",
      "Epoch [11/90], lter [401/1752] Loss: 17.9276\n",
      "Epoch [11/90], lter [411/1752] Loss: 23.4606\n",
      "Epoch [11/90], lter [421/1752] Loss: 29.8350\n",
      "Epoch [11/90], lter [431/1752] Loss: 21.9686\n",
      "Epoch [11/90], lter [441/1752] Loss: 36.2050\n",
      "Epoch [11/90], lter [451/1752] Loss: 29.5684\n",
      "Epoch [11/90], lter [461/1752] Loss: 22.6805\n",
      "Epoch [11/90], lter [471/1752] Loss: 33.5621\n",
      "Epoch [11/90], lter [481/1752] Loss: 9.9544\n",
      "Epoch [11/90], lter [491/1752] Loss: 23.7229\n",
      "Epoch [11/90], lter [501/1752] Loss: 23.6959\n",
      "Epoch [11/90], lter [511/1752] Loss: 29.9774\n",
      "Epoch [11/90], lter [521/1752] Loss: 37.4929\n",
      "Epoch [11/90], lter [531/1752] Loss: 23.2994\n",
      "Epoch [11/90], lter [541/1752] Loss: 23.9927\n",
      "Epoch [11/90], lter [551/1752] Loss: 19.0456\n",
      "Epoch [11/90], lter [561/1752] Loss: 22.4714\n",
      "Epoch [11/90], lter [571/1752] Loss: 41.6176\n",
      "Epoch [11/90], lter [581/1752] Loss: 36.4306\n",
      "Epoch [11/90], lter [591/1752] Loss: 19.9223\n",
      "Epoch [11/90], lter [601/1752] Loss: 21.4405\n",
      "Epoch [11/90], lter [611/1752] Loss: 30.0784\n",
      "Epoch [11/90], lter [621/1752] Loss: 24.9821\n",
      "Epoch [11/90], lter [631/1752] Loss: 28.0105\n",
      "Epoch [11/90], lter [641/1752] Loss: 35.7053\n",
      "Epoch [11/90], lter [651/1752] Loss: 22.7963\n",
      "Epoch [11/90], lter [661/1752] Loss: 33.6538\n",
      "Epoch [11/90], lter [671/1752] Loss: 25.0578\n",
      "Epoch [11/90], lter [681/1752] Loss: 32.8951\n",
      "Epoch [11/90], lter [691/1752] Loss: 38.2769\n",
      "Epoch [11/90], lter [701/1752] Loss: 26.8307\n",
      "Epoch [11/90], lter [711/1752] Loss: 25.7977\n",
      "Epoch [11/90], lter [721/1752] Loss: 26.1187\n",
      "Epoch [11/90], lter [731/1752] Loss: 22.8844\n",
      "Epoch [11/90], lter [741/1752] Loss: 16.4687\n",
      "Epoch [11/90], lter [751/1752] Loss: 17.8510\n",
      "Epoch [11/90], lter [761/1752] Loss: 14.3671\n",
      "Epoch [11/90], lter [771/1752] Loss: 27.6588\n",
      "Epoch [11/90], lter [781/1752] Loss: 39.8582\n",
      "Epoch [11/90], lter [791/1752] Loss: 23.8650\n",
      "Epoch [11/90], lter [801/1752] Loss: 43.9473\n",
      "Epoch [11/90], lter [811/1752] Loss: 30.1040\n",
      "Epoch [11/90], lter [821/1752] Loss: 18.3951\n",
      "Epoch [11/90], lter [831/1752] Loss: 32.3154\n",
      "Epoch [11/90], lter [841/1752] Loss: 23.6390\n",
      "Epoch [11/90], lter [851/1752] Loss: 30.2090\n",
      "Epoch [11/90], lter [861/1752] Loss: 27.0830\n",
      "Epoch [11/90], lter [871/1752] Loss: 31.5479\n",
      "Epoch [11/90], lter [881/1752] Loss: 19.4961\n",
      "Epoch [11/90], lter [891/1752] Loss: 24.4156\n",
      "Epoch [11/90], lter [901/1752] Loss: 19.1312\n",
      "Epoch [11/90], lter [911/1752] Loss: 21.3461\n",
      "Epoch [11/90], lter [921/1752] Loss: 21.4597\n",
      "Epoch [11/90], lter [931/1752] Loss: 24.1775\n",
      "Epoch [11/90], lter [941/1752] Loss: 21.1756\n",
      "Epoch [11/90], lter [951/1752] Loss: 24.4055\n",
      "Epoch [11/90], lter [961/1752] Loss: 23.2732\n",
      "Epoch [11/90], lter [971/1752] Loss: 23.8573\n",
      "Epoch [11/90], lter [981/1752] Loss: 21.1656\n",
      "Epoch [11/90], lter [991/1752] Loss: 20.4061\n",
      "Epoch [11/90], lter [1001/1752] Loss: 34.9141\n",
      "Epoch [11/90], lter [1011/1752] Loss: 20.4167\n",
      "Epoch [11/90], lter [1021/1752] Loss: 27.1114\n",
      "Epoch [11/90], lter [1031/1752] Loss: 30.1831\n",
      "Epoch [11/90], lter [1041/1752] Loss: 32.4821\n",
      "Epoch [11/90], lter [1051/1752] Loss: 23.3726\n",
      "Epoch [11/90], lter [1061/1752] Loss: 14.5840\n",
      "Epoch [11/90], lter [1071/1752] Loss: 27.3009\n",
      "Epoch [11/90], lter [1081/1752] Loss: 35.4762\n",
      "Epoch [11/90], lter [1091/1752] Loss: 14.9965\n",
      "Epoch [11/90], lter [1101/1752] Loss: 39.4541\n",
      "Epoch [11/90], lter [1111/1752] Loss: 51.9131\n",
      "Epoch [11/90], lter [1121/1752] Loss: 20.5000\n",
      "Epoch [11/90], lter [1131/1752] Loss: 23.6875\n",
      "Epoch [11/90], lter [1141/1752] Loss: 19.1235\n",
      "Epoch [11/90], lter [1151/1752] Loss: 34.8071\n",
      "Epoch [11/90], lter [1161/1752] Loss: 22.0778\n",
      "Epoch [11/90], lter [1171/1752] Loss: 42.0485\n",
      "Epoch [11/90], lter [1181/1752] Loss: 28.6342\n",
      "Epoch [11/90], lter [1191/1752] Loss: 14.9324\n",
      "Epoch [11/90], lter [1201/1752] Loss: 20.7336\n",
      "Epoch [11/90], lter [1211/1752] Loss: 26.6513\n",
      "Epoch [11/90], lter [1221/1752] Loss: 21.4499\n",
      "Epoch [11/90], lter [1231/1752] Loss: 22.7483\n",
      "Epoch [11/90], lter [1241/1752] Loss: 30.5962\n",
      "Epoch [11/90], lter [1251/1752] Loss: 35.1499\n",
      "Epoch [11/90], lter [1261/1752] Loss: 41.5249\n",
      "Epoch [11/90], lter [1271/1752] Loss: 30.3180\n",
      "Epoch [11/90], lter [1281/1752] Loss: 29.4065\n",
      "Epoch [11/90], lter [1291/1752] Loss: 21.9256\n",
      "Epoch [11/90], lter [1301/1752] Loss: 25.3752\n",
      "Epoch [11/90], lter [1311/1752] Loss: 27.2790\n",
      "Epoch [11/90], lter [1321/1752] Loss: 16.3021\n",
      "Epoch [11/90], lter [1331/1752] Loss: 29.6778\n",
      "Epoch [11/90], lter [1341/1752] Loss: 29.2485\n",
      "Epoch [11/90], lter [1351/1752] Loss: 27.3979\n",
      "Epoch [11/90], lter [1361/1752] Loss: 21.0803\n",
      "Epoch [11/90], lter [1371/1752] Loss: 29.9486\n",
      "Epoch [11/90], lter [1381/1752] Loss: 18.8354\n",
      "Epoch [11/90], lter [1391/1752] Loss: 26.8272\n",
      "Epoch [11/90], lter [1401/1752] Loss: 24.0046\n",
      "Epoch [11/90], lter [1411/1752] Loss: 34.9731\n",
      "Epoch [11/90], lter [1421/1752] Loss: 28.7156\n",
      "Epoch [11/90], lter [1431/1752] Loss: 24.8460\n",
      "Epoch [11/90], lter [1441/1752] Loss: 25.4402\n",
      "Epoch [11/90], lter [1451/1752] Loss: 23.2876\n",
      "Epoch [11/90], lter [1461/1752] Loss: 18.7919\n",
      "Epoch [11/90], lter [1471/1752] Loss: 33.3687\n",
      "Epoch [11/90], lter [1481/1752] Loss: 32.0373\n",
      "Epoch [11/90], lter [1491/1752] Loss: 22.3783\n",
      "Epoch [11/90], lter [1501/1752] Loss: 23.2391\n",
      "Epoch [11/90], lter [1511/1752] Loss: 20.8123\n",
      "Epoch [11/90], lter [1521/1752] Loss: 21.7043\n",
      "Epoch [11/90], lter [1531/1752] Loss: 12.2499\n",
      "Epoch [11/90], lter [1541/1752] Loss: 24.6092\n",
      "Epoch [11/90], lter [1551/1752] Loss: 11.4604\n",
      "Epoch [11/90], lter [1561/1752] Loss: 21.1309\n",
      "Epoch [11/90], lter [1571/1752] Loss: 21.6147\n",
      "Epoch [11/90], lter [1581/1752] Loss: 24.1179\n",
      "Epoch [11/90], lter [1591/1752] Loss: 18.4846\n",
      "Epoch [11/90], lter [1601/1752] Loss: 15.9497\n",
      "Epoch [11/90], lter [1611/1752] Loss: 24.5850\n",
      "Epoch [11/90], lter [1621/1752] Loss: 24.9158\n",
      "Epoch [11/90], lter [1631/1752] Loss: 21.6628\n",
      "Epoch [11/90], lter [1641/1752] Loss: 21.0131\n",
      "Epoch [11/90], lter [1651/1752] Loss: 7.6037\n",
      "Epoch [11/90], lter [1661/1752] Loss: 20.0761\n",
      "Epoch [11/90], lter [1671/1752] Loss: 21.7865\n",
      "Epoch [11/90], lter [1681/1752] Loss: 26.0722\n",
      "Epoch [11/90], lter [1691/1752] Loss: 27.4518\n",
      "Epoch [11/90], lter [1701/1752] Loss: 20.7800\n",
      "Epoch [11/90], lter [1711/1752] Loss: 17.9770\n",
      "Epoch [11/90], lter [1721/1752] Loss: 27.8995\n",
      "Epoch [11/90], lter [1731/1752] Loss: 20.0299\n",
      "Epoch [11/90], lter [1741/1752] Loss: 18.5332\n",
      "Epoch [11/90], lter [1751/1752] Loss: 18.8483\n",
      "Epoch:  11 | train loss : 24.8343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11/90 [65:09:32<61:11:38, 2788.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10 | test loss : 15.3932\n",
      "Epoch [12/90], lter [1/1752] Loss: 18.5808\n",
      "Epoch [12/90], lter [11/1752] Loss: 26.4618\n",
      "Epoch [12/90], lter [21/1752] Loss: 22.8975\n",
      "Epoch [12/90], lter [31/1752] Loss: 29.6595\n",
      "Epoch [12/90], lter [41/1752] Loss: 20.8982\n",
      "Epoch [12/90], lter [51/1752] Loss: 24.4104\n",
      "Epoch [12/90], lter [61/1752] Loss: 15.1980\n",
      "Epoch [12/90], lter [71/1752] Loss: 35.3368\n",
      "Epoch [12/90], lter [81/1752] Loss: 32.7989\n",
      "Epoch [12/90], lter [91/1752] Loss: 18.3152\n",
      "Epoch [12/90], lter [101/1752] Loss: 35.6596\n",
      "Epoch [12/90], lter [111/1752] Loss: 28.2740\n",
      "Epoch [12/90], lter [121/1752] Loss: 26.8338\n",
      "Epoch [12/90], lter [131/1752] Loss: 24.3369\n",
      "Epoch [12/90], lter [141/1752] Loss: 24.6101\n",
      "Epoch [12/90], lter [151/1752] Loss: 43.0569\n",
      "Epoch [12/90], lter [161/1752] Loss: 25.0011\n",
      "Epoch [12/90], lter [171/1752] Loss: 13.8020\n",
      "Epoch [12/90], lter [181/1752] Loss: 28.8373\n",
      "Epoch [12/90], lter [191/1752] Loss: 26.6436\n",
      "Epoch [12/90], lter [201/1752] Loss: 29.1125\n",
      "Epoch [12/90], lter [211/1752] Loss: 28.3087\n",
      "Epoch [12/90], lter [221/1752] Loss: 23.0525\n",
      "Epoch [12/90], lter [231/1752] Loss: 22.6594\n",
      "Epoch [12/90], lter [241/1752] Loss: 18.3952\n",
      "Epoch [12/90], lter [251/1752] Loss: 35.5599\n",
      "Epoch [12/90], lter [261/1752] Loss: 31.7562\n",
      "Epoch [12/90], lter [271/1752] Loss: 23.0125\n",
      "Epoch [12/90], lter [281/1752] Loss: 22.3137\n",
      "Epoch [12/90], lter [291/1752] Loss: 21.5221\n",
      "Epoch [12/90], lter [301/1752] Loss: 36.0369\n",
      "Epoch [12/90], lter [311/1752] Loss: 23.3426\n",
      "Epoch [12/90], lter [321/1752] Loss: 36.1509\n",
      "Epoch [12/90], lter [331/1752] Loss: 28.0931\n",
      "Epoch [12/90], lter [341/1752] Loss: 28.1467\n",
      "Epoch [12/90], lter [351/1752] Loss: 19.4826\n",
      "Epoch [12/90], lter [361/1752] Loss: 32.9153\n",
      "Epoch [12/90], lter [371/1752] Loss: 34.1063\n",
      "Epoch [12/90], lter [381/1752] Loss: 34.4563\n",
      "Epoch [12/90], lter [391/1752] Loss: 25.5419\n",
      "Epoch [12/90], lter [401/1752] Loss: 20.4264\n",
      "Epoch [12/90], lter [411/1752] Loss: 25.7592\n",
      "Epoch [12/90], lter [421/1752] Loss: 24.8401\n",
      "Epoch [12/90], lter [431/1752] Loss: 18.1315\n",
      "Epoch [12/90], lter [441/1752] Loss: 19.2112\n",
      "Epoch [12/90], lter [451/1752] Loss: 10.9692\n",
      "Epoch [12/90], lter [461/1752] Loss: 23.0508\n",
      "Epoch [12/90], lter [471/1752] Loss: 32.0599\n",
      "Epoch [12/90], lter [481/1752] Loss: 28.9094\n",
      "Epoch [12/90], lter [491/1752] Loss: 30.8414\n",
      "Epoch [12/90], lter [501/1752] Loss: 27.0851\n",
      "Epoch [12/90], lter [511/1752] Loss: 23.1157\n",
      "Epoch [12/90], lter [521/1752] Loss: 34.6459\n",
      "Epoch [12/90], lter [531/1752] Loss: 18.3768\n",
      "Epoch [12/90], lter [541/1752] Loss: 20.9383\n",
      "Epoch [12/90], lter [551/1752] Loss: 18.9561\n",
      "Epoch [12/90], lter [561/1752] Loss: 24.7066\n",
      "Epoch [12/90], lter [571/1752] Loss: 38.2567\n",
      "Epoch [12/90], lter [581/1752] Loss: 28.8057\n",
      "Epoch [12/90], lter [591/1752] Loss: 25.0588\n",
      "Epoch [12/90], lter [601/1752] Loss: 30.1425\n",
      "Epoch [12/90], lter [611/1752] Loss: 26.8967\n",
      "Epoch [12/90], lter [621/1752] Loss: 33.3887\n",
      "Epoch [12/90], lter [631/1752] Loss: 20.6500\n",
      "Epoch [12/90], lter [641/1752] Loss: 21.5530\n",
      "Epoch [12/90], lter [651/1752] Loss: 19.1365\n",
      "Epoch [12/90], lter [661/1752] Loss: 24.2412\n",
      "Epoch [12/90], lter [671/1752] Loss: 29.7684\n",
      "Epoch [12/90], lter [681/1752] Loss: 28.5601\n",
      "Epoch [12/90], lter [691/1752] Loss: 21.6443\n",
      "Epoch [12/90], lter [701/1752] Loss: 14.4515\n",
      "Epoch [12/90], lter [711/1752] Loss: 26.3108\n",
      "Epoch [12/90], lter [721/1752] Loss: 30.2172\n",
      "Epoch [12/90], lter [731/1752] Loss: 23.5043\n",
      "Epoch [12/90], lter [741/1752] Loss: 27.2553\n",
      "Epoch [12/90], lter [751/1752] Loss: 25.9623\n",
      "Epoch [12/90], lter [761/1752] Loss: 14.0125\n",
      "Epoch [12/90], lter [771/1752] Loss: 24.4446\n",
      "Epoch [12/90], lter [781/1752] Loss: 15.3898\n",
      "Epoch [12/90], lter [791/1752] Loss: 30.4135\n",
      "Epoch [12/90], lter [801/1752] Loss: 27.8364\n",
      "Epoch [12/90], lter [811/1752] Loss: 26.9509\n",
      "Epoch [12/90], lter [821/1752] Loss: 25.0189\n",
      "Epoch [12/90], lter [831/1752] Loss: 16.3551\n",
      "Epoch [12/90], lter [841/1752] Loss: 27.6825\n",
      "Epoch [12/90], lter [851/1752] Loss: 18.1252\n",
      "Epoch [12/90], lter [861/1752] Loss: 28.0672\n",
      "Epoch [12/90], lter [871/1752] Loss: 30.7564\n",
      "Epoch [12/90], lter [881/1752] Loss: 20.5568\n",
      "Epoch [12/90], lter [891/1752] Loss: 29.0606\n",
      "Epoch [12/90], lter [901/1752] Loss: 15.6311\n",
      "Epoch [12/90], lter [911/1752] Loss: 29.7586\n",
      "Epoch [12/90], lter [921/1752] Loss: 20.5212\n",
      "Epoch [12/90], lter [931/1752] Loss: 19.8714\n",
      "Epoch [12/90], lter [941/1752] Loss: 14.1589\n",
      "Epoch [12/90], lter [951/1752] Loss: 26.3385\n",
      "Epoch [12/90], lter [961/1752] Loss: 22.3881\n",
      "Epoch [12/90], lter [971/1752] Loss: 12.0299\n",
      "Epoch [12/90], lter [981/1752] Loss: 28.7040\n",
      "Epoch [12/90], lter [991/1752] Loss: 29.4914\n",
      "Epoch [12/90], lter [1001/1752] Loss: 23.6172\n",
      "Epoch [12/90], lter [1011/1752] Loss: 24.2477\n",
      "Epoch [12/90], lter [1021/1752] Loss: 25.3794\n",
      "Epoch [12/90], lter [1031/1752] Loss: 15.7057\n",
      "Epoch [12/90], lter [1041/1752] Loss: 26.9548\n",
      "Epoch [12/90], lter [1051/1752] Loss: 23.9787\n",
      "Epoch [12/90], lter [1061/1752] Loss: 16.8260\n",
      "Epoch [12/90], lter [1071/1752] Loss: 21.5328\n",
      "Epoch [12/90], lter [1081/1752] Loss: 23.0308\n",
      "Epoch [12/90], lter [1091/1752] Loss: 11.1247\n",
      "Epoch [12/90], lter [1101/1752] Loss: 20.4281\n",
      "Epoch [12/90], lter [1111/1752] Loss: 26.1620\n",
      "Epoch [12/90], lter [1121/1752] Loss: 25.6975\n",
      "Epoch [12/90], lter [1131/1752] Loss: 22.2948\n",
      "Epoch [12/90], lter [1141/1752] Loss: 35.5836\n",
      "Epoch [12/90], lter [1151/1752] Loss: 28.4462\n",
      "Epoch [12/90], lter [1161/1752] Loss: 33.1308\n",
      "Epoch [12/90], lter [1171/1752] Loss: 23.0959\n",
      "Epoch [12/90], lter [1181/1752] Loss: 24.7065\n",
      "Epoch [12/90], lter [1191/1752] Loss: 19.0951\n",
      "Epoch [12/90], lter [1201/1752] Loss: 24.8360\n",
      "Epoch [12/90], lter [1211/1752] Loss: 35.9510\n",
      "Epoch [12/90], lter [1221/1752] Loss: 26.5456\n",
      "Epoch [12/90], lter [1231/1752] Loss: 18.2122\n",
      "Epoch [12/90], lter [1241/1752] Loss: 23.4409\n",
      "Epoch [12/90], lter [1251/1752] Loss: 20.1770\n",
      "Epoch [12/90], lter [1261/1752] Loss: 27.3672\n",
      "Epoch [12/90], lter [1271/1752] Loss: 17.4130\n",
      "Epoch [12/90], lter [1281/1752] Loss: 41.4845\n",
      "Epoch [12/90], lter [1291/1752] Loss: 20.9764\n",
      "Epoch [12/90], lter [1301/1752] Loss: 20.1626\n",
      "Epoch [12/90], lter [1311/1752] Loss: 31.9971\n",
      "Epoch [12/90], lter [1321/1752] Loss: 25.1592\n",
      "Epoch [12/90], lter [1331/1752] Loss: 27.2071\n",
      "Epoch [12/90], lter [1341/1752] Loss: 19.0807\n",
      "Epoch [12/90], lter [1351/1752] Loss: 25.8177\n",
      "Epoch [12/90], lter [1361/1752] Loss: 14.9955\n",
      "Epoch [12/90], lter [1371/1752] Loss: 18.9848\n",
      "Epoch [12/90], lter [1381/1752] Loss: 17.4026\n",
      "Epoch [12/90], lter [1391/1752] Loss: 20.5924\n",
      "Epoch [12/90], lter [1401/1752] Loss: 32.2566\n",
      "Epoch [12/90], lter [1411/1752] Loss: 21.7476\n",
      "Epoch [12/90], lter [1421/1752] Loss: 15.4857\n",
      "Epoch [12/90], lter [1431/1752] Loss: 21.3291\n",
      "Epoch [12/90], lter [1441/1752] Loss: 23.0719\n",
      "Epoch [12/90], lter [1451/1752] Loss: 7.6533\n",
      "Epoch [12/90], lter [1461/1752] Loss: 20.1946\n",
      "Epoch [12/90], lter [1471/1752] Loss: 24.4264\n",
      "Epoch [12/90], lter [1481/1752] Loss: 25.3429\n",
      "Epoch [12/90], lter [1491/1752] Loss: 22.0544\n",
      "Epoch [12/90], lter [1501/1752] Loss: 17.3892\n",
      "Epoch [12/90], lter [1511/1752] Loss: 21.7783\n",
      "Epoch [12/90], lter [1521/1752] Loss: 24.8901\n",
      "Epoch [12/90], lter [1531/1752] Loss: 20.4790\n",
      "Epoch [12/90], lter [1541/1752] Loss: 35.7171\n",
      "Epoch [12/90], lter [1551/1752] Loss: 25.6305\n",
      "Epoch [12/90], lter [1561/1752] Loss: 23.8587\n",
      "Epoch [12/90], lter [1571/1752] Loss: 33.6168\n",
      "Epoch [12/90], lter [1581/1752] Loss: 26.0531\n",
      "Epoch [12/90], lter [1591/1752] Loss: 17.2178\n",
      "Epoch [12/90], lter [1601/1752] Loss: 23.2778\n",
      "Epoch [12/90], lter [1611/1752] Loss: 27.7629\n",
      "Epoch [12/90], lter [1621/1752] Loss: 25.4694\n",
      "Epoch [12/90], lter [1631/1752] Loss: 27.8367\n",
      "Epoch [12/90], lter [1641/1752] Loss: 27.3113\n",
      "Epoch [12/90], lter [1651/1752] Loss: 14.5248\n",
      "Epoch [12/90], lter [1661/1752] Loss: 17.1107\n",
      "Epoch [12/90], lter [1671/1752] Loss: 30.5597\n",
      "Epoch [12/90], lter [1681/1752] Loss: 19.4159\n",
      "Epoch [12/90], lter [1691/1752] Loss: 26.0122\n",
      "Epoch [12/90], lter [1701/1752] Loss: 28.0695\n",
      "Epoch [12/90], lter [1711/1752] Loss: 22.1659\n",
      "Epoch [12/90], lter [1721/1752] Loss: 16.3179\n",
      "Epoch [12/90], lter [1731/1752] Loss: 22.2910\n",
      "Epoch [12/90], lter [1741/1752] Loss: 20.0640\n",
      "Epoch [12/90], lter [1751/1752] Loss: 13.5185\n",
      "Epoch:  12 | train loss : 24.2295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12/90 [65:23:40<47:37:38, 2198.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11 | test loss : 11.4923\n",
      "Epoch [13/90], lter [1/1752] Loss: 29.2675\n",
      "Epoch [13/90], lter [11/1752] Loss: 24.8709\n",
      "Epoch [13/90], lter [21/1752] Loss: 23.9324\n",
      "Epoch [13/90], lter [31/1752] Loss: 27.1702\n",
      "Epoch [13/90], lter [41/1752] Loss: 34.3474\n",
      "Epoch [13/90], lter [51/1752] Loss: 32.0224\n",
      "Epoch [13/90], lter [61/1752] Loss: 22.9661\n",
      "Epoch [13/90], lter [71/1752] Loss: 28.4168\n",
      "Epoch [13/90], lter [81/1752] Loss: 22.8581\n",
      "Epoch [13/90], lter [91/1752] Loss: 16.5369\n",
      "Epoch [13/90], lter [101/1752] Loss: 14.3372\n",
      "Epoch [13/90], lter [111/1752] Loss: 21.2829\n",
      "Epoch [13/90], lter [121/1752] Loss: 25.1121\n",
      "Epoch [13/90], lter [131/1752] Loss: 26.1837\n",
      "Epoch [13/90], lter [141/1752] Loss: 21.7818\n",
      "Epoch [13/90], lter [151/1752] Loss: 15.2350\n",
      "Epoch [13/90], lter [161/1752] Loss: 15.3559\n",
      "Epoch [13/90], lter [171/1752] Loss: 24.5682\n",
      "Epoch [13/90], lter [181/1752] Loss: 25.6931\n",
      "Epoch [13/90], lter [191/1752] Loss: 22.8164\n",
      "Epoch [13/90], lter [201/1752] Loss: 15.1911\n",
      "Epoch [13/90], lter [211/1752] Loss: 17.6775\n",
      "Epoch [13/90], lter [221/1752] Loss: 35.3497\n",
      "Epoch [13/90], lter [231/1752] Loss: 15.6846\n",
      "Epoch [13/90], lter [241/1752] Loss: 19.1979\n",
      "Epoch [13/90], lter [251/1752] Loss: 41.4599\n",
      "Epoch [13/90], lter [261/1752] Loss: 21.3400\n",
      "Epoch [13/90], lter [271/1752] Loss: 22.6567\n",
      "Epoch [13/90], lter [281/1752] Loss: 25.2431\n",
      "Epoch [13/90], lter [291/1752] Loss: 19.9257\n",
      "Epoch [13/90], lter [301/1752] Loss: 21.5463\n",
      "Epoch [13/90], lter [311/1752] Loss: 19.8354\n",
      "Epoch [13/90], lter [321/1752] Loss: 23.1484\n",
      "Epoch [13/90], lter [331/1752] Loss: 21.7181\n",
      "Epoch [13/90], lter [341/1752] Loss: 31.3386\n",
      "Epoch [13/90], lter [351/1752] Loss: 23.2503\n",
      "Epoch [13/90], lter [361/1752] Loss: 25.6897\n",
      "Epoch [13/90], lter [371/1752] Loss: 18.0649\n",
      "Epoch [13/90], lter [381/1752] Loss: 17.3399\n",
      "Epoch [13/90], lter [391/1752] Loss: 22.5981\n",
      "Epoch [13/90], lter [401/1752] Loss: 12.9144\n",
      "Epoch [13/90], lter [411/1752] Loss: 22.5938\n",
      "Epoch [13/90], lter [421/1752] Loss: 33.3351\n",
      "Epoch [13/90], lter [431/1752] Loss: 16.4385\n",
      "Epoch [13/90], lter [441/1752] Loss: 24.1822\n",
      "Epoch [13/90], lter [451/1752] Loss: 19.5244\n",
      "Epoch [13/90], lter [461/1752] Loss: 17.2988\n",
      "Epoch [13/90], lter [471/1752] Loss: 28.7816\n",
      "Epoch [13/90], lter [481/1752] Loss: 28.2624\n",
      "Epoch [13/90], lter [491/1752] Loss: 14.9769\n",
      "Epoch [13/90], lter [501/1752] Loss: 30.9678\n",
      "Epoch [13/90], lter [511/1752] Loss: 25.0282\n",
      "Epoch [13/90], lter [521/1752] Loss: 25.1788\n",
      "Epoch [13/90], lter [531/1752] Loss: 26.0626\n",
      "Epoch [13/90], lter [541/1752] Loss: 50.1370\n",
      "Epoch [13/90], lter [551/1752] Loss: 36.2479\n",
      "Epoch [13/90], lter [561/1752] Loss: 22.9093\n",
      "Epoch [13/90], lter [571/1752] Loss: 18.5010\n",
      "Epoch [13/90], lter [581/1752] Loss: 28.1980\n",
      "Epoch [13/90], lter [591/1752] Loss: 23.8284\n",
      "Epoch [13/90], lter [601/1752] Loss: 29.2533\n",
      "Epoch [13/90], lter [611/1752] Loss: 19.7739\n",
      "Epoch [13/90], lter [621/1752] Loss: 19.2158\n",
      "Epoch [13/90], lter [631/1752] Loss: 18.9031\n",
      "Epoch [13/90], lter [641/1752] Loss: 22.5839\n",
      "Epoch [13/90], lter [651/1752] Loss: 25.3830\n",
      "Epoch [13/90], lter [661/1752] Loss: 22.8605\n",
      "Epoch [13/90], lter [671/1752] Loss: 10.8764\n",
      "Epoch [13/90], lter [681/1752] Loss: 24.6881\n",
      "Epoch [13/90], lter [691/1752] Loss: 22.7493\n",
      "Epoch [13/90], lter [701/1752] Loss: 12.5190\n",
      "Epoch [13/90], lter [711/1752] Loss: 13.1391\n",
      "Epoch [13/90], lter [721/1752] Loss: 19.2746\n",
      "Epoch [13/90], lter [731/1752] Loss: 17.8660\n",
      "Epoch [13/90], lter [741/1752] Loss: 22.7622\n",
      "Epoch [13/90], lter [751/1752] Loss: 32.9183\n",
      "Epoch [13/90], lter [761/1752] Loss: 14.4348\n",
      "Epoch [13/90], lter [771/1752] Loss: 17.1225\n",
      "Epoch [13/90], lter [781/1752] Loss: 33.0289\n",
      "Epoch [13/90], lter [791/1752] Loss: 29.5463\n",
      "Epoch [13/90], lter [801/1752] Loss: 23.2511\n",
      "Epoch [13/90], lter [811/1752] Loss: 26.3079\n",
      "Epoch [13/90], lter [821/1752] Loss: 23.8454\n",
      "Epoch [13/90], lter [831/1752] Loss: 19.3635\n",
      "Epoch [13/90], lter [841/1752] Loss: 15.3053\n",
      "Epoch [13/90], lter [851/1752] Loss: 17.9599\n",
      "Epoch [13/90], lter [861/1752] Loss: 17.8888\n",
      "Epoch [13/90], lter [871/1752] Loss: 26.7548\n",
      "Epoch [13/90], lter [881/1752] Loss: 27.5765\n",
      "Epoch [13/90], lter [891/1752] Loss: 22.9705\n",
      "Epoch [13/90], lter [901/1752] Loss: 22.9317\n",
      "Epoch [13/90], lter [911/1752] Loss: 16.9267\n",
      "Epoch [13/90], lter [921/1752] Loss: 23.2878\n",
      "Epoch [13/90], lter [931/1752] Loss: 25.1121\n",
      "Epoch [13/90], lter [941/1752] Loss: 18.9011\n",
      "Epoch [13/90], lter [951/1752] Loss: 23.5100\n",
      "Epoch [13/90], lter [961/1752] Loss: 12.7997\n",
      "Epoch [13/90], lter [971/1752] Loss: 25.6070\n",
      "Epoch [13/90], lter [981/1752] Loss: 32.5059\n",
      "Epoch [13/90], lter [991/1752] Loss: 27.5798\n",
      "Epoch [13/90], lter [1001/1752] Loss: 15.2678\n",
      "Epoch [13/90], lter [1011/1752] Loss: 28.5675\n",
      "Epoch [13/90], lter [1021/1752] Loss: 15.4978\n",
      "Epoch [13/90], lter [1031/1752] Loss: 27.3470\n",
      "Epoch [13/90], lter [1041/1752] Loss: 31.2099\n",
      "Epoch [13/90], lter [1051/1752] Loss: 28.3441\n",
      "Epoch [13/90], lter [1061/1752] Loss: 21.4645\n",
      "Epoch [13/90], lter [1071/1752] Loss: 21.0202\n",
      "Epoch [13/90], lter [1081/1752] Loss: 31.3672\n",
      "Epoch [13/90], lter [1091/1752] Loss: 24.6619\n",
      "Epoch [13/90], lter [1101/1752] Loss: 14.8238\n",
      "Epoch [13/90], lter [1111/1752] Loss: 30.8928\n",
      "Epoch [13/90], lter [1121/1752] Loss: 18.0779\n",
      "Epoch [13/90], lter [1131/1752] Loss: 15.4336\n",
      "Epoch [13/90], lter [1141/1752] Loss: 21.6940\n",
      "Epoch [13/90], lter [1151/1752] Loss: 21.2999\n",
      "Epoch [13/90], lter [1161/1752] Loss: 19.2508\n",
      "Epoch [13/90], lter [1171/1752] Loss: 28.8704\n",
      "Epoch [13/90], lter [1181/1752] Loss: 29.9088\n",
      "Epoch [13/90], lter [1191/1752] Loss: 26.7713\n",
      "Epoch [13/90], lter [1201/1752] Loss: 11.5180\n",
      "Epoch [13/90], lter [1211/1752] Loss: 20.4243\n",
      "Epoch [13/90], lter [1221/1752] Loss: 23.8160\n",
      "Epoch [13/90], lter [1231/1752] Loss: 25.6679\n",
      "Epoch [13/90], lter [1241/1752] Loss: 18.9848\n",
      "Epoch [13/90], lter [1251/1752] Loss: 35.1382\n",
      "Epoch [13/90], lter [1261/1752] Loss: 31.8875\n",
      "Epoch [13/90], lter [1271/1752] Loss: 23.4520\n",
      "Epoch [13/90], lter [1281/1752] Loss: 20.5576\n",
      "Epoch [13/90], lter [1291/1752] Loss: 27.1060\n",
      "Epoch [13/90], lter [1301/1752] Loss: 21.6647\n",
      "Epoch [13/90], lter [1311/1752] Loss: 34.6727\n",
      "Epoch [13/90], lter [1321/1752] Loss: 24.3513\n",
      "Epoch [13/90], lter [1331/1752] Loss: 21.3198\n",
      "Epoch [13/90], lter [1341/1752] Loss: 21.9049\n",
      "Epoch [13/90], lter [1351/1752] Loss: 25.1965\n",
      "Epoch [13/90], lter [1361/1752] Loss: 16.8760\n",
      "Epoch [13/90], lter [1371/1752] Loss: 18.1921\n",
      "Epoch [13/90], lter [1381/1752] Loss: 23.7476\n",
      "Epoch [13/90], lter [1391/1752] Loss: 20.7342\n",
      "Epoch [13/90], lter [1401/1752] Loss: 21.9699\n",
      "Epoch [13/90], lter [1411/1752] Loss: 32.2034\n",
      "Epoch [13/90], lter [1421/1752] Loss: 27.9375\n",
      "Epoch [13/90], lter [1431/1752] Loss: 34.2028\n",
      "Epoch [13/90], lter [1441/1752] Loss: 16.1753\n",
      "Epoch [13/90], lter [1451/1752] Loss: 22.1851\n",
      "Epoch [13/90], lter [1461/1752] Loss: 20.8926\n",
      "Epoch [13/90], lter [1471/1752] Loss: 26.9363\n",
      "Epoch [13/90], lter [1481/1752] Loss: 16.1151\n",
      "Epoch [13/90], lter [1491/1752] Loss: 28.9199\n",
      "Epoch [13/90], lter [1501/1752] Loss: 28.5698\n",
      "Epoch [13/90], lter [1511/1752] Loss: 19.9243\n",
      "Epoch [13/90], lter [1521/1752] Loss: 34.3747\n",
      "Epoch [13/90], lter [1531/1752] Loss: 25.7519\n",
      "Epoch [13/90], lter [1541/1752] Loss: 28.3927\n",
      "Epoch [13/90], lter [1551/1752] Loss: 35.9575\n",
      "Epoch [13/90], lter [1561/1752] Loss: 33.0891\n",
      "Epoch [13/90], lter [1571/1752] Loss: 40.8926\n",
      "Epoch [13/90], lter [1581/1752] Loss: 22.3619\n",
      "Epoch [13/90], lter [1591/1752] Loss: 33.2923\n",
      "Epoch [13/90], lter [1601/1752] Loss: 23.6581\n",
      "Epoch [13/90], lter [1611/1752] Loss: 41.6496\n",
      "Epoch [13/90], lter [1621/1752] Loss: 44.3514\n",
      "Epoch [13/90], lter [1631/1752] Loss: 21.2807\n",
      "Epoch [13/90], lter [1641/1752] Loss: 22.7204\n",
      "Epoch [13/90], lter [1651/1752] Loss: 20.1273\n",
      "Epoch [13/90], lter [1661/1752] Loss: 28.5420\n",
      "Epoch [13/90], lter [1671/1752] Loss: 29.8417\n",
      "Epoch [13/90], lter [1681/1752] Loss: 24.4810\n",
      "Epoch [13/90], lter [1691/1752] Loss: 21.8482\n",
      "Epoch [13/90], lter [1701/1752] Loss: 29.2502\n",
      "Epoch [13/90], lter [1711/1752] Loss: 25.3436\n",
      "Epoch [13/90], lter [1721/1752] Loss: 23.6519\n",
      "Epoch [13/90], lter [1731/1752] Loss: 22.6423\n",
      "Epoch [13/90], lter [1741/1752] Loss: 18.8615\n",
      "Epoch [13/90], lter [1751/1752] Loss: 22.9068\n",
      "Epoch:  13 | train loss : 24.7797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 13/90 [65:37:49<38:16:37, 1789.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12 | test loss : 17.1301\n",
      "Epoch [14/90], lter [1/1752] Loss: 30.7084\n",
      "Epoch [14/90], lter [11/1752] Loss: 30.0702\n",
      "Epoch [14/90], lter [21/1752] Loss: 27.0275\n",
      "Epoch [14/90], lter [31/1752] Loss: 27.8906\n",
      "Epoch [14/90], lter [41/1752] Loss: 31.1888\n",
      "Epoch [14/90], lter [51/1752] Loss: 18.8365\n",
      "Epoch [14/90], lter [61/1752] Loss: 22.5705\n",
      "Epoch [14/90], lter [71/1752] Loss: 19.2207\n",
      "Epoch [14/90], lter [81/1752] Loss: 30.0771\n",
      "Epoch [14/90], lter [91/1752] Loss: 26.9933\n",
      "Epoch [14/90], lter [101/1752] Loss: 19.5009\n",
      "Epoch [14/90], lter [111/1752] Loss: 17.5193\n",
      "Epoch [14/90], lter [121/1752] Loss: 27.7395\n",
      "Epoch [14/90], lter [131/1752] Loss: 28.3039\n",
      "Epoch [14/90], lter [141/1752] Loss: 23.5404\n",
      "Epoch [14/90], lter [151/1752] Loss: 24.4486\n",
      "Epoch [14/90], lter [161/1752] Loss: 18.8094\n",
      "Epoch [14/90], lter [171/1752] Loss: 22.4524\n",
      "Epoch [14/90], lter [181/1752] Loss: 17.4920\n",
      "Epoch [14/90], lter [191/1752] Loss: 17.4391\n",
      "Epoch [14/90], lter [201/1752] Loss: 22.5592\n",
      "Epoch [14/90], lter [211/1752] Loss: 31.1566\n",
      "Epoch [14/90], lter [221/1752] Loss: 25.9393\n",
      "Epoch [14/90], lter [231/1752] Loss: 24.9894\n",
      "Epoch [14/90], lter [241/1752] Loss: 23.4323\n",
      "Epoch [14/90], lter [251/1752] Loss: 24.6262\n",
      "Epoch [14/90], lter [261/1752] Loss: 16.5115\n",
      "Epoch [14/90], lter [271/1752] Loss: 23.3606\n",
      "Epoch [14/90], lter [281/1752] Loss: 41.6352\n",
      "Epoch [14/90], lter [291/1752] Loss: 23.8974\n",
      "Epoch [14/90], lter [301/1752] Loss: 25.1745\n",
      "Epoch [14/90], lter [311/1752] Loss: 21.5055\n",
      "Epoch [14/90], lter [321/1752] Loss: 26.6103\n",
      "Epoch [14/90], lter [331/1752] Loss: 24.6378\n",
      "Epoch [14/90], lter [341/1752] Loss: 25.5480\n",
      "Epoch [14/90], lter [351/1752] Loss: 34.4502\n",
      "Epoch [14/90], lter [361/1752] Loss: 14.5974\n",
      "Epoch [14/90], lter [371/1752] Loss: 22.3649\n",
      "Epoch [14/90], lter [381/1752] Loss: 26.1071\n",
      "Epoch [14/90], lter [391/1752] Loss: 21.4916\n",
      "Epoch [14/90], lter [401/1752] Loss: 11.8987\n",
      "Epoch [14/90], lter [411/1752] Loss: 28.8349\n",
      "Epoch [14/90], lter [421/1752] Loss: 26.7385\n",
      "Epoch [14/90], lter [431/1752] Loss: 22.9578\n",
      "Epoch [14/90], lter [441/1752] Loss: 23.5952\n",
      "Epoch [14/90], lter [451/1752] Loss: 25.6487\n",
      "Epoch [14/90], lter [461/1752] Loss: 22.2120\n",
      "Epoch [14/90], lter [471/1752] Loss: 31.9683\n",
      "Epoch [14/90], lter [481/1752] Loss: 30.6531\n",
      "Epoch [14/90], lter [491/1752] Loss: 26.2693\n",
      "Epoch [14/90], lter [501/1752] Loss: 13.0039\n",
      "Epoch [14/90], lter [511/1752] Loss: 21.6706\n",
      "Epoch [14/90], lter [521/1752] Loss: 18.7786\n",
      "Epoch [14/90], lter [531/1752] Loss: 33.0827\n",
      "Epoch [14/90], lter [541/1752] Loss: 48.7167\n",
      "Epoch [14/90], lter [551/1752] Loss: 48.3751\n",
      "Epoch [14/90], lter [561/1752] Loss: 18.7164\n",
      "Epoch [14/90], lter [571/1752] Loss: 26.5990\n",
      "Epoch [14/90], lter [581/1752] Loss: 19.7384\n",
      "Epoch [14/90], lter [591/1752] Loss: 25.5155\n",
      "Epoch [14/90], lter [601/1752] Loss: 17.5570\n",
      "Epoch [14/90], lter [611/1752] Loss: 22.6615\n",
      "Epoch [14/90], lter [621/1752] Loss: 32.6547\n",
      "Epoch [14/90], lter [631/1752] Loss: 15.3067\n",
      "Epoch [14/90], lter [641/1752] Loss: 26.5881\n",
      "Epoch [14/90], lter [651/1752] Loss: 18.6143\n",
      "Epoch [14/90], lter [661/1752] Loss: 28.1126\n",
      "Epoch [14/90], lter [671/1752] Loss: 21.1131\n",
      "Epoch [14/90], lter [681/1752] Loss: 27.3825\n",
      "Epoch [14/90], lter [691/1752] Loss: 23.8514\n",
      "Epoch [14/90], lter [701/1752] Loss: 25.9281\n",
      "Epoch [14/90], lter [711/1752] Loss: 28.6338\n",
      "Epoch [14/90], lter [721/1752] Loss: 26.0112\n",
      "Epoch [14/90], lter [731/1752] Loss: 29.3524\n",
      "Epoch [14/90], lter [741/1752] Loss: 22.2335\n",
      "Epoch [14/90], lter [751/1752] Loss: 29.3485\n",
      "Epoch [14/90], lter [761/1752] Loss: 23.2050\n",
      "Epoch [14/90], lter [771/1752] Loss: 25.7820\n",
      "Epoch [14/90], lter [781/1752] Loss: 25.0319\n",
      "Epoch [14/90], lter [791/1752] Loss: 24.7426\n",
      "Epoch [14/90], lter [801/1752] Loss: 32.9196\n",
      "Epoch [14/90], lter [811/1752] Loss: 14.6744\n",
      "Epoch [14/90], lter [821/1752] Loss: 14.4096\n",
      "Epoch [14/90], lter [831/1752] Loss: 21.5987\n",
      "Epoch [14/90], lter [841/1752] Loss: 31.5464\n",
      "Epoch [14/90], lter [851/1752] Loss: 24.0275\n",
      "Epoch [14/90], lter [861/1752] Loss: 30.3451\n",
      "Epoch [14/90], lter [871/1752] Loss: 27.3986\n",
      "Epoch [14/90], lter [881/1752] Loss: 13.2048\n",
      "Epoch [14/90], lter [891/1752] Loss: 22.1505\n",
      "Epoch [14/90], lter [901/1752] Loss: 25.1056\n",
      "Epoch [14/90], lter [911/1752] Loss: 27.0964\n",
      "Epoch [14/90], lter [921/1752] Loss: 25.9515\n",
      "Epoch [14/90], lter [931/1752] Loss: 16.4529\n",
      "Epoch [14/90], lter [941/1752] Loss: 26.8572\n",
      "Epoch [14/90], lter [951/1752] Loss: 21.7533\n",
      "Epoch [14/90], lter [961/1752] Loss: 19.7561\n",
      "Epoch [14/90], lter [971/1752] Loss: 21.4175\n",
      "Epoch [14/90], lter [981/1752] Loss: 22.6077\n",
      "Epoch [14/90], lter [991/1752] Loss: 19.5306\n",
      "Epoch [14/90], lter [1001/1752] Loss: 30.5025\n",
      "Epoch [14/90], lter [1011/1752] Loss: 15.9956\n",
      "Epoch [14/90], lter [1021/1752] Loss: 17.5243\n",
      "Epoch [14/90], lter [1031/1752] Loss: 15.2293\n",
      "Epoch [14/90], lter [1041/1752] Loss: 19.8116\n",
      "Epoch [14/90], lter [1051/1752] Loss: 18.5377\n",
      "Epoch [14/90], lter [1061/1752] Loss: 19.0710\n",
      "Epoch [14/90], lter [1071/1752] Loss: 14.4554\n",
      "Epoch [14/90], lter [1081/1752] Loss: 29.2975\n",
      "Epoch [14/90], lter [1091/1752] Loss: 27.8181\n",
      "Epoch [14/90], lter [1101/1752] Loss: 18.6800\n",
      "Epoch [14/90], lter [1111/1752] Loss: 17.7486\n",
      "Epoch [14/90], lter [1121/1752] Loss: 28.8241\n",
      "Epoch [14/90], lter [1131/1752] Loss: 27.3600\n",
      "Epoch [14/90], lter [1141/1752] Loss: 16.3184\n",
      "Epoch [14/90], lter [1151/1752] Loss: 32.7727\n",
      "Epoch [14/90], lter [1161/1752] Loss: 30.6112\n",
      "Epoch [14/90], lter [1171/1752] Loss: 13.5803\n",
      "Epoch [14/90], lter [1181/1752] Loss: 17.8286\n",
      "Epoch [14/90], lter [1191/1752] Loss: 36.2754\n",
      "Epoch [14/90], lter [1201/1752] Loss: 36.1076\n",
      "Epoch [14/90], lter [1211/1752] Loss: 34.4359\n",
      "Epoch [14/90], lter [1221/1752] Loss: 17.1970\n",
      "Epoch [14/90], lter [1231/1752] Loss: 24.2540\n",
      "Epoch [14/90], lter [1241/1752] Loss: 17.3776\n",
      "Epoch [14/90], lter [1251/1752] Loss: 32.1561\n",
      "Epoch [14/90], lter [1261/1752] Loss: 23.3113\n",
      "Epoch [14/90], lter [1271/1752] Loss: 23.4928\n",
      "Epoch [14/90], lter [1281/1752] Loss: 36.2764\n",
      "Epoch [14/90], lter [1291/1752] Loss: 36.7852\n",
      "Epoch [14/90], lter [1301/1752] Loss: 36.8765\n",
      "Epoch [14/90], lter [1311/1752] Loss: 19.7266\n",
      "Epoch [14/90], lter [1321/1752] Loss: 19.7805\n",
      "Epoch [14/90], lter [1331/1752] Loss: 20.0807\n",
      "Epoch [14/90], lter [1341/1752] Loss: 39.5277\n",
      "Epoch [14/90], lter [1351/1752] Loss: 24.1867\n",
      "Epoch [14/90], lter [1361/1752] Loss: 25.0924\n",
      "Epoch [14/90], lter [1371/1752] Loss: 24.0019\n",
      "Epoch [14/90], lter [1381/1752] Loss: 29.4764\n",
      "Epoch [14/90], lter [1391/1752] Loss: 29.4904\n",
      "Epoch [14/90], lter [1401/1752] Loss: 20.2621\n",
      "Epoch [14/90], lter [1411/1752] Loss: 21.9943\n",
      "Epoch [14/90], lter [1421/1752] Loss: 14.2322\n",
      "Epoch [14/90], lter [1431/1752] Loss: 24.2311\n",
      "Epoch [14/90], lter [1441/1752] Loss: 19.3928\n",
      "Epoch [14/90], lter [1451/1752] Loss: 15.9588\n",
      "Epoch [14/90], lter [1461/1752] Loss: 24.6626\n",
      "Epoch [14/90], lter [1471/1752] Loss: 25.8261\n",
      "Epoch [14/90], lter [1481/1752] Loss: 10.5828\n",
      "Epoch [14/90], lter [1491/1752] Loss: 19.0178\n",
      "Epoch [14/90], lter [1501/1752] Loss: 22.7991\n",
      "Epoch [14/90], lter [1511/1752] Loss: 26.7315\n",
      "Epoch [14/90], lter [1521/1752] Loss: 27.1265\n",
      "Epoch [14/90], lter [1531/1752] Loss: 20.0954\n",
      "Epoch [14/90], lter [1541/1752] Loss: 25.8650\n",
      "Epoch [14/90], lter [1551/1752] Loss: 17.5749\n",
      "Epoch [14/90], lter [1561/1752] Loss: 18.7260\n",
      "Epoch [14/90], lter [1571/1752] Loss: 28.4454\n",
      "Epoch [14/90], lter [1581/1752] Loss: 23.1689\n",
      "Epoch [14/90], lter [1591/1752] Loss: 22.3706\n",
      "Epoch [14/90], lter [1601/1752] Loss: 18.1548\n",
      "Epoch [14/90], lter [1611/1752] Loss: 23.3190\n",
      "Epoch [14/90], lter [1621/1752] Loss: 11.0727\n",
      "Epoch [14/90], lter [1631/1752] Loss: 23.9708\n",
      "Epoch [14/90], lter [1641/1752] Loss: 23.4210\n",
      "Epoch [14/90], lter [1651/1752] Loss: 29.4301\n",
      "Epoch [14/90], lter [1661/1752] Loss: 22.8424\n",
      "Epoch [14/90], lter [1671/1752] Loss: 26.3548\n",
      "Epoch [14/90], lter [1681/1752] Loss: 33.6107\n",
      "Epoch [14/90], lter [1691/1752] Loss: 21.1435\n",
      "Epoch [14/90], lter [1701/1752] Loss: 24.2438\n",
      "Epoch [14/90], lter [1711/1752] Loss: 28.2557\n",
      "Epoch [14/90], lter [1721/1752] Loss: 20.3750\n",
      "Epoch [14/90], lter [1731/1752] Loss: 30.3254\n",
      "Epoch [14/90], lter [1741/1752] Loss: 23.5492\n",
      "Epoch [14/90], lter [1751/1752] Loss: 22.0993\n",
      "Epoch:  14 | train loss : 24.1586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 14/90 [65:51:58<31:46:45, 1505.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13 | test loss : 14.5198\n",
      "Epoch [15/90], lter [1/1752] Loss: 31.0789\n",
      "Epoch [15/90], lter [11/1752] Loss: 18.4658\n",
      "Epoch [15/90], lter [21/1752] Loss: 28.7305\n",
      "Epoch [15/90], lter [31/1752] Loss: 21.9323\n",
      "Epoch [15/90], lter [41/1752] Loss: 23.3609\n",
      "Epoch [15/90], lter [51/1752] Loss: 30.9635\n",
      "Epoch [15/90], lter [61/1752] Loss: 23.8662\n",
      "Epoch [15/90], lter [71/1752] Loss: 35.4904\n",
      "Epoch [15/90], lter [81/1752] Loss: 18.3699\n",
      "Epoch [15/90], lter [91/1752] Loss: 30.2433\n",
      "Epoch [15/90], lter [101/1752] Loss: 30.9602\n",
      "Epoch [15/90], lter [111/1752] Loss: 22.8016\n",
      "Epoch [15/90], lter [121/1752] Loss: 25.9303\n",
      "Epoch [15/90], lter [131/1752] Loss: 31.1898\n",
      "Epoch [15/90], lter [141/1752] Loss: 27.6796\n",
      "Epoch [15/90], lter [151/1752] Loss: 24.8518\n",
      "Epoch [15/90], lter [161/1752] Loss: 20.0852\n",
      "Epoch [15/90], lter [171/1752] Loss: 19.9879\n",
      "Epoch [15/90], lter [181/1752] Loss: 29.3202\n",
      "Epoch [15/90], lter [191/1752] Loss: 29.9732\n",
      "Epoch [15/90], lter [201/1752] Loss: 18.9980\n",
      "Epoch [15/90], lter [211/1752] Loss: 33.3165\n",
      "Epoch [15/90], lter [221/1752] Loss: 25.7634\n",
      "Epoch [15/90], lter [231/1752] Loss: 28.9860\n",
      "Epoch [15/90], lter [241/1752] Loss: 14.0115\n",
      "Epoch [15/90], lter [251/1752] Loss: 19.5270\n",
      "Epoch [15/90], lter [261/1752] Loss: 16.7322\n",
      "Epoch [15/90], lter [271/1752] Loss: 36.7928\n",
      "Epoch [15/90], lter [281/1752] Loss: 28.7472\n",
      "Epoch [15/90], lter [291/1752] Loss: 14.3472\n",
      "Epoch [15/90], lter [301/1752] Loss: 16.7033\n",
      "Epoch [15/90], lter [311/1752] Loss: 33.6524\n",
      "Epoch [15/90], lter [321/1752] Loss: 18.8429\n",
      "Epoch [15/90], lter [331/1752] Loss: 11.4600\n",
      "Epoch [15/90], lter [341/1752] Loss: 13.3197\n",
      "Epoch [15/90], lter [351/1752] Loss: 23.2143\n",
      "Epoch [15/90], lter [361/1752] Loss: 37.9782\n",
      "Epoch [15/90], lter [371/1752] Loss: 19.0062\n",
      "Epoch [15/90], lter [381/1752] Loss: 30.5582\n",
      "Epoch [15/90], lter [391/1752] Loss: 19.2428\n",
      "Epoch [15/90], lter [401/1752] Loss: 13.7729\n",
      "Epoch [15/90], lter [411/1752] Loss: 26.6536\n",
      "Epoch [15/90], lter [421/1752] Loss: 26.2713\n",
      "Epoch [15/90], lter [431/1752] Loss: 21.9956\n",
      "Epoch [15/90], lter [441/1752] Loss: 16.7679\n",
      "Epoch [15/90], lter [451/1752] Loss: 35.6839\n",
      "Epoch [15/90], lter [461/1752] Loss: 26.0063\n",
      "Epoch [15/90], lter [471/1752] Loss: 21.0035\n",
      "Epoch [15/90], lter [481/1752] Loss: 18.0542\n",
      "Epoch [15/90], lter [491/1752] Loss: 32.4471\n",
      "Epoch [15/90], lter [501/1752] Loss: 22.2611\n",
      "Epoch [15/90], lter [511/1752] Loss: 14.7966\n",
      "Epoch [15/90], lter [521/1752] Loss: 32.5011\n",
      "Epoch [15/90], lter [531/1752] Loss: 17.6074\n",
      "Epoch [15/90], lter [541/1752] Loss: 21.5677\n",
      "Epoch [15/90], lter [551/1752] Loss: 23.6447\n",
      "Epoch [15/90], lter [561/1752] Loss: 26.9455\n",
      "Epoch [15/90], lter [571/1752] Loss: 22.1128\n",
      "Epoch [15/90], lter [581/1752] Loss: 21.0656\n",
      "Epoch [15/90], lter [591/1752] Loss: 25.2088\n",
      "Epoch [15/90], lter [601/1752] Loss: 22.0650\n",
      "Epoch [15/90], lter [611/1752] Loss: 16.5653\n",
      "Epoch [15/90], lter [621/1752] Loss: 32.9073\n",
      "Epoch [15/90], lter [631/1752] Loss: 49.0846\n",
      "Epoch [15/90], lter [641/1752] Loss: 21.1857\n",
      "Epoch [15/90], lter [651/1752] Loss: 24.0356\n",
      "Epoch [15/90], lter [661/1752] Loss: 23.0326\n",
      "Epoch [15/90], lter [671/1752] Loss: 21.4837\n",
      "Epoch [15/90], lter [681/1752] Loss: 23.4908\n",
      "Epoch [15/90], lter [691/1752] Loss: 25.2925\n",
      "Epoch [15/90], lter [701/1752] Loss: 10.7487\n",
      "Epoch [15/90], lter [711/1752] Loss: 29.7017\n",
      "Epoch [15/90], lter [721/1752] Loss: 21.5922\n",
      "Epoch [15/90], lter [731/1752] Loss: 19.8894\n",
      "Epoch [15/90], lter [741/1752] Loss: 23.7566\n",
      "Epoch [15/90], lter [751/1752] Loss: 11.2080\n",
      "Epoch [15/90], lter [761/1752] Loss: 23.0379\n",
      "Epoch [15/90], lter [771/1752] Loss: 24.3672\n",
      "Epoch [15/90], lter [781/1752] Loss: 30.5388\n",
      "Epoch [15/90], lter [791/1752] Loss: 15.5855\n",
      "Epoch [15/90], lter [801/1752] Loss: 21.1309\n",
      "Epoch [15/90], lter [811/1752] Loss: 34.8142\n",
      "Epoch [15/90], lter [821/1752] Loss: 35.1438\n",
      "Epoch [15/90], lter [831/1752] Loss: 25.6059\n",
      "Epoch [15/90], lter [841/1752] Loss: 23.8915\n",
      "Epoch [15/90], lter [851/1752] Loss: 30.0579\n",
      "Epoch [15/90], lter [861/1752] Loss: 33.7166\n",
      "Epoch [15/90], lter [871/1752] Loss: 17.1483\n",
      "Epoch [15/90], lter [881/1752] Loss: 29.9390\n",
      "Epoch [15/90], lter [891/1752] Loss: 22.4275\n",
      "Epoch [15/90], lter [901/1752] Loss: 19.7868\n",
      "Epoch [15/90], lter [911/1752] Loss: 26.3244\n",
      "Epoch [15/90], lter [921/1752] Loss: 23.1111\n",
      "Epoch [15/90], lter [931/1752] Loss: 43.6588\n",
      "Epoch [15/90], lter [941/1752] Loss: 24.7586\n",
      "Epoch [15/90], lter [951/1752] Loss: 27.1251\n",
      "Epoch [15/90], lter [961/1752] Loss: 26.0424\n",
      "Epoch [15/90], lter [971/1752] Loss: 16.6870\n",
      "Epoch [15/90], lter [981/1752] Loss: 22.5140\n",
      "Epoch [15/90], lter [991/1752] Loss: 26.9273\n",
      "Epoch [15/90], lter [1001/1752] Loss: 17.5606\n",
      "Epoch [15/90], lter [1011/1752] Loss: 29.7663\n",
      "Epoch [15/90], lter [1021/1752] Loss: 33.3138\n",
      "Epoch [15/90], lter [1031/1752] Loss: 23.3877\n",
      "Epoch [15/90], lter [1041/1752] Loss: 17.4410\n",
      "Epoch [15/90], lter [1051/1752] Loss: 16.7302\n",
      "Epoch [15/90], lter [1061/1752] Loss: 21.5996\n",
      "Epoch [15/90], lter [1071/1752] Loss: 23.2841\n",
      "Epoch [15/90], lter [1081/1752] Loss: 24.4239\n",
      "Epoch [15/90], lter [1091/1752] Loss: 20.4986\n",
      "Epoch [15/90], lter [1101/1752] Loss: 26.6391\n",
      "Epoch [15/90], lter [1111/1752] Loss: 18.4203\n",
      "Epoch [15/90], lter [1121/1752] Loss: 27.8758\n",
      "Epoch [15/90], lter [1131/1752] Loss: 19.1439\n",
      "Epoch [15/90], lter [1141/1752] Loss: 19.9592\n",
      "Epoch [15/90], lter [1151/1752] Loss: 21.5956\n",
      "Epoch [15/90], lter [1161/1752] Loss: 22.4515\n",
      "Epoch [15/90], lter [1171/1752] Loss: 25.6695\n",
      "Epoch [15/90], lter [1181/1752] Loss: 39.1605\n",
      "Epoch [15/90], lter [1191/1752] Loss: 21.6673\n",
      "Epoch [15/90], lter [1201/1752] Loss: 21.7101\n",
      "Epoch [15/90], lter [1211/1752] Loss: 20.3793\n",
      "Epoch [15/90], lter [1221/1752] Loss: 25.3008\n",
      "Epoch [15/90], lter [1231/1752] Loss: 27.2836\n",
      "Epoch [15/90], lter [1241/1752] Loss: 15.3440\n",
      "Epoch [15/90], lter [1251/1752] Loss: 26.2383\n",
      "Epoch [15/90], lter [1261/1752] Loss: 27.2481\n",
      "Epoch [15/90], lter [1271/1752] Loss: 26.1154\n",
      "Epoch [15/90], lter [1281/1752] Loss: 24.1964\n",
      "Epoch [15/90], lter [1291/1752] Loss: 24.2549\n",
      "Epoch [15/90], lter [1301/1752] Loss: 20.6400\n",
      "Epoch [15/90], lter [1311/1752] Loss: 16.9621\n",
      "Epoch [15/90], lter [1321/1752] Loss: 32.9164\n",
      "Epoch [15/90], lter [1331/1752] Loss: 21.5383\n",
      "Epoch [15/90], lter [1341/1752] Loss: 33.3486\n",
      "Epoch [15/90], lter [1351/1752] Loss: 17.1475\n",
      "Epoch [15/90], lter [1361/1752] Loss: 12.9455\n",
      "Epoch [15/90], lter [1371/1752] Loss: 19.1190\n",
      "Epoch [15/90], lter [1381/1752] Loss: 23.6309\n",
      "Epoch [15/90], lter [1391/1752] Loss: 23.9901\n",
      "Epoch [15/90], lter [1401/1752] Loss: 20.2392\n",
      "Epoch [15/90], lter [1411/1752] Loss: 27.3824\n",
      "Epoch [15/90], lter [1421/1752] Loss: 30.2701\n",
      "Epoch [15/90], lter [1431/1752] Loss: 36.5166\n",
      "Epoch [15/90], lter [1441/1752] Loss: 34.5872\n",
      "Epoch [15/90], lter [1451/1752] Loss: 20.3358\n",
      "Epoch [15/90], lter [1461/1752] Loss: 22.9409\n",
      "Epoch [15/90], lter [1471/1752] Loss: 12.3253\n",
      "Epoch [15/90], lter [1481/1752] Loss: 27.8763\n",
      "Epoch [15/90], lter [1491/1752] Loss: 27.8408\n",
      "Epoch [15/90], lter [1501/1752] Loss: 29.2311\n",
      "Epoch [15/90], lter [1511/1752] Loss: 25.3720\n",
      "Epoch [15/90], lter [1521/1752] Loss: 17.6794\n",
      "Epoch [15/90], lter [1531/1752] Loss: 14.3641\n",
      "Epoch [15/90], lter [1541/1752] Loss: 24.6457\n",
      "Epoch [15/90], lter [1551/1752] Loss: 29.5904\n",
      "Epoch [15/90], lter [1561/1752] Loss: 35.1928\n",
      "Epoch [15/90], lter [1571/1752] Loss: 23.4628\n",
      "Epoch [15/90], lter [1581/1752] Loss: 30.5907\n",
      "Epoch [15/90], lter [1591/1752] Loss: 28.2646\n",
      "Epoch [15/90], lter [1601/1752] Loss: 15.7963\n",
      "Epoch [15/90], lter [1611/1752] Loss: 29.1237\n",
      "Epoch [15/90], lter [1621/1752] Loss: 18.6220\n",
      "Epoch [15/90], lter [1631/1752] Loss: 33.4308\n",
      "Epoch [15/90], lter [1641/1752] Loss: 39.5032\n",
      "Epoch [15/90], lter [1651/1752] Loss: 21.7061\n",
      "Epoch [15/90], lter [1661/1752] Loss: 31.2898\n",
      "Epoch [15/90], lter [1671/1752] Loss: 38.4946\n",
      "Epoch [15/90], lter [1681/1752] Loss: 30.7300\n",
      "Epoch [15/90], lter [1691/1752] Loss: 32.7556\n",
      "Epoch [15/90], lter [1701/1752] Loss: 21.8996\n",
      "Epoch [15/90], lter [1711/1752] Loss: 19.7790\n",
      "Epoch [15/90], lter [1721/1752] Loss: 33.6524\n",
      "Epoch [15/90], lter [1731/1752] Loss: 34.8490\n",
      "Epoch [15/90], lter [1741/1752] Loss: 37.0998\n",
      "Epoch [15/90], lter [1751/1752] Loss: 26.2425\n",
      "Epoch:  15 | train loss : 24.2174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 15/90 [66:06:08<27:14:54, 1307.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14 | test loss : 16.1013\n",
      "Epoch [16/90], lter [1/1752] Loss: 23.1022\n",
      "Epoch [16/90], lter [11/1752] Loss: 22.1929\n",
      "Epoch [16/90], lter [21/1752] Loss: 34.1658\n",
      "Epoch [16/90], lter [31/1752] Loss: 24.8679\n",
      "Epoch [16/90], lter [41/1752] Loss: 25.5085\n",
      "Epoch [16/90], lter [51/1752] Loss: 24.2003\n",
      "Epoch [16/90], lter [61/1752] Loss: 28.4639\n",
      "Epoch [16/90], lter [71/1752] Loss: 14.7110\n",
      "Epoch [16/90], lter [81/1752] Loss: 17.4207\n",
      "Epoch [16/90], lter [91/1752] Loss: 27.6383\n",
      "Epoch [16/90], lter [101/1752] Loss: 30.4953\n",
      "Epoch [16/90], lter [111/1752] Loss: 36.3879\n",
      "Epoch [16/90], lter [121/1752] Loss: 32.2689\n",
      "Epoch [16/90], lter [131/1752] Loss: 21.1526\n",
      "Epoch [16/90], lter [141/1752] Loss: 21.5436\n",
      "Epoch [16/90], lter [151/1752] Loss: 34.1201\n",
      "Epoch [16/90], lter [161/1752] Loss: 31.4478\n",
      "Epoch [16/90], lter [171/1752] Loss: 36.7528\n",
      "Epoch [16/90], lter [181/1752] Loss: 26.7049\n",
      "Epoch [16/90], lter [191/1752] Loss: 28.1679\n",
      "Epoch [16/90], lter [201/1752] Loss: 20.5080\n",
      "Epoch [16/90], lter [211/1752] Loss: 33.0863\n",
      "Epoch [16/90], lter [221/1752] Loss: 21.0412\n",
      "Epoch [16/90], lter [231/1752] Loss: 21.8154\n",
      "Epoch [16/90], lter [241/1752] Loss: 24.8663\n",
      "Epoch [16/90], lter [251/1752] Loss: 30.4892\n",
      "Epoch [16/90], lter [261/1752] Loss: 21.5968\n",
      "Epoch [16/90], lter [271/1752] Loss: 26.6855\n",
      "Epoch [16/90], lter [281/1752] Loss: 17.7984\n",
      "Epoch [16/90], lter [291/1752] Loss: 22.8908\n",
      "Epoch [16/90], lter [301/1752] Loss: 21.0493\n",
      "Epoch [16/90], lter [311/1752] Loss: 19.5746\n",
      "Epoch [16/90], lter [321/1752] Loss: 17.6431\n",
      "Epoch [16/90], lter [331/1752] Loss: 22.1903\n",
      "Epoch [16/90], lter [341/1752] Loss: 23.8260\n",
      "Epoch [16/90], lter [351/1752] Loss: 17.8793\n",
      "Epoch [16/90], lter [361/1752] Loss: 33.7899\n",
      "Epoch [16/90], lter [371/1752] Loss: 18.1975\n",
      "Epoch [16/90], lter [381/1752] Loss: 25.4157\n",
      "Epoch [16/90], lter [391/1752] Loss: 21.8924\n",
      "Epoch [16/90], lter [401/1752] Loss: 30.1199\n",
      "Epoch [16/90], lter [411/1752] Loss: 31.2257\n",
      "Epoch [16/90], lter [421/1752] Loss: 25.5243\n",
      "Epoch [16/90], lter [431/1752] Loss: 32.1138\n",
      "Epoch [16/90], lter [441/1752] Loss: 23.2915\n",
      "Epoch [16/90], lter [451/1752] Loss: 23.5019\n",
      "Epoch [16/90], lter [461/1752] Loss: 18.4569\n",
      "Epoch [16/90], lter [471/1752] Loss: 17.9447\n",
      "Epoch [16/90], lter [481/1752] Loss: 11.1026\n",
      "Epoch [16/90], lter [491/1752] Loss: 21.7607\n",
      "Epoch [16/90], lter [501/1752] Loss: 31.7088\n",
      "Epoch [16/90], lter [511/1752] Loss: 27.0308\n",
      "Epoch [16/90], lter [521/1752] Loss: 34.5243\n",
      "Epoch [16/90], lter [531/1752] Loss: 20.4661\n",
      "Epoch [16/90], lter [541/1752] Loss: 14.2614\n",
      "Epoch [16/90], lter [551/1752] Loss: 30.0372\n",
      "Epoch [16/90], lter [561/1752] Loss: 32.7662\n",
      "Epoch [16/90], lter [571/1752] Loss: 21.2809\n",
      "Epoch [16/90], lter [581/1752] Loss: 13.3876\n",
      "Epoch [16/90], lter [591/1752] Loss: 23.6913\n",
      "Epoch [16/90], lter [601/1752] Loss: 37.0378\n",
      "Epoch [16/90], lter [611/1752] Loss: 25.8680\n",
      "Epoch [16/90], lter [621/1752] Loss: 29.9030\n",
      "Epoch [16/90], lter [631/1752] Loss: 35.1943\n",
      "Epoch [16/90], lter [641/1752] Loss: 40.7275\n",
      "Epoch [16/90], lter [651/1752] Loss: 15.0493\n",
      "Epoch [16/90], lter [661/1752] Loss: 27.0919\n",
      "Epoch [16/90], lter [671/1752] Loss: 11.5552\n",
      "Epoch [16/90], lter [681/1752] Loss: 12.8682\n",
      "Epoch [16/90], lter [691/1752] Loss: 21.2900\n",
      "Epoch [16/90], lter [701/1752] Loss: 24.9645\n",
      "Epoch [16/90], lter [711/1752] Loss: 12.3113\n",
      "Epoch [16/90], lter [721/1752] Loss: 24.0442\n",
      "Epoch [16/90], lter [731/1752] Loss: 26.3435\n",
      "Epoch [16/90], lter [741/1752] Loss: 25.6478\n",
      "Epoch [16/90], lter [751/1752] Loss: 14.6553\n",
      "Epoch [16/90], lter [761/1752] Loss: 30.6802\n",
      "Epoch [16/90], lter [771/1752] Loss: 26.9117\n",
      "Epoch [16/90], lter [781/1752] Loss: 33.7570\n",
      "Epoch [16/90], lter [791/1752] Loss: 23.0400\n",
      "Epoch [16/90], lter [801/1752] Loss: 28.6371\n",
      "Epoch [16/90], lter [811/1752] Loss: 24.0886\n",
      "Epoch [16/90], lter [821/1752] Loss: 28.6001\n",
      "Epoch [16/90], lter [831/1752] Loss: 20.6206\n",
      "Epoch [16/90], lter [841/1752] Loss: 32.2276\n",
      "Epoch [16/90], lter [851/1752] Loss: 28.3165\n",
      "Epoch [16/90], lter [861/1752] Loss: 22.0786\n",
      "Epoch [16/90], lter [871/1752] Loss: 19.1042\n",
      "Epoch [16/90], lter [881/1752] Loss: 16.3630\n",
      "Epoch [16/90], lter [891/1752] Loss: 31.2188\n",
      "Epoch [16/90], lter [901/1752] Loss: 15.9233\n",
      "Epoch [16/90], lter [911/1752] Loss: 24.2391\n",
      "Epoch [16/90], lter [921/1752] Loss: 34.3913\n",
      "Epoch [16/90], lter [931/1752] Loss: 33.3184\n",
      "Epoch [16/90], lter [941/1752] Loss: 24.1790\n",
      "Epoch [16/90], lter [951/1752] Loss: 29.9747\n",
      "Epoch [16/90], lter [961/1752] Loss: 36.2519\n",
      "Epoch [16/90], lter [971/1752] Loss: 35.6390\n",
      "Epoch [16/90], lter [981/1752] Loss: 28.7743\n",
      "Epoch [16/90], lter [991/1752] Loss: 30.9906\n",
      "Epoch [16/90], lter [1001/1752] Loss: 20.0178\n",
      "Epoch [16/90], lter [1011/1752] Loss: 26.6875\n",
      "Epoch [16/90], lter [1021/1752] Loss: 22.6361\n",
      "Epoch [16/90], lter [1031/1752] Loss: 23.8447\n",
      "Epoch [16/90], lter [1041/1752] Loss: 29.1549\n",
      "Epoch [16/90], lter [1051/1752] Loss: 24.9107\n",
      "Epoch [16/90], lter [1061/1752] Loss: 20.4153\n",
      "Epoch [16/90], lter [1071/1752] Loss: 27.9701\n",
      "Epoch [16/90], lter [1081/1752] Loss: 19.3124\n",
      "Epoch [16/90], lter [1091/1752] Loss: 14.5847\n",
      "Epoch [16/90], lter [1101/1752] Loss: 21.2485\n",
      "Epoch [16/90], lter [1111/1752] Loss: 16.9663\n",
      "Epoch [16/90], lter [1121/1752] Loss: 32.8554\n",
      "Epoch [16/90], lter [1131/1752] Loss: 31.8329\n",
      "Epoch [16/90], lter [1141/1752] Loss: 27.0959\n",
      "Epoch [16/90], lter [1151/1752] Loss: 19.1319\n",
      "Epoch [16/90], lter [1161/1752] Loss: 35.1070\n",
      "Epoch [16/90], lter [1171/1752] Loss: 27.3967\n",
      "Epoch [16/90], lter [1181/1752] Loss: 29.2708\n",
      "Epoch [16/90], lter [1191/1752] Loss: 16.2758\n",
      "Epoch [16/90], lter [1201/1752] Loss: 12.3979\n",
      "Epoch [16/90], lter [1211/1752] Loss: 25.8731\n",
      "Epoch [16/90], lter [1221/1752] Loss: 19.9291\n",
      "Epoch [16/90], lter [1231/1752] Loss: 29.0853\n",
      "Epoch [16/90], lter [1241/1752] Loss: 26.1362\n",
      "Epoch [16/90], lter [1251/1752] Loss: 24.3979\n",
      "Epoch [16/90], lter [1261/1752] Loss: 29.7551\n",
      "Epoch [16/90], lter [1271/1752] Loss: 17.2488\n",
      "Epoch [16/90], lter [1281/1752] Loss: 16.4159\n",
      "Epoch [16/90], lter [1291/1752] Loss: 45.4217\n",
      "Epoch [16/90], lter [1301/1752] Loss: 32.3951\n",
      "Epoch [16/90], lter [1311/1752] Loss: 22.2966\n",
      "Epoch [16/90], lter [1321/1752] Loss: 16.5143\n",
      "Epoch [16/90], lter [1331/1752] Loss: 21.1407\n",
      "Epoch [16/90], lter [1341/1752] Loss: 15.9043\n",
      "Epoch [16/90], lter [1351/1752] Loss: 20.3123\n",
      "Epoch [16/90], lter [1361/1752] Loss: 21.7220\n",
      "Epoch [16/90], lter [1371/1752] Loss: 13.2992\n",
      "Epoch [16/90], lter [1381/1752] Loss: 25.1922\n",
      "Epoch [16/90], lter [1391/1752] Loss: 25.6232\n",
      "Epoch [16/90], lter [1401/1752] Loss: 23.3799\n",
      "Epoch [16/90], lter [1411/1752] Loss: 19.1371\n",
      "Epoch [16/90], lter [1421/1752] Loss: 29.1499\n",
      "Epoch [16/90], lter [1431/1752] Loss: 26.8016\n",
      "Epoch [16/90], lter [1441/1752] Loss: 13.6036\n",
      "Epoch [16/90], lter [1451/1752] Loss: 24.4726\n",
      "Epoch [16/90], lter [1461/1752] Loss: 27.9013\n",
      "Epoch [16/90], lter [1471/1752] Loss: 29.0877\n",
      "Epoch [16/90], lter [1481/1752] Loss: 23.4626\n",
      "Epoch [16/90], lter [1491/1752] Loss: 29.5936\n",
      "Epoch [16/90], lter [1501/1752] Loss: 23.2291\n",
      "Epoch [16/90], lter [1511/1752] Loss: 27.1092\n",
      "Epoch [16/90], lter [1521/1752] Loss: 18.5720\n",
      "Epoch [16/90], lter [1531/1752] Loss: 34.1372\n",
      "Epoch [16/90], lter [1541/1752] Loss: 20.3266\n",
      "Epoch [16/90], lter [1551/1752] Loss: 21.7265\n",
      "Epoch [16/90], lter [1561/1752] Loss: 36.1688\n",
      "Epoch [16/90], lter [1571/1752] Loss: 26.1975\n",
      "Epoch [16/90], lter [1581/1752] Loss: 21.9722\n",
      "Epoch [16/90], lter [1591/1752] Loss: 19.9370\n",
      "Epoch [16/90], lter [1601/1752] Loss: 18.7712\n",
      "Epoch [16/90], lter [1611/1752] Loss: 19.5449\n",
      "Epoch [16/90], lter [1621/1752] Loss: 33.7137\n",
      "Epoch [16/90], lter [1631/1752] Loss: 17.8797\n",
      "Epoch [16/90], lter [1641/1752] Loss: 35.9526\n",
      "Epoch [16/90], lter [1651/1752] Loss: 27.7857\n",
      "Epoch [16/90], lter [1661/1752] Loss: 18.8218\n",
      "Epoch [16/90], lter [1671/1752] Loss: 21.6183\n",
      "Epoch [16/90], lter [1681/1752] Loss: 22.7956\n",
      "Epoch [16/90], lter [1691/1752] Loss: 11.3420\n",
      "Epoch [16/90], lter [1701/1752] Loss: 16.6030\n",
      "Epoch [16/90], lter [1711/1752] Loss: 22.7546\n",
      "Epoch [16/90], lter [1721/1752] Loss: 20.3715\n",
      "Epoch [16/90], lter [1731/1752] Loss: 42.6789\n",
      "Epoch [16/90], lter [1741/1752] Loss: 27.6779\n",
      "Epoch [16/90], lter [1751/1752] Loss: 21.0852\n",
      "Epoch:  16 | train loss : 24.5076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 16/90 [66:20:17<24:02:26, 1169.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15 | test loss : 14.5980\n",
      "Epoch [17/90], lter [1/1752] Loss: 19.0450\n",
      "Epoch [17/90], lter [11/1752] Loss: 25.3255\n",
      "Epoch [17/90], lter [21/1752] Loss: 23.9900\n",
      "Epoch [17/90], lter [31/1752] Loss: 22.0036\n",
      "Epoch [17/90], lter [41/1752] Loss: 21.6490\n",
      "Epoch [17/90], lter [51/1752] Loss: 25.3777\n",
      "Epoch [17/90], lter [61/1752] Loss: 21.1570\n",
      "Epoch [17/90], lter [71/1752] Loss: 30.7633\n",
      "Epoch [17/90], lter [81/1752] Loss: 18.9111\n",
      "Epoch [17/90], lter [91/1752] Loss: 17.7587\n",
      "Epoch [17/90], lter [101/1752] Loss: 24.0399\n",
      "Epoch [17/90], lter [111/1752] Loss: 23.8117\n",
      "Epoch [17/90], lter [121/1752] Loss: 26.3871\n",
      "Epoch [17/90], lter [131/1752] Loss: 28.7074\n",
      "Epoch [17/90], lter [141/1752] Loss: 25.1848\n",
      "Epoch [17/90], lter [151/1752] Loss: 28.5006\n",
      "Epoch [17/90], lter [161/1752] Loss: 23.1282\n",
      "Epoch [17/90], lter [171/1752] Loss: 20.0080\n",
      "Epoch [17/90], lter [181/1752] Loss: 35.8353\n",
      "Epoch [17/90], lter [191/1752] Loss: 31.1422\n",
      "Epoch [17/90], lter [201/1752] Loss: 26.3919\n",
      "Epoch [17/90], lter [211/1752] Loss: 22.7530\n",
      "Epoch [17/90], lter [221/1752] Loss: 24.2488\n",
      "Epoch [17/90], lter [231/1752] Loss: 18.5023\n",
      "Epoch [17/90], lter [241/1752] Loss: 22.7050\n",
      "Epoch [17/90], lter [251/1752] Loss: 15.7640\n",
      "Epoch [17/90], lter [261/1752] Loss: 37.4110\n",
      "Epoch [17/90], lter [271/1752] Loss: 21.8063\n",
      "Epoch [17/90], lter [281/1752] Loss: 25.2155\n",
      "Epoch [17/90], lter [291/1752] Loss: 23.4762\n",
      "Epoch [17/90], lter [301/1752] Loss: 28.1941\n",
      "Epoch [17/90], lter [311/1752] Loss: 17.1786\n",
      "Epoch [17/90], lter [321/1752] Loss: 14.3170\n",
      "Epoch [17/90], lter [331/1752] Loss: 26.1401\n",
      "Epoch [17/90], lter [341/1752] Loss: 17.4816\n",
      "Epoch [17/90], lter [351/1752] Loss: 21.1934\n",
      "Epoch [17/90], lter [361/1752] Loss: 16.0802\n",
      "Epoch [17/90], lter [371/1752] Loss: 12.4790\n",
      "Epoch [17/90], lter [381/1752] Loss: 23.7077\n",
      "Epoch [17/90], lter [391/1752] Loss: 24.2308\n",
      "Epoch [17/90], lter [401/1752] Loss: 23.7579\n",
      "Epoch [17/90], lter [411/1752] Loss: 23.5927\n",
      "Epoch [17/90], lter [421/1752] Loss: 16.9609\n",
      "Epoch [17/90], lter [431/1752] Loss: 21.9758\n",
      "Epoch [17/90], lter [441/1752] Loss: 33.5753\n",
      "Epoch [17/90], lter [451/1752] Loss: 19.4413\n",
      "Epoch [17/90], lter [461/1752] Loss: 22.7713\n",
      "Epoch [17/90], lter [471/1752] Loss: 34.0215\n",
      "Epoch [17/90], lter [481/1752] Loss: 24.7545\n",
      "Epoch [17/90], lter [491/1752] Loss: 14.3654\n",
      "Epoch [17/90], lter [501/1752] Loss: 15.7391\n",
      "Epoch [17/90], lter [511/1752] Loss: 16.2731\n",
      "Epoch [17/90], lter [521/1752] Loss: 16.0915\n",
      "Epoch [17/90], lter [531/1752] Loss: 38.2995\n",
      "Epoch [17/90], lter [541/1752] Loss: 25.6199\n",
      "Epoch [17/90], lter [551/1752] Loss: 14.6869\n",
      "Epoch [17/90], lter [561/1752] Loss: 25.2659\n",
      "Epoch [17/90], lter [571/1752] Loss: 18.0631\n",
      "Epoch [17/90], lter [581/1752] Loss: 25.3131\n",
      "Epoch [17/90], lter [591/1752] Loss: 21.8157\n",
      "Epoch [17/90], lter [601/1752] Loss: 25.6311\n",
      "Epoch [17/90], lter [611/1752] Loss: 28.2180\n",
      "Epoch [17/90], lter [621/1752] Loss: 39.4246\n",
      "Epoch [17/90], lter [631/1752] Loss: 30.5033\n",
      "Epoch [17/90], lter [641/1752] Loss: 15.6212\n",
      "Epoch [17/90], lter [651/1752] Loss: 13.7909\n",
      "Epoch [17/90], lter [661/1752] Loss: 21.4441\n",
      "Epoch [17/90], lter [671/1752] Loss: 14.2308\n",
      "Epoch [17/90], lter [681/1752] Loss: 29.9454\n",
      "Epoch [17/90], lter [691/1752] Loss: 20.4197\n",
      "Epoch [17/90], lter [701/1752] Loss: 24.3105\n",
      "Epoch [17/90], lter [711/1752] Loss: 22.9782\n",
      "Epoch [17/90], lter [721/1752] Loss: 28.2801\n",
      "Epoch [17/90], lter [731/1752] Loss: 21.9376\n",
      "Epoch [17/90], lter [741/1752] Loss: 28.2423\n",
      "Epoch [17/90], lter [751/1752] Loss: 23.4829\n",
      "Epoch [17/90], lter [761/1752] Loss: 25.1328\n",
      "Epoch [17/90], lter [771/1752] Loss: 17.2648\n",
      "Epoch [17/90], lter [781/1752] Loss: 25.2378\n",
      "Epoch [17/90], lter [791/1752] Loss: 27.3225\n",
      "Epoch [17/90], lter [801/1752] Loss: 26.0390\n",
      "Epoch [17/90], lter [811/1752] Loss: 20.5021\n",
      "Epoch [17/90], lter [821/1752] Loss: 28.1887\n",
      "Epoch [17/90], lter [831/1752] Loss: 19.7853\n",
      "Epoch [17/90], lter [841/1752] Loss: 19.3352\n",
      "Epoch [17/90], lter [851/1752] Loss: 33.0359\n",
      "Epoch [17/90], lter [861/1752] Loss: 49.8966\n",
      "Epoch [17/90], lter [871/1752] Loss: 36.6016\n",
      "Epoch [17/90], lter [881/1752] Loss: 21.3027\n",
      "Epoch [17/90], lter [891/1752] Loss: 21.9075\n",
      "Epoch [17/90], lter [901/1752] Loss: 21.2992\n",
      "Epoch [17/90], lter [911/1752] Loss: 21.4978\n",
      "Epoch [17/90], lter [921/1752] Loss: 24.4731\n",
      "Epoch [17/90], lter [931/1752] Loss: 39.0926\n",
      "Epoch [17/90], lter [941/1752] Loss: 30.9782\n",
      "Epoch [17/90], lter [951/1752] Loss: 28.2102\n",
      "Epoch [17/90], lter [961/1752] Loss: 32.0343\n",
      "Epoch [17/90], lter [971/1752] Loss: 20.1886\n",
      "Epoch [17/90], lter [981/1752] Loss: 25.8076\n",
      "Epoch [17/90], lter [991/1752] Loss: 24.5535\n",
      "Epoch [17/90], lter [1001/1752] Loss: 24.5130\n",
      "Epoch [17/90], lter [1011/1752] Loss: 32.2134\n",
      "Epoch [17/90], lter [1021/1752] Loss: 34.5709\n",
      "Epoch [17/90], lter [1031/1752] Loss: 29.4513\n",
      "Epoch [17/90], lter [1041/1752] Loss: 22.6932\n",
      "Epoch [17/90], lter [1051/1752] Loss: 31.9859\n",
      "Epoch [17/90], lter [1061/1752] Loss: 19.9782\n",
      "Epoch [17/90], lter [1071/1752] Loss: 21.2251\n",
      "Epoch [17/90], lter [1081/1752] Loss: 20.6736\n",
      "Epoch [17/90], lter [1091/1752] Loss: 31.9250\n",
      "Epoch [17/90], lter [1101/1752] Loss: 30.5424\n",
      "Epoch [17/90], lter [1111/1752] Loss: 24.3505\n",
      "Epoch [17/90], lter [1121/1752] Loss: 22.8352\n",
      "Epoch [17/90], lter [1131/1752] Loss: 23.8441\n",
      "Epoch [17/90], lter [1141/1752] Loss: 34.3080\n",
      "Epoch [17/90], lter [1151/1752] Loss: 34.2673\n",
      "Epoch [17/90], lter [1161/1752] Loss: 28.6492\n",
      "Epoch [17/90], lter [1171/1752] Loss: 30.3318\n",
      "Epoch [17/90], lter [1181/1752] Loss: 33.3307\n",
      "Epoch [17/90], lter [1191/1752] Loss: 25.9222\n",
      "Epoch [17/90], lter [1201/1752] Loss: 26.8088\n",
      "Epoch [17/90], lter [1211/1752] Loss: 30.5817\n",
      "Epoch [17/90], lter [1221/1752] Loss: 22.7279\n",
      "Epoch [17/90], lter [1231/1752] Loss: 16.3029\n",
      "Epoch [17/90], lter [1241/1752] Loss: 16.7382\n",
      "Epoch [17/90], lter [1251/1752] Loss: 32.1175\n",
      "Epoch [17/90], lter [1261/1752] Loss: 28.4294\n",
      "Epoch [17/90], lter [1271/1752] Loss: 21.3041\n",
      "Epoch [17/90], lter [1281/1752] Loss: 18.4929\n",
      "Epoch [17/90], lter [1291/1752] Loss: 10.8117\n",
      "Epoch [17/90], lter [1301/1752] Loss: 19.9856\n",
      "Epoch [17/90], lter [1311/1752] Loss: 20.3953\n",
      "Epoch [17/90], lter [1321/1752] Loss: 28.1417\n",
      "Epoch [17/90], lter [1331/1752] Loss: 27.1691\n",
      "Epoch [17/90], lter [1341/1752] Loss: 26.7210\n",
      "Epoch [17/90], lter [1351/1752] Loss: 15.5093\n",
      "Epoch [17/90], lter [1361/1752] Loss: 19.5981\n",
      "Epoch [17/90], lter [1371/1752] Loss: 22.8326\n",
      "Epoch [17/90], lter [1381/1752] Loss: 19.8705\n",
      "Epoch [17/90], lter [1391/1752] Loss: 21.2464\n",
      "Epoch [17/90], lter [1401/1752] Loss: 28.4141\n",
      "Epoch [17/90], lter [1411/1752] Loss: 23.8406\n",
      "Epoch [17/90], lter [1421/1752] Loss: 30.3633\n",
      "Epoch [17/90], lter [1431/1752] Loss: 24.8455\n",
      "Epoch [17/90], lter [1441/1752] Loss: 21.7406\n",
      "Epoch [17/90], lter [1451/1752] Loss: 20.2803\n",
      "Epoch [17/90], lter [1461/1752] Loss: 26.0327\n",
      "Epoch [17/90], lter [1471/1752] Loss: 21.5869\n",
      "Epoch [17/90], lter [1481/1752] Loss: 21.2940\n",
      "Epoch [17/90], lter [1491/1752] Loss: 20.3740\n",
      "Epoch [17/90], lter [1501/1752] Loss: 24.1242\n",
      "Epoch [17/90], lter [1511/1752] Loss: 23.6676\n",
      "Epoch [17/90], lter [1521/1752] Loss: 19.8184\n",
      "Epoch [17/90], lter [1531/1752] Loss: 15.7499\n",
      "Epoch [17/90], lter [1541/1752] Loss: 27.7546\n",
      "Epoch [17/90], lter [1551/1752] Loss: 23.8593\n",
      "Epoch [17/90], lter [1561/1752] Loss: 18.4452\n",
      "Epoch [17/90], lter [1571/1752] Loss: 22.5948\n",
      "Epoch [17/90], lter [1581/1752] Loss: 22.7103\n",
      "Epoch [17/90], lter [1591/1752] Loss: 19.1212\n",
      "Epoch [17/90], lter [1601/1752] Loss: 18.8934\n",
      "Epoch [17/90], lter [1611/1752] Loss: 24.2787\n",
      "Epoch [17/90], lter [1621/1752] Loss: 17.7646\n",
      "Epoch [17/90], lter [1631/1752] Loss: 30.2463\n",
      "Epoch [17/90], lter [1641/1752] Loss: 35.5413\n",
      "Epoch [17/90], lter [1651/1752] Loss: 27.8980\n",
      "Epoch [17/90], lter [1661/1752] Loss: 9.8778\n",
      "Epoch [17/90], lter [1671/1752] Loss: 27.5099\n",
      "Epoch [17/90], lter [1681/1752] Loss: 32.3844\n",
      "Epoch [17/90], lter [1691/1752] Loss: 32.7924\n",
      "Epoch [17/90], lter [1701/1752] Loss: 16.0550\n",
      "Epoch [17/90], lter [1711/1752] Loss: 26.2575\n",
      "Epoch [17/90], lter [1721/1752] Loss: 17.7906\n",
      "Epoch [17/90], lter [1731/1752] Loss: 28.8424\n",
      "Epoch [17/90], lter [1741/1752] Loss: 21.8942\n",
      "Epoch [17/90], lter [1751/1752] Loss: 28.5946\n",
      "Epoch:  17 | train loss : 24.3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 17/90 [66:34:26<21:45:40, 1073.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16 | test loss : 19.7235\n",
      "Epoch [18/90], lter [1/1752] Loss: 29.3096\n",
      "Epoch [18/90], lter [11/1752] Loss: 31.2996\n",
      "Epoch [18/90], lter [21/1752] Loss: 24.1908\n",
      "Epoch [18/90], lter [31/1752] Loss: 26.8785\n",
      "Epoch [18/90], lter [41/1752] Loss: 36.4774\n",
      "Epoch [18/90], lter [51/1752] Loss: 18.3928\n",
      "Epoch [18/90], lter [61/1752] Loss: 28.7523\n",
      "Epoch [18/90], lter [71/1752] Loss: 18.4683\n",
      "Epoch [18/90], lter [81/1752] Loss: 21.7165\n",
      "Epoch [18/90], lter [91/1752] Loss: 19.8604\n",
      "Epoch [18/90], lter [101/1752] Loss: 23.2940\n",
      "Epoch [18/90], lter [111/1752] Loss: 28.0910\n",
      "Epoch [18/90], lter [121/1752] Loss: 25.8574\n",
      "Epoch [18/90], lter [131/1752] Loss: 16.2688\n",
      "Epoch [18/90], lter [141/1752] Loss: 24.1324\n",
      "Epoch [18/90], lter [151/1752] Loss: 18.3678\n",
      "Epoch [18/90], lter [161/1752] Loss: 22.2251\n",
      "Epoch [18/90], lter [171/1752] Loss: 30.4905\n",
      "Epoch [18/90], lter [181/1752] Loss: 28.7064\n",
      "Epoch [18/90], lter [191/1752] Loss: 20.2741\n",
      "Epoch [18/90], lter [201/1752] Loss: 19.6540\n",
      "Epoch [18/90], lter [211/1752] Loss: 39.5942\n",
      "Epoch [18/90], lter [221/1752] Loss: 22.3741\n",
      "Epoch [18/90], lter [231/1752] Loss: 33.4881\n",
      "Epoch [18/90], lter [241/1752] Loss: 26.9806\n",
      "Epoch [18/90], lter [251/1752] Loss: 20.3864\n",
      "Epoch [18/90], lter [261/1752] Loss: 21.8696\n",
      "Epoch [18/90], lter [271/1752] Loss: 22.7533\n",
      "Epoch [18/90], lter [281/1752] Loss: 34.4319\n",
      "Epoch [18/90], lter [291/1752] Loss: 23.5857\n",
      "Epoch [18/90], lter [301/1752] Loss: 38.2401\n",
      "Epoch [18/90], lter [311/1752] Loss: 15.6765\n",
      "Epoch [18/90], lter [321/1752] Loss: 16.1264\n",
      "Epoch [18/90], lter [331/1752] Loss: 30.0134\n",
      "Epoch [18/90], lter [341/1752] Loss: 20.9046\n",
      "Epoch [18/90], lter [351/1752] Loss: 24.3416\n",
      "Epoch [18/90], lter [361/1752] Loss: 42.3087\n",
      "Epoch [18/90], lter [371/1752] Loss: 43.4081\n",
      "Epoch [18/90], lter [381/1752] Loss: 31.7493\n",
      "Epoch [18/90], lter [391/1752] Loss: 28.9893\n",
      "Epoch [18/90], lter [401/1752] Loss: 25.1025\n",
      "Epoch [18/90], lter [411/1752] Loss: 29.0903\n",
      "Epoch [18/90], lter [421/1752] Loss: 40.4607\n",
      "Epoch [18/90], lter [431/1752] Loss: 27.1143\n",
      "Epoch [18/90], lter [441/1752] Loss: 26.5218\n",
      "Epoch [18/90], lter [451/1752] Loss: 12.8962\n",
      "Epoch [18/90], lter [461/1752] Loss: 34.1515\n",
      "Epoch [18/90], lter [471/1752] Loss: 32.2646\n",
      "Epoch [18/90], lter [481/1752] Loss: 22.0058\n",
      "Epoch [18/90], lter [491/1752] Loss: 37.7959\n",
      "Epoch [18/90], lter [501/1752] Loss: 20.1265\n",
      "Epoch [18/90], lter [511/1752] Loss: 36.1921\n",
      "Epoch [18/90], lter [521/1752] Loss: 26.2760\n",
      "Epoch [18/90], lter [531/1752] Loss: 21.5277\n",
      "Epoch [18/90], lter [541/1752] Loss: 19.6465\n",
      "Epoch [18/90], lter [551/1752] Loss: 27.8121\n",
      "Epoch [18/90], lter [561/1752] Loss: 33.0336\n",
      "Epoch [18/90], lter [571/1752] Loss: 18.4668\n",
      "Epoch [18/90], lter [581/1752] Loss: 23.9993\n",
      "Epoch [18/90], lter [591/1752] Loss: 23.8406\n",
      "Epoch [18/90], lter [601/1752] Loss: 23.8047\n",
      "Epoch [18/90], lter [611/1752] Loss: 18.2437\n",
      "Epoch [18/90], lter [621/1752] Loss: 35.3974\n",
      "Epoch [18/90], lter [631/1752] Loss: 19.1530\n",
      "Epoch [18/90], lter [641/1752] Loss: 18.8909\n",
      "Epoch [18/90], lter [651/1752] Loss: 34.2189\n",
      "Epoch [18/90], lter [661/1752] Loss: 14.8505\n",
      "Epoch [18/90], lter [671/1752] Loss: 22.8334\n",
      "Epoch [18/90], lter [681/1752] Loss: 29.1218\n",
      "Epoch [18/90], lter [691/1752] Loss: 21.0967\n",
      "Epoch [18/90], lter [701/1752] Loss: 23.8025\n",
      "Epoch [18/90], lter [711/1752] Loss: 39.3538\n",
      "Epoch [18/90], lter [721/1752] Loss: 17.8287\n",
      "Epoch [18/90], lter [731/1752] Loss: 16.2748\n",
      "Epoch [18/90], lter [741/1752] Loss: 26.8180\n",
      "Epoch [18/90], lter [751/1752] Loss: 26.2027\n",
      "Epoch [18/90], lter [761/1752] Loss: 26.1697\n",
      "Epoch [18/90], lter [771/1752] Loss: 32.2254\n",
      "Epoch [18/90], lter [781/1752] Loss: 21.8477\n",
      "Epoch [18/90], lter [791/1752] Loss: 15.2223\n",
      "Epoch [18/90], lter [801/1752] Loss: 28.6305\n",
      "Epoch [18/90], lter [811/1752] Loss: 27.6739\n",
      "Epoch [18/90], lter [821/1752] Loss: 26.7021\n",
      "Epoch [18/90], lter [831/1752] Loss: 29.8659\n",
      "Epoch [18/90], lter [841/1752] Loss: 13.5589\n",
      "Epoch [18/90], lter [851/1752] Loss: 42.2533\n",
      "Epoch [18/90], lter [861/1752] Loss: 30.7281\n",
      "Epoch [18/90], lter [871/1752] Loss: 29.9725\n",
      "Epoch [18/90], lter [881/1752] Loss: 25.2862\n",
      "Epoch [18/90], lter [891/1752] Loss: 18.5883\n",
      "Epoch [18/90], lter [901/1752] Loss: 26.9171\n",
      "Epoch [18/90], lter [911/1752] Loss: 23.7199\n",
      "Epoch [18/90], lter [921/1752] Loss: 26.3665\n",
      "Epoch [18/90], lter [931/1752] Loss: 28.6212\n",
      "Epoch [18/90], lter [941/1752] Loss: 21.4687\n",
      "Epoch [18/90], lter [951/1752] Loss: 25.3887\n",
      "Epoch [18/90], lter [961/1752] Loss: 25.1465\n",
      "Epoch [18/90], lter [971/1752] Loss: 23.6363\n",
      "Epoch [18/90], lter [981/1752] Loss: 28.5967\n",
      "Epoch [18/90], lter [991/1752] Loss: 25.0492\n",
      "Epoch [18/90], lter [1001/1752] Loss: 24.5263\n",
      "Epoch [18/90], lter [1011/1752] Loss: 31.8118\n",
      "Epoch [18/90], lter [1021/1752] Loss: 35.0962\n",
      "Epoch [18/90], lter [1031/1752] Loss: 23.0724\n",
      "Epoch [18/90], lter [1041/1752] Loss: 23.7541\n",
      "Epoch [18/90], lter [1051/1752] Loss: 31.2821\n",
      "Epoch [18/90], lter [1061/1752] Loss: 25.8294\n",
      "Epoch [18/90], lter [1071/1752] Loss: 35.1792\n",
      "Epoch [18/90], lter [1081/1752] Loss: 21.8655\n",
      "Epoch [18/90], lter [1091/1752] Loss: 19.6459\n",
      "Epoch [18/90], lter [1101/1752] Loss: 22.6900\n",
      "Epoch [18/90], lter [1111/1752] Loss: 27.8460\n",
      "Epoch [18/90], lter [1121/1752] Loss: 13.0730\n",
      "Epoch [18/90], lter [1131/1752] Loss: 47.0988\n",
      "Epoch [18/90], lter [1141/1752] Loss: 25.9396\n",
      "Epoch [18/90], lter [1151/1752] Loss: 22.2394\n",
      "Epoch [18/90], lter [1161/1752] Loss: 12.1780\n",
      "Epoch [18/90], lter [1171/1752] Loss: 26.5033\n",
      "Epoch [18/90], lter [1181/1752] Loss: 20.1316\n",
      "Epoch [18/90], lter [1191/1752] Loss: 19.8954\n",
      "Epoch [18/90], lter [1201/1752] Loss: 27.5701\n",
      "Epoch [18/90], lter [1211/1752] Loss: 16.4672\n",
      "Epoch [18/90], lter [1221/1752] Loss: 15.3206\n",
      "Epoch [18/90], lter [1231/1752] Loss: 17.4370\n",
      "Epoch [18/90], lter [1241/1752] Loss: 34.3601\n",
      "Epoch [18/90], lter [1251/1752] Loss: 14.4446\n",
      "Epoch [18/90], lter [1261/1752] Loss: 21.6060\n",
      "Epoch [18/90], lter [1271/1752] Loss: 24.3208\n",
      "Epoch [18/90], lter [1281/1752] Loss: 13.9823\n",
      "Epoch [18/90], lter [1291/1752] Loss: 36.7838\n",
      "Epoch [18/90], lter [1301/1752] Loss: 19.6659\n",
      "Epoch [18/90], lter [1311/1752] Loss: 20.3998\n",
      "Epoch [18/90], lter [1321/1752] Loss: 26.9678\n",
      "Epoch [18/90], lter [1331/1752] Loss: 20.9017\n",
      "Epoch [18/90], lter [1341/1752] Loss: 21.1734\n",
      "Epoch [18/90], lter [1351/1752] Loss: 17.9498\n",
      "Epoch [18/90], lter [1361/1752] Loss: 31.8336\n",
      "Epoch [18/90], lter [1371/1752] Loss: 26.9317\n",
      "Epoch [18/90], lter [1381/1752] Loss: 25.2963\n",
      "Epoch [18/90], lter [1391/1752] Loss: 18.4922\n",
      "Epoch [18/90], lter [1401/1752] Loss: 18.9766\n",
      "Epoch [18/90], lter [1411/1752] Loss: 28.6513\n",
      "Epoch [18/90], lter [1421/1752] Loss: 31.5466\n",
      "Epoch [18/90], lter [1431/1752] Loss: 26.5029\n",
      "Epoch [18/90], lter [1441/1752] Loss: 40.0360\n",
      "Epoch [18/90], lter [1451/1752] Loss: 27.4140\n",
      "Epoch [18/90], lter [1461/1752] Loss: 15.9170\n",
      "Epoch [18/90], lter [1471/1752] Loss: 16.9315\n",
      "Epoch [18/90], lter [1481/1752] Loss: 21.4397\n",
      "Epoch [18/90], lter [1491/1752] Loss: 36.6337\n",
      "Epoch [18/90], lter [1501/1752] Loss: 16.7732\n",
      "Epoch [18/90], lter [1511/1752] Loss: 24.6493\n",
      "Epoch [18/90], lter [1521/1752] Loss: 26.5416\n",
      "Epoch [18/90], lter [1531/1752] Loss: 24.4254\n",
      "Epoch [18/90], lter [1541/1752] Loss: 26.1670\n",
      "Epoch [18/90], lter [1551/1752] Loss: 22.1722\n",
      "Epoch [18/90], lter [1561/1752] Loss: 22.4021\n",
      "Epoch [18/90], lter [1571/1752] Loss: 23.3097\n",
      "Epoch [18/90], lter [1581/1752] Loss: 26.5552\n",
      "Epoch [18/90], lter [1591/1752] Loss: 32.8927\n",
      "Epoch [18/90], lter [1601/1752] Loss: 16.5257\n",
      "Epoch [18/90], lter [1611/1752] Loss: 37.9724\n",
      "Epoch [18/90], lter [1621/1752] Loss: 30.8466\n",
      "Epoch [18/90], lter [1631/1752] Loss: 19.6270\n",
      "Epoch [18/90], lter [1641/1752] Loss: 22.4240\n",
      "Epoch [18/90], lter [1651/1752] Loss: 23.7674\n",
      "Epoch [18/90], lter [1661/1752] Loss: 9.2347\n",
      "Epoch [18/90], lter [1671/1752] Loss: 27.5059\n",
      "Epoch [18/90], lter [1681/1752] Loss: 28.9472\n",
      "Epoch [18/90], lter [1691/1752] Loss: 20.5532\n",
      "Epoch [18/90], lter [1701/1752] Loss: 17.1290\n",
      "Epoch [18/90], lter [1711/1752] Loss: 33.2527\n",
      "Epoch [18/90], lter [1721/1752] Loss: 11.2427\n",
      "Epoch [18/90], lter [1731/1752] Loss: 26.1749\n",
      "Epoch [18/90], lter [1741/1752] Loss: 34.7056\n",
      "Epoch [18/90], lter [1751/1752] Loss: 29.5059\n",
      "Epoch:  18 | train loss : 25.1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 18/90 [66:48:32<20:06:09, 1005.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17 | test loss : 13.2934\n",
      "Epoch [19/90], lter [1/1752] Loss: 12.0746\n",
      "Epoch [19/90], lter [11/1752] Loss: 12.8006\n",
      "Epoch [19/90], lter [21/1752] Loss: 24.9282\n",
      "Epoch [19/90], lter [31/1752] Loss: 15.5842\n",
      "Epoch [19/90], lter [41/1752] Loss: 30.6927\n",
      "Epoch [19/90], lter [51/1752] Loss: 19.4219\n",
      "Epoch [19/90], lter [61/1752] Loss: 26.0966\n",
      "Epoch [19/90], lter [71/1752] Loss: 23.9364\n",
      "Epoch [19/90], lter [81/1752] Loss: 25.0065\n",
      "Epoch [19/90], lter [91/1752] Loss: 23.0873\n",
      "Epoch [19/90], lter [101/1752] Loss: 28.9011\n",
      "Epoch [19/90], lter [111/1752] Loss: 19.5522\n",
      "Epoch [19/90], lter [121/1752] Loss: 31.6922\n",
      "Epoch [19/90], lter [131/1752] Loss: 23.2473\n",
      "Epoch [19/90], lter [141/1752] Loss: 26.1009\n",
      "Epoch [19/90], lter [151/1752] Loss: 27.2048\n",
      "Epoch [19/90], lter [161/1752] Loss: 18.7886\n",
      "Epoch [19/90], lter [171/1752] Loss: 31.9962\n",
      "Epoch [19/90], lter [181/1752] Loss: 22.9991\n",
      "Epoch [19/90], lter [191/1752] Loss: 10.1866\n",
      "Epoch [19/90], lter [201/1752] Loss: 27.7848\n",
      "Epoch [19/90], lter [211/1752] Loss: 20.5661\n",
      "Epoch [19/90], lter [221/1752] Loss: 19.1685\n",
      "Epoch [19/90], lter [231/1752] Loss: 27.3417\n",
      "Epoch [19/90], lter [241/1752] Loss: 17.9611\n",
      "Epoch [19/90], lter [251/1752] Loss: 37.8450\n",
      "Epoch [19/90], lter [261/1752] Loss: 25.8171\n",
      "Epoch [19/90], lter [271/1752] Loss: 32.2077\n",
      "Epoch [19/90], lter [281/1752] Loss: 21.3882\n",
      "Epoch [19/90], lter [291/1752] Loss: 20.4210\n",
      "Epoch [19/90], lter [301/1752] Loss: 21.9619\n",
      "Epoch [19/90], lter [311/1752] Loss: 21.8308\n",
      "Epoch [19/90], lter [321/1752] Loss: 16.9095\n",
      "Epoch [19/90], lter [331/1752] Loss: 32.9757\n",
      "Epoch [19/90], lter [341/1752] Loss: 18.3596\n",
      "Epoch [19/90], lter [351/1752] Loss: 26.0284\n",
      "Epoch [19/90], lter [361/1752] Loss: 25.1230\n",
      "Epoch [19/90], lter [371/1752] Loss: 23.6181\n",
      "Epoch [19/90], lter [381/1752] Loss: 14.3764\n",
      "Epoch [19/90], lter [391/1752] Loss: 24.0264\n",
      "Epoch [19/90], lter [401/1752] Loss: 16.4185\n",
      "Epoch [19/90], lter [411/1752] Loss: 28.4907\n",
      "Epoch [19/90], lter [421/1752] Loss: 21.8645\n",
      "Epoch [19/90], lter [431/1752] Loss: 16.3608\n",
      "Epoch [19/90], lter [441/1752] Loss: 23.5614\n",
      "Epoch [19/90], lter [451/1752] Loss: 17.1957\n",
      "Epoch [19/90], lter [461/1752] Loss: 21.0730\n",
      "Epoch [19/90], lter [471/1752] Loss: 26.8141\n",
      "Epoch [19/90], lter [481/1752] Loss: 18.9762\n",
      "Epoch [19/90], lter [491/1752] Loss: 17.0965\n",
      "Epoch [19/90], lter [501/1752] Loss: 32.1957\n",
      "Epoch [19/90], lter [511/1752] Loss: 37.2666\n",
      "Epoch [19/90], lter [521/1752] Loss: 21.1659\n",
      "Epoch [19/90], lter [531/1752] Loss: 24.2306\n",
      "Epoch [19/90], lter [541/1752] Loss: 27.1515\n",
      "Epoch [19/90], lter [551/1752] Loss: 25.2354\n",
      "Epoch [19/90], lter [561/1752] Loss: 24.9795\n",
      "Epoch [19/90], lter [571/1752] Loss: 19.4700\n",
      "Epoch [19/90], lter [581/1752] Loss: 47.3960\n",
      "Epoch [19/90], lter [591/1752] Loss: 38.1915\n",
      "Epoch [19/90], lter [601/1752] Loss: 25.8254\n",
      "Epoch [19/90], lter [611/1752] Loss: 21.2193\n",
      "Epoch [19/90], lter [621/1752] Loss: 13.3352\n",
      "Epoch [19/90], lter [631/1752] Loss: 28.5139\n",
      "Epoch [19/90], lter [641/1752] Loss: 21.4862\n",
      "Epoch [19/90], lter [651/1752] Loss: 27.9419\n",
      "Epoch [19/90], lter [661/1752] Loss: 23.1239\n",
      "Epoch [19/90], lter [671/1752] Loss: 12.2849\n",
      "Epoch [19/90], lter [681/1752] Loss: 20.1398\n",
      "Epoch [19/90], lter [691/1752] Loss: 17.6972\n",
      "Epoch [19/90], lter [701/1752] Loss: 19.5946\n",
      "Epoch [19/90], lter [711/1752] Loss: 21.8289\n",
      "Epoch [19/90], lter [721/1752] Loss: 24.4828\n",
      "Epoch [19/90], lter [731/1752] Loss: 29.3568\n",
      "Epoch [19/90], lter [741/1752] Loss: 17.1314\n",
      "Epoch [19/90], lter [751/1752] Loss: 23.7409\n",
      "Epoch [19/90], lter [761/1752] Loss: 17.1193\n",
      "Epoch [19/90], lter [771/1752] Loss: 30.0532\n",
      "Epoch [19/90], lter [781/1752] Loss: 18.6073\n",
      "Epoch [19/90], lter [791/1752] Loss: 36.4872\n",
      "Epoch [19/90], lter [801/1752] Loss: 22.3319\n",
      "Epoch [19/90], lter [811/1752] Loss: 26.9887\n",
      "Epoch [19/90], lter [821/1752] Loss: 20.3573\n",
      "Epoch [19/90], lter [831/1752] Loss: 23.5128\n",
      "Epoch [19/90], lter [841/1752] Loss: 21.7556\n",
      "Epoch [19/90], lter [851/1752] Loss: 24.8499\n",
      "Epoch [19/90], lter [861/1752] Loss: 28.8793\n",
      "Epoch [19/90], lter [871/1752] Loss: 12.8567\n",
      "Epoch [19/90], lter [881/1752] Loss: 20.9459\n",
      "Epoch [19/90], lter [891/1752] Loss: 26.0361\n",
      "Epoch [19/90], lter [901/1752] Loss: 13.4654\n",
      "Epoch [19/90], lter [911/1752] Loss: 24.1732\n",
      "Epoch [19/90], lter [921/1752] Loss: 30.7360\n",
      "Epoch [19/90], lter [931/1752] Loss: 9.1471\n",
      "Epoch [19/90], lter [941/1752] Loss: 43.3977\n",
      "Epoch [19/90], lter [951/1752] Loss: 30.0326\n",
      "Epoch [19/90], lter [961/1752] Loss: 13.5186\n",
      "Epoch [19/90], lter [971/1752] Loss: 30.7212\n",
      "Epoch [19/90], lter [981/1752] Loss: 21.4372\n",
      "Epoch [19/90], lter [991/1752] Loss: 30.2377\n",
      "Epoch [19/90], lter [1001/1752] Loss: 31.9007\n",
      "Epoch [19/90], lter [1011/1752] Loss: 16.9295\n",
      "Epoch [19/90], lter [1021/1752] Loss: 20.5440\n",
      "Epoch [19/90], lter [1031/1752] Loss: 22.0287\n",
      "Epoch [19/90], lter [1041/1752] Loss: 23.8496\n",
      "Epoch [19/90], lter [1051/1752] Loss: 26.6283\n",
      "Epoch [19/90], lter [1061/1752] Loss: 16.3309\n",
      "Epoch [19/90], lter [1071/1752] Loss: 24.4235\n",
      "Epoch [19/90], lter [1081/1752] Loss: 30.4929\n",
      "Epoch [19/90], lter [1091/1752] Loss: 23.2362\n",
      "Epoch [19/90], lter [1101/1752] Loss: 33.0192\n",
      "Epoch [19/90], lter [1111/1752] Loss: 24.5464\n",
      "Epoch [19/90], lter [1121/1752] Loss: 25.2505\n",
      "Epoch [19/90], lter [1131/1752] Loss: 23.2169\n",
      "Epoch [19/90], lter [1141/1752] Loss: 23.0868\n",
      "Epoch [19/90], lter [1151/1752] Loss: 17.8648\n",
      "Epoch [19/90], lter [1161/1752] Loss: 22.3064\n",
      "Epoch [19/90], lter [1171/1752] Loss: 23.9110\n",
      "Epoch [19/90], lter [1181/1752] Loss: 30.4199\n",
      "Epoch [19/90], lter [1191/1752] Loss: 15.8697\n",
      "Epoch [19/90], lter [1201/1752] Loss: 18.9972\n",
      "Epoch [19/90], lter [1211/1752] Loss: 22.2576\n",
      "Epoch [19/90], lter [1221/1752] Loss: 22.2559\n",
      "Epoch [19/90], lter [1231/1752] Loss: 26.9933\n",
      "Epoch [19/90], lter [1241/1752] Loss: 20.4643\n",
      "Epoch [19/90], lter [1251/1752] Loss: 26.7637\n",
      "Epoch [19/90], lter [1261/1752] Loss: 12.0370\n",
      "Epoch [19/90], lter [1271/1752] Loss: 15.5111\n",
      "Epoch [19/90], lter [1281/1752] Loss: 22.1895\n",
      "Epoch [19/90], lter [1291/1752] Loss: 20.5597\n",
      "Epoch [19/90], lter [1301/1752] Loss: 23.2455\n",
      "Epoch [19/90], lter [1311/1752] Loss: 30.3396\n",
      "Epoch [19/90], lter [1321/1752] Loss: 18.0454\n",
      "Epoch [19/90], lter [1331/1752] Loss: 32.2164\n",
      "Epoch [19/90], lter [1341/1752] Loss: 44.3868\n",
      "Epoch [19/90], lter [1351/1752] Loss: 22.3499\n",
      "Epoch [19/90], lter [1361/1752] Loss: 20.9168\n",
      "Epoch [19/90], lter [1371/1752] Loss: 14.6550\n",
      "Epoch [19/90], lter [1381/1752] Loss: 27.6614\n",
      "Epoch [19/90], lter [1391/1752] Loss: 23.9299\n",
      "Epoch [19/90], lter [1401/1752] Loss: 25.4265\n",
      "Epoch [19/90], lter [1411/1752] Loss: 23.0464\n",
      "Epoch [19/90], lter [1421/1752] Loss: 33.0379\n",
      "Epoch [19/90], lter [1431/1752] Loss: 26.8453\n",
      "Epoch [19/90], lter [1441/1752] Loss: 24.3614\n",
      "Epoch [19/90], lter [1451/1752] Loss: 33.2297\n",
      "Epoch [19/90], lter [1461/1752] Loss: 27.7576\n",
      "Epoch [19/90], lter [1471/1752] Loss: 24.2586\n",
      "Epoch [19/90], lter [1481/1752] Loss: 23.0833\n",
      "Epoch [19/90], lter [1491/1752] Loss: 24.1364\n",
      "Epoch [19/90], lter [1501/1752] Loss: 26.3701\n",
      "Epoch [19/90], lter [1511/1752] Loss: 28.0265\n",
      "Epoch [19/90], lter [1521/1752] Loss: 38.2511\n",
      "Epoch [19/90], lter [1531/1752] Loss: 28.8576\n",
      "Epoch [19/90], lter [1541/1752] Loss: 27.2202\n",
      "Epoch [19/90], lter [1551/1752] Loss: 19.7014\n",
      "Epoch [19/90], lter [1561/1752] Loss: 23.8234\n",
      "Epoch [19/90], lter [1571/1752] Loss: 27.4358\n",
      "Epoch [19/90], lter [1581/1752] Loss: 31.5402\n",
      "Epoch [19/90], lter [1591/1752] Loss: 21.1280\n",
      "Epoch [19/90], lter [1601/1752] Loss: 22.8753\n",
      "Epoch [19/90], lter [1611/1752] Loss: 24.1305\n",
      "Epoch [19/90], lter [1621/1752] Loss: 14.9747\n",
      "Epoch [19/90], lter [1631/1752] Loss: 35.3076\n",
      "Epoch [19/90], lter [1641/1752] Loss: 33.6143\n",
      "Epoch [19/90], lter [1651/1752] Loss: 25.0214\n",
      "Epoch [19/90], lter [1661/1752] Loss: 32.4525\n",
      "Epoch [19/90], lter [1671/1752] Loss: 32.7790\n",
      "Epoch [19/90], lter [1681/1752] Loss: 39.1676\n",
      "Epoch [19/90], lter [1691/1752] Loss: 34.1453\n",
      "Epoch [19/90], lter [1701/1752] Loss: 18.4289\n",
      "Epoch [19/90], lter [1711/1752] Loss: 20.4488\n",
      "Epoch [19/90], lter [1721/1752] Loss: 19.1364\n",
      "Epoch [19/90], lter [1731/1752] Loss: 35.3050\n",
      "Epoch [19/90], lter [1741/1752] Loss: 12.7630\n",
      "Epoch [19/90], lter [1751/1752] Loss: 21.6723\n",
      "Epoch:  19 | train loss : 24.5334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 19/90 [67:02:41<18:53:39, 958.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18 | test loss : 23.6425\n",
      "Epoch [20/90], lter [1/1752] Loss: 40.7260\n",
      "Epoch [20/90], lter [11/1752] Loss: 29.6156\n",
      "Epoch [20/90], lter [21/1752] Loss: 25.3194\n",
      "Epoch [20/90], lter [31/1752] Loss: 35.2173\n",
      "Epoch [20/90], lter [41/1752] Loss: 20.8120\n",
      "Epoch [20/90], lter [51/1752] Loss: 39.7707\n",
      "Epoch [20/90], lter [61/1752] Loss: 26.5569\n",
      "Epoch [20/90], lter [71/1752] Loss: 15.9928\n",
      "Epoch [20/90], lter [81/1752] Loss: 32.7592\n",
      "Epoch [20/90], lter [91/1752] Loss: 17.8363\n",
      "Epoch [20/90], lter [101/1752] Loss: 33.7058\n",
      "Epoch [20/90], lter [111/1752] Loss: 22.2726\n",
      "Epoch [20/90], lter [121/1752] Loss: 16.7634\n",
      "Epoch [20/90], lter [131/1752] Loss: 19.2558\n",
      "Epoch [20/90], lter [141/1752] Loss: 11.5977\n",
      "Epoch [20/90], lter [151/1752] Loss: 22.7617\n",
      "Epoch [20/90], lter [161/1752] Loss: 26.9195\n",
      "Epoch [20/90], lter [171/1752] Loss: 21.8325\n",
      "Epoch [20/90], lter [181/1752] Loss: 23.1065\n",
      "Epoch [20/90], lter [191/1752] Loss: 23.4927\n",
      "Epoch [20/90], lter [201/1752] Loss: 25.6351\n",
      "Epoch [20/90], lter [211/1752] Loss: 31.1989\n",
      "Epoch [20/90], lter [221/1752] Loss: 16.2032\n",
      "Epoch [20/90], lter [231/1752] Loss: 26.6960\n",
      "Epoch [20/90], lter [241/1752] Loss: 33.5451\n",
      "Epoch [20/90], lter [251/1752] Loss: 17.0569\n",
      "Epoch [20/90], lter [261/1752] Loss: 21.8200\n",
      "Epoch [20/90], lter [271/1752] Loss: 26.9275\n",
      "Epoch [20/90], lter [281/1752] Loss: 22.2610\n",
      "Epoch [20/90], lter [291/1752] Loss: 24.9992\n",
      "Epoch [20/90], lter [301/1752] Loss: 36.5429\n",
      "Epoch [20/90], lter [311/1752] Loss: 19.4587\n",
      "Epoch [20/90], lter [321/1752] Loss: 16.3450\n",
      "Epoch [20/90], lter [331/1752] Loss: 23.5162\n",
      "Epoch [20/90], lter [341/1752] Loss: 37.2680\n",
      "Epoch [20/90], lter [351/1752] Loss: 23.4581\n",
      "Epoch [20/90], lter [361/1752] Loss: 20.3865\n",
      "Epoch [20/90], lter [371/1752] Loss: 24.6759\n",
      "Epoch [20/90], lter [381/1752] Loss: 31.5003\n",
      "Epoch [20/90], lter [391/1752] Loss: 34.0824\n",
      "Epoch [20/90], lter [401/1752] Loss: 14.0046\n",
      "Epoch [20/90], lter [411/1752] Loss: 26.6194\n",
      "Epoch [20/90], lter [421/1752] Loss: 26.6266\n",
      "Epoch [20/90], lter [431/1752] Loss: 32.1371\n",
      "Epoch [20/90], lter [441/1752] Loss: 31.0062\n",
      "Epoch [20/90], lter [451/1752] Loss: 23.6588\n",
      "Epoch [20/90], lter [461/1752] Loss: 17.9649\n",
      "Epoch [20/90], lter [471/1752] Loss: 22.3214\n",
      "Epoch [20/90], lter [481/1752] Loss: 18.0823\n",
      "Epoch [20/90], lter [491/1752] Loss: 43.2524\n",
      "Epoch [20/90], lter [501/1752] Loss: 20.8866\n",
      "Epoch [20/90], lter [511/1752] Loss: 24.3583\n",
      "Epoch [20/90], lter [521/1752] Loss: 32.0687\n",
      "Epoch [20/90], lter [531/1752] Loss: 30.4863\n",
      "Epoch [20/90], lter [541/1752] Loss: 31.0548\n",
      "Epoch [20/90], lter [551/1752] Loss: 30.6720\n",
      "Epoch [20/90], lter [561/1752] Loss: 26.1246\n",
      "Epoch [20/90], lter [571/1752] Loss: 26.0675\n",
      "Epoch [20/90], lter [581/1752] Loss: 15.5395\n",
      "Epoch [20/90], lter [591/1752] Loss: 15.4191\n",
      "Epoch [20/90], lter [601/1752] Loss: 24.4289\n",
      "Epoch [20/90], lter [611/1752] Loss: 34.2373\n",
      "Epoch [20/90], lter [621/1752] Loss: 13.3771\n",
      "Epoch [20/90], lter [631/1752] Loss: 27.5538\n",
      "Epoch [20/90], lter [641/1752] Loss: 17.6161\n",
      "Epoch [20/90], lter [651/1752] Loss: 22.3439\n",
      "Epoch [20/90], lter [661/1752] Loss: 25.8078\n",
      "Epoch [20/90], lter [671/1752] Loss: 28.1145\n",
      "Epoch [20/90], lter [681/1752] Loss: 27.1706\n",
      "Epoch [20/90], lter [691/1752] Loss: 34.2119\n",
      "Epoch [20/90], lter [701/1752] Loss: 27.2857\n",
      "Epoch [20/90], lter [711/1752] Loss: 19.3771\n",
      "Epoch [20/90], lter [721/1752] Loss: 32.1437\n",
      "Epoch [20/90], lter [731/1752] Loss: 23.6136\n",
      "Epoch [20/90], lter [741/1752] Loss: 27.7540\n",
      "Epoch [20/90], lter [751/1752] Loss: 16.7894\n",
      "Epoch [20/90], lter [761/1752] Loss: 28.3904\n",
      "Epoch [20/90], lter [771/1752] Loss: 21.6318\n",
      "Epoch [20/90], lter [781/1752] Loss: 17.0439\n",
      "Epoch [20/90], lter [791/1752] Loss: 18.4946\n",
      "Epoch [20/90], lter [801/1752] Loss: 24.1768\n",
      "Epoch [20/90], lter [811/1752] Loss: 16.8411\n",
      "Epoch [20/90], lter [821/1752] Loss: 25.9837\n",
      "Epoch [20/90], lter [831/1752] Loss: 26.3877\n",
      "Epoch [20/90], lter [841/1752] Loss: 16.0313\n",
      "Epoch [20/90], lter [851/1752] Loss: 20.0279\n",
      "Epoch [20/90], lter [861/1752] Loss: 24.8767\n",
      "Epoch [20/90], lter [871/1752] Loss: 31.0173\n",
      "Epoch [20/90], lter [881/1752] Loss: 26.1376\n",
      "Epoch [20/90], lter [891/1752] Loss: 17.3555\n",
      "Epoch [20/90], lter [901/1752] Loss: 24.6895\n",
      "Epoch [20/90], lter [911/1752] Loss: 16.4431\n",
      "Epoch [20/90], lter [921/1752] Loss: 19.2042\n",
      "Epoch [20/90], lter [931/1752] Loss: 31.4268\n",
      "Epoch [20/90], lter [941/1752] Loss: 30.0780\n",
      "Epoch [20/90], lter [951/1752] Loss: 20.0762\n",
      "Epoch [20/90], lter [961/1752] Loss: 21.0117\n",
      "Epoch [20/90], lter [971/1752] Loss: 18.2523\n",
      "Epoch [20/90], lter [981/1752] Loss: 19.7811\n",
      "Epoch [20/90], lter [991/1752] Loss: 17.5917\n",
      "Epoch [20/90], lter [1001/1752] Loss: 23.3181\n",
      "Epoch [20/90], lter [1011/1752] Loss: 19.8393\n",
      "Epoch [20/90], lter [1021/1752] Loss: 22.4526\n",
      "Epoch [20/90], lter [1031/1752] Loss: 41.7785\n",
      "Epoch [20/90], lter [1041/1752] Loss: 30.7112\n",
      "Epoch [20/90], lter [1051/1752] Loss: 28.3422\n",
      "Epoch [20/90], lter [1061/1752] Loss: 35.4215\n",
      "Epoch [20/90], lter [1071/1752] Loss: 21.1860\n",
      "Epoch [20/90], lter [1081/1752] Loss: 21.9128\n",
      "Epoch [20/90], lter [1091/1752] Loss: 39.0105\n",
      "Epoch [20/90], lter [1101/1752] Loss: 18.5264\n",
      "Epoch [20/90], lter [1111/1752] Loss: 20.4780\n",
      "Epoch [20/90], lter [1121/1752] Loss: 31.6449\n",
      "Epoch [20/90], lter [1131/1752] Loss: 25.5070\n",
      "Epoch [20/90], lter [1141/1752] Loss: 29.5878\n",
      "Epoch [20/90], lter [1151/1752] Loss: 25.0262\n",
      "Epoch [20/90], lter [1161/1752] Loss: 15.0267\n",
      "Epoch [20/90], lter [1171/1752] Loss: 16.2059\n",
      "Epoch [20/90], lter [1181/1752] Loss: 27.4615\n",
      "Epoch [20/90], lter [1191/1752] Loss: 21.0860\n",
      "Epoch [20/90], lter [1201/1752] Loss: 23.8671\n",
      "Epoch [20/90], lter [1211/1752] Loss: 22.2055\n",
      "Epoch [20/90], lter [1221/1752] Loss: 25.1099\n",
      "Epoch [20/90], lter [1231/1752] Loss: 30.8544\n",
      "Epoch [20/90], lter [1241/1752] Loss: 25.5040\n",
      "Epoch [20/90], lter [1251/1752] Loss: 24.8265\n",
      "Epoch [20/90], lter [1261/1752] Loss: 24.0360\n",
      "Epoch [20/90], lter [1271/1752] Loss: 21.5598\n",
      "Epoch [20/90], lter [1281/1752] Loss: 31.0163\n",
      "Epoch [20/90], lter [1291/1752] Loss: 26.9810\n",
      "Epoch [20/90], lter [1301/1752] Loss: 19.3559\n",
      "Epoch [20/90], lter [1311/1752] Loss: 23.8576\n",
      "Epoch [20/90], lter [1321/1752] Loss: 32.1045\n",
      "Epoch [20/90], lter [1331/1752] Loss: 31.7104\n",
      "Epoch [20/90], lter [1341/1752] Loss: 26.6440\n",
      "Epoch [20/90], lter [1351/1752] Loss: 24.7663\n",
      "Epoch [20/90], lter [1361/1752] Loss: 30.2974\n",
      "Epoch [20/90], lter [1371/1752] Loss: 18.0640\n",
      "Epoch [20/90], lter [1381/1752] Loss: 19.9638\n",
      "Epoch [20/90], lter [1391/1752] Loss: 31.5592\n",
      "Epoch [20/90], lter [1401/1752] Loss: 27.9694\n",
      "Epoch [20/90], lter [1411/1752] Loss: 30.2555\n",
      "Epoch [20/90], lter [1421/1752] Loss: 30.1813\n",
      "Epoch [20/90], lter [1431/1752] Loss: 31.7188\n",
      "Epoch [20/90], lter [1441/1752] Loss: 18.1932\n",
      "Epoch [20/90], lter [1451/1752] Loss: 25.4483\n",
      "Epoch [20/90], lter [1461/1752] Loss: 26.3595\n",
      "Epoch [20/90], lter [1471/1752] Loss: 26.5622\n",
      "Epoch [20/90], lter [1481/1752] Loss: 34.9042\n",
      "Epoch [20/90], lter [1491/1752] Loss: 34.1414\n",
      "Epoch [20/90], lter [1501/1752] Loss: 25.0854\n",
      "Epoch [20/90], lter [1511/1752] Loss: 30.1791\n",
      "Epoch [20/90], lter [1521/1752] Loss: 34.4438\n",
      "Epoch [20/90], lter [1531/1752] Loss: 18.4484\n",
      "Epoch [20/90], lter [1541/1752] Loss: 16.3641\n",
      "Epoch [20/90], lter [1551/1752] Loss: 19.1601\n",
      "Epoch [20/90], lter [1561/1752] Loss: 23.9165\n",
      "Epoch [20/90], lter [1571/1752] Loss: 28.3401\n",
      "Epoch [20/90], lter [1581/1752] Loss: 15.5766\n",
      "Epoch [20/90], lter [1591/1752] Loss: 30.1407\n",
      "Epoch [20/90], lter [1601/1752] Loss: 26.4142\n",
      "Epoch [20/90], lter [1611/1752] Loss: 26.4247\n",
      "Epoch [20/90], lter [1621/1752] Loss: 21.9892\n",
      "Epoch [20/90], lter [1631/1752] Loss: 25.9701\n",
      "Epoch [20/90], lter [1641/1752] Loss: 23.2834\n",
      "Epoch [20/90], lter [1651/1752] Loss: 33.3708\n",
      "Epoch [20/90], lter [1661/1752] Loss: 32.9192\n",
      "Epoch [20/90], lter [1671/1752] Loss: 19.0120\n",
      "Epoch [20/90], lter [1681/1752] Loss: 37.8787\n",
      "Epoch [20/90], lter [1691/1752] Loss: 28.7334\n",
      "Epoch [20/90], lter [1701/1752] Loss: 31.2133\n",
      "Epoch [20/90], lter [1711/1752] Loss: 28.5655\n",
      "Epoch [20/90], lter [1721/1752] Loss: 34.4768\n",
      "Epoch [20/90], lter [1731/1752] Loss: 12.1151\n",
      "Epoch [20/90], lter [1741/1752] Loss: 22.0313\n",
      "Epoch [20/90], lter [1751/1752] Loss: 17.6254\n",
      "Epoch:  20 | train loss : 25.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/90 [67:16:48<17:58:59, 924.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19 | test loss : 26.2723\n",
      "Epoch [21/90], lter [1/1752] Loss: 36.4123\n",
      "Epoch [21/90], lter [11/1752] Loss: 37.4177\n",
      "Epoch [21/90], lter [21/1752] Loss: 24.4151\n",
      "Epoch [21/90], lter [31/1752] Loss: 34.0513\n",
      "Epoch [21/90], lter [41/1752] Loss: 13.6648\n",
      "Epoch [21/90], lter [51/1752] Loss: 29.0561\n",
      "Epoch [21/90], lter [61/1752] Loss: 26.2384\n",
      "Epoch [21/90], lter [71/1752] Loss: 23.0317\n",
      "Epoch [21/90], lter [81/1752] Loss: 12.8224\n",
      "Epoch [21/90], lter [91/1752] Loss: 20.0446\n",
      "Epoch [21/90], lter [101/1752] Loss: 29.0706\n",
      "Epoch [21/90], lter [111/1752] Loss: 23.3870\n",
      "Epoch [21/90], lter [121/1752] Loss: 32.9688\n",
      "Epoch [21/90], lter [131/1752] Loss: 14.0693\n",
      "Epoch [21/90], lter [141/1752] Loss: 29.5569\n",
      "Epoch [21/90], lter [151/1752] Loss: 14.9572\n",
      "Epoch [21/90], lter [161/1752] Loss: 15.7024\n",
      "Epoch [21/90], lter [171/1752] Loss: 23.9694\n",
      "Epoch [21/90], lter [181/1752] Loss: 22.6205\n",
      "Epoch [21/90], lter [191/1752] Loss: 29.2203\n",
      "Epoch [21/90], lter [201/1752] Loss: 18.4855\n",
      "Epoch [21/90], lter [211/1752] Loss: 25.5780\n",
      "Epoch [21/90], lter [221/1752] Loss: 25.1567\n",
      "Epoch [21/90], lter [231/1752] Loss: 20.9567\n",
      "Epoch [21/90], lter [241/1752] Loss: 29.2635\n",
      "Epoch [21/90], lter [251/1752] Loss: 24.0377\n",
      "Epoch [21/90], lter [261/1752] Loss: 29.0942\n",
      "Epoch [21/90], lter [271/1752] Loss: 27.4046\n",
      "Epoch [21/90], lter [281/1752] Loss: 25.7925\n",
      "Epoch [21/90], lter [291/1752] Loss: 22.0389\n",
      "Epoch [21/90], lter [301/1752] Loss: 21.9682\n",
      "Epoch [21/90], lter [311/1752] Loss: 25.4879\n",
      "Epoch [21/90], lter [321/1752] Loss: 31.6972\n",
      "Epoch [21/90], lter [331/1752] Loss: 18.2637\n",
      "Epoch [21/90], lter [341/1752] Loss: 23.3715\n",
      "Epoch [21/90], lter [351/1752] Loss: 17.5551\n",
      "Epoch [21/90], lter [361/1752] Loss: 28.2549\n",
      "Epoch [21/90], lter [371/1752] Loss: 14.6380\n",
      "Epoch [21/90], lter [381/1752] Loss: 39.3537\n",
      "Epoch [21/90], lter [391/1752] Loss: 14.4427\n",
      "Epoch [21/90], lter [401/1752] Loss: 28.9650\n",
      "Epoch [21/90], lter [411/1752] Loss: 21.9809\n",
      "Epoch [21/90], lter [421/1752] Loss: 22.6608\n",
      "Epoch [21/90], lter [431/1752] Loss: 36.4469\n",
      "Epoch [21/90], lter [441/1752] Loss: 31.1338\n",
      "Epoch [21/90], lter [451/1752] Loss: 40.5155\n",
      "Epoch [21/90], lter [461/1752] Loss: 28.1875\n",
      "Epoch [21/90], lter [471/1752] Loss: 32.7538\n",
      "Epoch [21/90], lter [481/1752] Loss: 40.9164\n",
      "Epoch [21/90], lter [491/1752] Loss: 20.3694\n",
      "Epoch [21/90], lter [501/1752] Loss: 24.2339\n",
      "Epoch [21/90], lter [511/1752] Loss: 16.4824\n",
      "Epoch [21/90], lter [521/1752] Loss: 16.8101\n",
      "Epoch [21/90], lter [531/1752] Loss: 33.3902\n",
      "Epoch [21/90], lter [541/1752] Loss: 20.5103\n",
      "Epoch [21/90], lter [551/1752] Loss: 19.9106\n",
      "Epoch [21/90], lter [561/1752] Loss: 26.7656\n",
      "Epoch [21/90], lter [571/1752] Loss: 27.3277\n",
      "Epoch [21/90], lter [581/1752] Loss: 25.2859\n",
      "Epoch [21/90], lter [591/1752] Loss: 24.8871\n",
      "Epoch [21/90], lter [601/1752] Loss: 21.8679\n",
      "Epoch [21/90], lter [611/1752] Loss: 34.0980\n",
      "Epoch [21/90], lter [621/1752] Loss: 31.5149\n",
      "Epoch [21/90], lter [631/1752] Loss: 30.1324\n",
      "Epoch [21/90], lter [641/1752] Loss: 32.8005\n",
      "Epoch [21/90], lter [651/1752] Loss: 38.1141\n",
      "Epoch [21/90], lter [661/1752] Loss: 26.2243\n",
      "Epoch [21/90], lter [671/1752] Loss: 26.1017\n",
      "Epoch [21/90], lter [681/1752] Loss: 28.3109\n",
      "Epoch [21/90], lter [691/1752] Loss: 22.7429\n",
      "Epoch [21/90], lter [701/1752] Loss: 31.1088\n",
      "Epoch [21/90], lter [711/1752] Loss: 25.1173\n",
      "Epoch [21/90], lter [721/1752] Loss: 19.4967\n",
      "Epoch [21/90], lter [731/1752] Loss: 27.5416\n",
      "Epoch [21/90], lter [741/1752] Loss: 15.5238\n",
      "Epoch [21/90], lter [751/1752] Loss: 20.3114\n",
      "Epoch [21/90], lter [761/1752] Loss: 22.4127\n",
      "Epoch [21/90], lter [771/1752] Loss: 22.5437\n",
      "Epoch [21/90], lter [781/1752] Loss: 16.6530\n",
      "Epoch [21/90], lter [791/1752] Loss: 27.4607\n",
      "Epoch [21/90], lter [801/1752] Loss: 20.4261\n",
      "Epoch [21/90], lter [811/1752] Loss: 12.9070\n",
      "Epoch [21/90], lter [821/1752] Loss: 32.3500\n",
      "Epoch [21/90], lter [831/1752] Loss: 27.0592\n",
      "Epoch [21/90], lter [841/1752] Loss: 29.4268\n",
      "Epoch [21/90], lter [851/1752] Loss: 29.1423\n",
      "Epoch [21/90], lter [861/1752] Loss: 25.1514\n",
      "Epoch [21/90], lter [871/1752] Loss: 23.3633\n",
      "Epoch [21/90], lter [881/1752] Loss: 24.6176\n",
      "Epoch [21/90], lter [891/1752] Loss: 15.5286\n",
      "Epoch [21/90], lter [901/1752] Loss: 17.4989\n",
      "Epoch [21/90], lter [911/1752] Loss: 21.9034\n",
      "Epoch [21/90], lter [921/1752] Loss: 29.7029\n",
      "Epoch [21/90], lter [931/1752] Loss: 25.9404\n",
      "Epoch [21/90], lter [941/1752] Loss: 16.2501\n",
      "Epoch [21/90], lter [951/1752] Loss: 20.9419\n",
      "Epoch [21/90], lter [961/1752] Loss: 26.3990\n",
      "Epoch [21/90], lter [971/1752] Loss: 27.1377\n",
      "Epoch [21/90], lter [981/1752] Loss: 20.7221\n",
      "Epoch [21/90], lter [991/1752] Loss: 24.7422\n",
      "Epoch [21/90], lter [1001/1752] Loss: 29.2915\n",
      "Epoch [21/90], lter [1011/1752] Loss: 35.0659\n",
      "Epoch [21/90], lter [1021/1752] Loss: 23.6903\n",
      "Epoch [21/90], lter [1031/1752] Loss: 31.9244\n",
      "Epoch [21/90], lter [1041/1752] Loss: 40.4601\n",
      "Epoch [21/90], lter [1051/1752] Loss: 37.9060\n",
      "Epoch [21/90], lter [1061/1752] Loss: 27.3016\n",
      "Epoch [21/90], lter [1071/1752] Loss: 38.1806\n",
      "Epoch [21/90], lter [1081/1752] Loss: 14.5151\n",
      "Epoch [21/90], lter [1091/1752] Loss: 37.6906\n",
      "Epoch [21/90], lter [1101/1752] Loss: 18.0526\n",
      "Epoch [21/90], lter [1111/1752] Loss: 19.2377\n",
      "Epoch [21/90], lter [1121/1752] Loss: 31.9264\n",
      "Epoch [21/90], lter [1131/1752] Loss: 18.6620\n",
      "Epoch [21/90], lter [1141/1752] Loss: 23.1245\n",
      "Epoch [21/90], lter [1151/1752] Loss: 33.8921\n",
      "Epoch [21/90], lter [1161/1752] Loss: 26.8136\n",
      "Epoch [21/90], lter [1171/1752] Loss: 11.9744\n",
      "Epoch [21/90], lter [1181/1752] Loss: 26.3218\n",
      "Epoch [21/90], lter [1191/1752] Loss: 27.0299\n",
      "Epoch [21/90], lter [1201/1752] Loss: 18.4021\n",
      "Epoch [21/90], lter [1211/1752] Loss: 27.7919\n",
      "Epoch [21/90], lter [1221/1752] Loss: 18.1700\n",
      "Epoch [21/90], lter [1231/1752] Loss: 25.8005\n",
      "Epoch [21/90], lter [1241/1752] Loss: 23.3505\n",
      "Epoch [21/90], lter [1251/1752] Loss: 25.5328\n",
      "Epoch [21/90], lter [1261/1752] Loss: 21.0298\n",
      "Epoch [21/90], lter [1271/1752] Loss: 38.2181\n",
      "Epoch [21/90], lter [1281/1752] Loss: 47.9267\n",
      "Epoch [21/90], lter [1291/1752] Loss: 24.3936\n",
      "Epoch [21/90], lter [1301/1752] Loss: 25.9471\n",
      "Epoch [21/90], lter [1311/1752] Loss: 32.2037\n",
      "Epoch [21/90], lter [1321/1752] Loss: 22.8255\n",
      "Epoch [21/90], lter [1331/1752] Loss: 25.4697\n",
      "Epoch [21/90], lter [1341/1752] Loss: 12.0248\n",
      "Epoch [21/90], lter [1351/1752] Loss: 15.0287\n",
      "Epoch [21/90], lter [1361/1752] Loss: 25.2515\n",
      "Epoch [21/90], lter [1371/1752] Loss: 18.0434\n",
      "Epoch [21/90], lter [1381/1752] Loss: 26.2108\n",
      "Epoch [21/90], lter [1391/1752] Loss: 25.9022\n",
      "Epoch [21/90], lter [1401/1752] Loss: 25.8696\n",
      "Epoch [21/90], lter [1411/1752] Loss: 42.0366\n",
      "Epoch [21/90], lter [1421/1752] Loss: 39.8421\n",
      "Epoch [21/90], lter [1431/1752] Loss: 27.4370\n",
      "Epoch [21/90], lter [1441/1752] Loss: 28.4970\n",
      "Epoch [21/90], lter [1451/1752] Loss: 21.6310\n",
      "Epoch [21/90], lter [1461/1752] Loss: 26.4180\n",
      "Epoch [21/90], lter [1471/1752] Loss: 19.2898\n",
      "Epoch [21/90], lter [1481/1752] Loss: 19.8219\n",
      "Epoch [21/90], lter [1491/1752] Loss: 24.3703\n",
      "Epoch [21/90], lter [1501/1752] Loss: 20.9220\n",
      "Epoch [21/90], lter [1511/1752] Loss: 27.9968\n",
      "Epoch [21/90], lter [1521/1752] Loss: 12.6588\n",
      "Epoch [21/90], lter [1531/1752] Loss: 24.9419\n",
      "Epoch [21/90], lter [1541/1752] Loss: 23.7695\n",
      "Epoch [21/90], lter [1551/1752] Loss: 32.9621\n",
      "Epoch [21/90], lter [1561/1752] Loss: 29.5543\n",
      "Epoch [21/90], lter [1571/1752] Loss: 29.7890\n",
      "Epoch [21/90], lter [1581/1752] Loss: 39.9852\n",
      "Epoch [21/90], lter [1591/1752] Loss: 33.1233\n",
      "Epoch [21/90], lter [1601/1752] Loss: 14.7668\n",
      "Epoch [21/90], lter [1611/1752] Loss: 22.1492\n",
      "Epoch [21/90], lter [1621/1752] Loss: 20.8305\n",
      "Epoch [21/90], lter [1631/1752] Loss: 20.1842\n",
      "Epoch [21/90], lter [1641/1752] Loss: 23.3413\n",
      "Epoch [21/90], lter [1651/1752] Loss: 23.4358\n",
      "Epoch [21/90], lter [1661/1752] Loss: 25.4393\n",
      "Epoch [21/90], lter [1671/1752] Loss: 23.6404\n",
      "Epoch [21/90], lter [1681/1752] Loss: 25.0708\n",
      "Epoch [21/90], lter [1691/1752] Loss: 35.1613\n",
      "Epoch [21/90], lter [1701/1752] Loss: 28.5773\n",
      "Epoch [21/90], lter [1711/1752] Loss: 32.9836\n",
      "Epoch [21/90], lter [1721/1752] Loss: 21.5901\n",
      "Epoch [21/90], lter [1731/1752] Loss: 19.2573\n",
      "Epoch [21/90], lter [1741/1752] Loss: 28.2607\n",
      "Epoch [21/90], lter [1751/1752] Loss: 24.8668\n",
      "Epoch:  21 | train loss : 24.7930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 21/90 [67:30:59<17:17:51, 902.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20 | test loss : 14.8498\n",
      "Epoch [22/90], lter [1/1752] Loss: 33.7604\n",
      "Epoch [22/90], lter [11/1752] Loss: 26.1560\n",
      "Epoch [22/90], lter [21/1752] Loss: 34.0441\n",
      "Epoch [22/90], lter [31/1752] Loss: 11.0350\n",
      "Epoch [22/90], lter [41/1752] Loss: 27.9588\n",
      "Epoch [22/90], lter [51/1752] Loss: 15.7697\n",
      "Epoch [22/90], lter [61/1752] Loss: 21.6841\n",
      "Epoch [22/90], lter [71/1752] Loss: 20.2826\n",
      "Epoch [22/90], lter [81/1752] Loss: 31.0375\n",
      "Epoch [22/90], lter [91/1752] Loss: 15.3527\n",
      "Epoch [22/90], lter [101/1752] Loss: 18.4933\n",
      "Epoch [22/90], lter [111/1752] Loss: 24.2063\n",
      "Epoch [22/90], lter [121/1752] Loss: 33.4356\n",
      "Epoch [22/90], lter [131/1752] Loss: 16.0870\n",
      "Epoch [22/90], lter [141/1752] Loss: 22.2148\n",
      "Epoch [22/90], lter [151/1752] Loss: 38.8650\n",
      "Epoch [22/90], lter [161/1752] Loss: 23.6571\n",
      "Epoch [22/90], lter [171/1752] Loss: 27.0713\n",
      "Epoch [22/90], lter [181/1752] Loss: 28.3004\n",
      "Epoch [22/90], lter [191/1752] Loss: 43.6152\n",
      "Epoch [22/90], lter [201/1752] Loss: 28.2104\n",
      "Epoch [22/90], lter [211/1752] Loss: 38.3357\n",
      "Epoch [22/90], lter [221/1752] Loss: 27.5954\n",
      "Epoch [22/90], lter [231/1752] Loss: 22.8740\n",
      "Epoch [22/90], lter [241/1752] Loss: 23.6218\n",
      "Epoch [22/90], lter [251/1752] Loss: 30.1713\n",
      "Epoch [22/90], lter [261/1752] Loss: 20.3711\n",
      "Epoch [22/90], lter [271/1752] Loss: 21.8222\n",
      "Epoch [22/90], lter [281/1752] Loss: 22.3152\n",
      "Epoch [22/90], lter [291/1752] Loss: 20.5801\n",
      "Epoch [22/90], lter [301/1752] Loss: 42.7409\n",
      "Epoch [22/90], lter [311/1752] Loss: 23.3532\n",
      "Epoch [22/90], lter [321/1752] Loss: 26.2546\n",
      "Epoch [22/90], lter [331/1752] Loss: 22.5245\n",
      "Epoch [22/90], lter [341/1752] Loss: 34.7800\n",
      "Epoch [22/90], lter [351/1752] Loss: 22.1580\n",
      "Epoch [22/90], lter [361/1752] Loss: 26.2082\n",
      "Epoch [22/90], lter [371/1752] Loss: 30.0476\n",
      "Epoch [22/90], lter [381/1752] Loss: 26.5905\n",
      "Epoch [22/90], lter [391/1752] Loss: 41.8629\n",
      "Epoch [22/90], lter [401/1752] Loss: 27.7184\n",
      "Epoch [22/90], lter [411/1752] Loss: 33.6386\n",
      "Epoch [22/90], lter [421/1752] Loss: 34.1828\n",
      "Epoch [22/90], lter [431/1752] Loss: 22.6925\n",
      "Epoch [22/90], lter [441/1752] Loss: 29.0637\n",
      "Epoch [22/90], lter [451/1752] Loss: 21.1764\n",
      "Epoch [22/90], lter [461/1752] Loss: 22.6714\n",
      "Epoch [22/90], lter [471/1752] Loss: 16.1253\n",
      "Epoch [22/90], lter [481/1752] Loss: 24.3621\n",
      "Epoch [22/90], lter [491/1752] Loss: 30.4281\n",
      "Epoch [22/90], lter [501/1752] Loss: 24.3531\n",
      "Epoch [22/90], lter [511/1752] Loss: 28.2335\n",
      "Epoch [22/90], lter [521/1752] Loss: 30.5385\n",
      "Epoch [22/90], lter [531/1752] Loss: 29.1820\n",
      "Epoch [22/90], lter [541/1752] Loss: 13.7454\n",
      "Epoch [22/90], lter [551/1752] Loss: 17.8589\n",
      "Epoch [22/90], lter [561/1752] Loss: 13.4053\n",
      "Epoch [22/90], lter [571/1752] Loss: 16.6008\n",
      "Epoch [22/90], lter [581/1752] Loss: 26.7492\n",
      "Epoch [22/90], lter [591/1752] Loss: 18.0491\n",
      "Epoch [22/90], lter [601/1752] Loss: 19.2130\n",
      "Epoch [22/90], lter [611/1752] Loss: 48.2369\n",
      "Epoch [22/90], lter [621/1752] Loss: 41.7231\n",
      "Epoch [22/90], lter [631/1752] Loss: 17.8248\n",
      "Epoch [22/90], lter [641/1752] Loss: 15.2715\n",
      "Epoch [22/90], lter [651/1752] Loss: 22.7381\n",
      "Epoch [22/90], lter [661/1752] Loss: 26.2086\n",
      "Epoch [22/90], lter [671/1752] Loss: 38.3387\n",
      "Epoch [22/90], lter [681/1752] Loss: 39.6756\n",
      "Epoch [22/90], lter [691/1752] Loss: 14.8573\n",
      "Epoch [22/90], lter [701/1752] Loss: 23.9007\n",
      "Epoch [22/90], lter [711/1752] Loss: 28.8099\n",
      "Epoch [22/90], lter [721/1752] Loss: 24.8973\n",
      "Epoch [22/90], lter [731/1752] Loss: 32.6920\n",
      "Epoch [22/90], lter [741/1752] Loss: 28.9688\n",
      "Epoch [22/90], lter [751/1752] Loss: 23.4225\n",
      "Epoch [22/90], lter [761/1752] Loss: 26.7561\n",
      "Epoch [22/90], lter [771/1752] Loss: 20.8825\n",
      "Epoch [22/90], lter [781/1752] Loss: 32.3996\n",
      "Epoch [22/90], lter [791/1752] Loss: 28.2240\n",
      "Epoch [22/90], lter [801/1752] Loss: 20.1134\n",
      "Epoch [22/90], lter [811/1752] Loss: 34.2108\n",
      "Epoch [22/90], lter [821/1752] Loss: 19.5483\n",
      "Epoch [22/90], lter [831/1752] Loss: 25.7783\n",
      "Epoch [22/90], lter [841/1752] Loss: 35.5452\n",
      "Epoch [22/90], lter [851/1752] Loss: 25.0215\n",
      "Epoch [22/90], lter [861/1752] Loss: 21.0737\n",
      "Epoch [22/90], lter [871/1752] Loss: 28.8295\n",
      "Epoch [22/90], lter [881/1752] Loss: 29.1303\n",
      "Epoch [22/90], lter [891/1752] Loss: 33.6988\n",
      "Epoch [22/90], lter [901/1752] Loss: 26.4695\n",
      "Epoch [22/90], lter [911/1752] Loss: 26.5567\n",
      "Epoch [22/90], lter [921/1752] Loss: 37.2930\n",
      "Epoch [22/90], lter [931/1752] Loss: 38.6737\n",
      "Epoch [22/90], lter [941/1752] Loss: 33.0116\n",
      "Epoch [22/90], lter [951/1752] Loss: 28.4563\n",
      "Epoch [22/90], lter [961/1752] Loss: 34.6389\n",
      "Epoch [22/90], lter [971/1752] Loss: 42.3340\n",
      "Epoch [22/90], lter [981/1752] Loss: 21.4018\n",
      "Epoch [22/90], lter [991/1752] Loss: 29.4182\n",
      "Epoch [22/90], lter [1001/1752] Loss: 32.7234\n",
      "Epoch [22/90], lter [1011/1752] Loss: 44.5836\n",
      "Epoch [22/90], lter [1021/1752] Loss: 21.7875\n",
      "Epoch [22/90], lter [1031/1752] Loss: 24.4382\n",
      "Epoch [22/90], lter [1041/1752] Loss: 27.8179\n",
      "Epoch [22/90], lter [1051/1752] Loss: 25.3739\n",
      "Epoch [22/90], lter [1061/1752] Loss: 44.2179\n",
      "Epoch [22/90], lter [1071/1752] Loss: 27.4345\n",
      "Epoch [22/90], lter [1081/1752] Loss: 35.4698\n",
      "Epoch [22/90], lter [1091/1752] Loss: 27.3511\n",
      "Epoch [22/90], lter [1101/1752] Loss: 21.1223\n",
      "Epoch [22/90], lter [1111/1752] Loss: 14.4153\n",
      "Epoch [22/90], lter [1121/1752] Loss: 23.3700\n",
      "Epoch [22/90], lter [1131/1752] Loss: 28.6166\n",
      "Epoch [22/90], lter [1141/1752] Loss: 20.6279\n",
      "Epoch [22/90], lter [1151/1752] Loss: 23.2836\n",
      "Epoch [22/90], lter [1161/1752] Loss: 17.0115\n",
      "Epoch [22/90], lter [1171/1752] Loss: 19.1730\n",
      "Epoch [22/90], lter [1181/1752] Loss: 23.6807\n",
      "Epoch [22/90], lter [1191/1752] Loss: 27.5117\n",
      "Epoch [22/90], lter [1201/1752] Loss: 25.8004\n",
      "Epoch [22/90], lter [1211/1752] Loss: 18.1663\n",
      "Epoch [22/90], lter [1221/1752] Loss: 39.5736\n",
      "Epoch [22/90], lter [1231/1752] Loss: 30.6010\n",
      "Epoch [22/90], lter [1241/1752] Loss: 35.3220\n",
      "Epoch [22/90], lter [1251/1752] Loss: 24.0390\n",
      "Epoch [22/90], lter [1261/1752] Loss: 20.1789\n",
      "Epoch [22/90], lter [1271/1752] Loss: 15.1668\n",
      "Epoch [22/90], lter [1281/1752] Loss: 28.1731\n",
      "Epoch [22/90], lter [1291/1752] Loss: 23.0333\n",
      "Epoch [22/90], lter [1301/1752] Loss: 29.0147\n",
      "Epoch [22/90], lter [1311/1752] Loss: 18.5660\n",
      "Epoch [22/90], lter [1321/1752] Loss: 27.8050\n",
      "Epoch [22/90], lter [1331/1752] Loss: 27.5721\n",
      "Epoch [22/90], lter [1341/1752] Loss: 30.4267\n",
      "Epoch [22/90], lter [1351/1752] Loss: 17.3219\n",
      "Epoch [22/90], lter [1361/1752] Loss: 36.8038\n",
      "Epoch [22/90], lter [1371/1752] Loss: 21.0719\n",
      "Epoch [22/90], lter [1381/1752] Loss: 25.7156\n",
      "Epoch [22/90], lter [1391/1752] Loss: 29.3289\n",
      "Epoch [22/90], lter [1401/1752] Loss: 29.6403\n",
      "Epoch [22/90], lter [1411/1752] Loss: 32.9456\n",
      "Epoch [22/90], lter [1421/1752] Loss: 28.7708\n",
      "Epoch [22/90], lter [1431/1752] Loss: 16.4343\n",
      "Epoch [22/90], lter [1441/1752] Loss: 24.1336\n",
      "Epoch [22/90], lter [1451/1752] Loss: 21.4092\n",
      "Epoch [22/90], lter [1461/1752] Loss: 23.2851\n",
      "Epoch [22/90], lter [1471/1752] Loss: 28.0967\n",
      "Epoch [22/90], lter [1481/1752] Loss: 26.2931\n",
      "Epoch [22/90], lter [1491/1752] Loss: 32.2883\n",
      "Epoch [22/90], lter [1501/1752] Loss: 26.2923\n",
      "Epoch [22/90], lter [1511/1752] Loss: 29.2724\n",
      "Epoch [22/90], lter [1521/1752] Loss: 14.9833\n",
      "Epoch [22/90], lter [1531/1752] Loss: 19.4921\n",
      "Epoch [22/90], lter [1541/1752] Loss: 13.9507\n",
      "Epoch [22/90], lter [1551/1752] Loss: 18.7517\n",
      "Epoch [22/90], lter [1561/1752] Loss: 23.2823\n",
      "Epoch [22/90], lter [1571/1752] Loss: 14.5704\n",
      "Epoch [22/90], lter [1581/1752] Loss: 15.6185\n",
      "Epoch [22/90], lter [1591/1752] Loss: 27.1191\n",
      "Epoch [22/90], lter [1601/1752] Loss: 19.9750\n",
      "Epoch [22/90], lter [1611/1752] Loss: 28.5065\n",
      "Epoch [22/90], lter [1621/1752] Loss: 47.5760\n",
      "Epoch [22/90], lter [1631/1752] Loss: 19.7572\n",
      "Epoch [22/90], lter [1641/1752] Loss: 22.6397\n",
      "Epoch [22/90], lter [1651/1752] Loss: 20.4702\n",
      "Epoch [22/90], lter [1661/1752] Loss: 31.0044\n",
      "Epoch [22/90], lter [1671/1752] Loss: 24.4693\n",
      "Epoch [22/90], lter [1681/1752] Loss: 23.4537\n",
      "Epoch [22/90], lter [1691/1752] Loss: 23.7695\n",
      "Epoch [22/90], lter [1701/1752] Loss: 28.4568\n",
      "Epoch [22/90], lter [1711/1752] Loss: 33.8129\n",
      "Epoch [22/90], lter [1721/1752] Loss: 28.5757\n",
      "Epoch [22/90], lter [1731/1752] Loss: 29.0503\n",
      "Epoch [22/90], lter [1741/1752] Loss: 21.4290\n",
      "Epoch [22/90], lter [1751/1752] Loss: 31.6157\n",
      "Epoch:  22 | train loss : 25.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 22/90 [67:44:57<16:41:07, 883.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21 | test loss : 14.8736\n",
      "Epoch [23/90], lter [1/1752] Loss: 20.6433\n",
      "Epoch [23/90], lter [11/1752] Loss: 19.6484\n",
      "Epoch [23/90], lter [21/1752] Loss: 11.8166\n",
      "Epoch [23/90], lter [31/1752] Loss: 20.9713\n",
      "Epoch [23/90], lter [41/1752] Loss: 24.7597\n",
      "Epoch [23/90], lter [51/1752] Loss: 26.3668\n",
      "Epoch [23/90], lter [61/1752] Loss: 19.6507\n",
      "Epoch [23/90], lter [71/1752] Loss: 14.1773\n",
      "Epoch [23/90], lter [81/1752] Loss: 25.8207\n",
      "Epoch [23/90], lter [91/1752] Loss: 30.6568\n",
      "Epoch [23/90], lter [101/1752] Loss: 35.1230\n",
      "Epoch [23/90], lter [111/1752] Loss: 18.6786\n",
      "Epoch [23/90], lter [121/1752] Loss: 19.2757\n",
      "Epoch [23/90], lter [131/1752] Loss: 10.0693\n",
      "Epoch [23/90], lter [141/1752] Loss: 19.5022\n",
      "Epoch [23/90], lter [151/1752] Loss: 20.0831\n",
      "Epoch [23/90], lter [161/1752] Loss: 22.7894\n",
      "Epoch [23/90], lter [171/1752] Loss: 27.0580\n",
      "Epoch [23/90], lter [181/1752] Loss: 32.9009\n",
      "Epoch [23/90], lter [191/1752] Loss: 29.2374\n",
      "Epoch [23/90], lter [201/1752] Loss: 27.4101\n",
      "Epoch [23/90], lter [211/1752] Loss: 19.2100\n",
      "Epoch [23/90], lter [221/1752] Loss: 15.9465\n",
      "Epoch [23/90], lter [231/1752] Loss: 32.7063\n",
      "Epoch [23/90], lter [241/1752] Loss: 28.1281\n",
      "Epoch [23/90], lter [251/1752] Loss: 22.6498\n",
      "Epoch [23/90], lter [261/1752] Loss: 21.0573\n",
      "Epoch [23/90], lter [271/1752] Loss: 15.4147\n",
      "Epoch [23/90], lter [281/1752] Loss: 13.8718\n",
      "Epoch [23/90], lter [291/1752] Loss: 25.0546\n",
      "Epoch [23/90], lter [301/1752] Loss: 41.6259\n",
      "Epoch [23/90], lter [311/1752] Loss: 21.3324\n",
      "Epoch [23/90], lter [321/1752] Loss: 23.1688\n",
      "Epoch [23/90], lter [331/1752] Loss: 15.2363\n",
      "Epoch [23/90], lter [341/1752] Loss: 42.5773\n",
      "Epoch [23/90], lter [351/1752] Loss: 20.5343\n",
      "Epoch [23/90], lter [361/1752] Loss: 25.0200\n",
      "Epoch [23/90], lter [371/1752] Loss: 25.2273\n",
      "Epoch [23/90], lter [381/1752] Loss: 29.1812\n",
      "Epoch [23/90], lter [391/1752] Loss: 15.7353\n",
      "Epoch [23/90], lter [401/1752] Loss: 30.6793\n",
      "Epoch [23/90], lter [411/1752] Loss: 22.0659\n",
      "Epoch [23/90], lter [421/1752] Loss: 27.0782\n",
      "Epoch [23/90], lter [431/1752] Loss: 18.9977\n",
      "Epoch [23/90], lter [441/1752] Loss: 25.5004\n",
      "Epoch [23/90], lter [451/1752] Loss: 33.5138\n",
      "Epoch [23/90], lter [461/1752] Loss: 19.2989\n",
      "Epoch [23/90], lter [471/1752] Loss: 20.1908\n",
      "Epoch [23/90], lter [481/1752] Loss: 17.3199\n",
      "Epoch [23/90], lter [491/1752] Loss: 22.7393\n",
      "Epoch [23/90], lter [501/1752] Loss: 16.7361\n",
      "Epoch [23/90], lter [511/1752] Loss: 11.6967\n",
      "Epoch [23/90], lter [521/1752] Loss: 22.7875\n",
      "Epoch [23/90], lter [531/1752] Loss: 33.7249\n",
      "Epoch [23/90], lter [541/1752] Loss: 16.8262\n",
      "Epoch [23/90], lter [551/1752] Loss: 18.5900\n",
      "Epoch [23/90], lter [561/1752] Loss: 35.5732\n",
      "Epoch [23/90], lter [571/1752] Loss: 27.8572\n",
      "Epoch [23/90], lter [581/1752] Loss: 28.3130\n",
      "Epoch [23/90], lter [591/1752] Loss: 23.8962\n",
      "Epoch [23/90], lter [601/1752] Loss: 16.2768\n",
      "Epoch [23/90], lter [611/1752] Loss: 16.2190\n",
      "Epoch [23/90], lter [621/1752] Loss: 38.0416\n",
      "Epoch [23/90], lter [631/1752] Loss: 20.7265\n",
      "Epoch [23/90], lter [641/1752] Loss: 26.7947\n",
      "Epoch [23/90], lter [651/1752] Loss: 31.7563\n",
      "Epoch [23/90], lter [661/1752] Loss: 33.6649\n",
      "Epoch [23/90], lter [671/1752] Loss: 20.4062\n",
      "Epoch [23/90], lter [681/1752] Loss: 17.1759\n",
      "Epoch [23/90], lter [691/1752] Loss: 26.8046\n",
      "Epoch [23/90], lter [701/1752] Loss: 16.8323\n",
      "Epoch [23/90], lter [711/1752] Loss: 21.7939\n",
      "Epoch [23/90], lter [721/1752] Loss: 29.3692\n",
      "Epoch [23/90], lter [731/1752] Loss: 23.8270\n",
      "Epoch [23/90], lter [741/1752] Loss: 22.9792\n",
      "Epoch [23/90], lter [751/1752] Loss: 27.1675\n",
      "Epoch [23/90], lter [761/1752] Loss: 16.6819\n",
      "Epoch [23/90], lter [771/1752] Loss: 25.3920\n",
      "Epoch [23/90], lter [781/1752] Loss: 19.1321\n",
      "Epoch [23/90], lter [791/1752] Loss: 19.9641\n",
      "Epoch [23/90], lter [801/1752] Loss: 23.2044\n",
      "Epoch [23/90], lter [811/1752] Loss: 23.0162\n",
      "Epoch [23/90], lter [821/1752] Loss: 19.7248\n",
      "Epoch [23/90], lter [831/1752] Loss: 25.2230\n",
      "Epoch [23/90], lter [841/1752] Loss: 29.4216\n",
      "Epoch [23/90], lter [851/1752] Loss: 28.1913\n",
      "Epoch [23/90], lter [861/1752] Loss: 24.5317\n",
      "Epoch [23/90], lter [871/1752] Loss: 19.7454\n",
      "Epoch [23/90], lter [881/1752] Loss: 19.7306\n",
      "Epoch [23/90], lter [891/1752] Loss: 25.2275\n",
      "Epoch [23/90], lter [901/1752] Loss: 20.6195\n",
      "Epoch [23/90], lter [911/1752] Loss: 16.3055\n",
      "Epoch [23/90], lter [921/1752] Loss: 16.1187\n",
      "Epoch [23/90], lter [931/1752] Loss: 17.9467\n",
      "Epoch [23/90], lter [941/1752] Loss: 24.0160\n",
      "Epoch [23/90], lter [951/1752] Loss: 31.9433\n",
      "Epoch [23/90], lter [961/1752] Loss: 23.7652\n",
      "Epoch [23/90], lter [971/1752] Loss: 16.3646\n",
      "Epoch [23/90], lter [981/1752] Loss: 24.1908\n",
      "Epoch [23/90], lter [991/1752] Loss: 19.3015\n",
      "Epoch [23/90], lter [1001/1752] Loss: 36.0542\n",
      "Epoch [23/90], lter [1011/1752] Loss: 19.1020\n",
      "Epoch [23/90], lter [1021/1752] Loss: 15.3193\n",
      "Epoch [23/90], lter [1031/1752] Loss: 30.2710\n",
      "Epoch [23/90], lter [1041/1752] Loss: 36.5268\n",
      "Epoch [23/90], lter [1051/1752] Loss: 18.2077\n",
      "Epoch [23/90], lter [1061/1752] Loss: 16.2331\n",
      "Epoch [23/90], lter [1071/1752] Loss: 27.5462\n",
      "Epoch [23/90], lter [1081/1752] Loss: 30.6389\n",
      "Epoch [23/90], lter [1091/1752] Loss: 15.6377\n",
      "Epoch [23/90], lter [1101/1752] Loss: 18.3822\n",
      "Epoch [23/90], lter [1111/1752] Loss: 37.7827\n",
      "Epoch [23/90], lter [1121/1752] Loss: 28.0208\n",
      "Epoch [23/90], lter [1131/1752] Loss: 32.3570\n",
      "Epoch [23/90], lter [1141/1752] Loss: 23.3693\n",
      "Epoch [23/90], lter [1151/1752] Loss: 24.3044\n",
      "Epoch [23/90], lter [1161/1752] Loss: 26.8355\n",
      "Epoch [23/90], lter [1171/1752] Loss: 20.6054\n",
      "Epoch [23/90], lter [1181/1752] Loss: 17.5489\n",
      "Epoch [23/90], lter [1191/1752] Loss: 10.9127\n",
      "Epoch [23/90], lter [1201/1752] Loss: 22.5505\n",
      "Epoch [23/90], lter [1211/1752] Loss: 18.8663\n",
      "Epoch [23/90], lter [1221/1752] Loss: 20.5885\n",
      "Epoch [23/90], lter [1231/1752] Loss: 24.1672\n",
      "Epoch [23/90], lter [1241/1752] Loss: 29.3686\n",
      "Epoch [23/90], lter [1251/1752] Loss: 16.5943\n",
      "Epoch [23/90], lter [1261/1752] Loss: 28.9236\n",
      "Epoch [23/90], lter [1271/1752] Loss: 28.6144\n",
      "Epoch [23/90], lter [1281/1752] Loss: 25.2050\n",
      "Epoch [23/90], lter [1291/1752] Loss: 29.2364\n",
      "Epoch [23/90], lter [1301/1752] Loss: 26.8098\n",
      "Epoch [23/90], lter [1311/1752] Loss: 32.4906\n",
      "Epoch [23/90], lter [1321/1752] Loss: 42.0261\n",
      "Epoch [23/90], lter [1331/1752] Loss: 21.6027\n",
      "Epoch [23/90], lter [1341/1752] Loss: 33.2680\n",
      "Epoch [23/90], lter [1351/1752] Loss: 25.5053\n",
      "Epoch [23/90], lter [1361/1752] Loss: 30.3075\n",
      "Epoch [23/90], lter [1371/1752] Loss: 23.9452\n",
      "Epoch [23/90], lter [1381/1752] Loss: 34.8862\n",
      "Epoch [23/90], lter [1391/1752] Loss: 26.1301\n",
      "Epoch [23/90], lter [1401/1752] Loss: 21.8692\n",
      "Epoch [23/90], lter [1411/1752] Loss: 27.6905\n",
      "Epoch [23/90], lter [1421/1752] Loss: 23.5963\n",
      "Epoch [23/90], lter [1431/1752] Loss: 34.4866\n",
      "Epoch [23/90], lter [1441/1752] Loss: 20.9202\n",
      "Epoch [23/90], lter [1451/1752] Loss: 26.5455\n",
      "Epoch [23/90], lter [1461/1752] Loss: 37.4291\n",
      "Epoch [23/90], lter [1471/1752] Loss: 36.3257\n",
      "Epoch [23/90], lter [1481/1752] Loss: 26.2062\n",
      "Epoch [23/90], lter [1491/1752] Loss: 19.5587\n",
      "Epoch [23/90], lter [1501/1752] Loss: 18.5657\n",
      "Epoch [23/90], lter [1511/1752] Loss: 25.6618\n",
      "Epoch [23/90], lter [1521/1752] Loss: 33.3717\n",
      "Epoch [23/90], lter [1531/1752] Loss: 14.3677\n",
      "Epoch [23/90], lter [1541/1752] Loss: 20.3340\n",
      "Epoch [23/90], lter [1551/1752] Loss: 23.3524\n",
      "Epoch [23/90], lter [1561/1752] Loss: 23.9093\n",
      "Epoch [23/90], lter [1571/1752] Loss: 24.0196\n",
      "Epoch [23/90], lter [1581/1752] Loss: 21.7889\n",
      "Epoch [23/90], lter [1591/1752] Loss: 28.1100\n",
      "Epoch [23/90], lter [1601/1752] Loss: 17.6021\n",
      "Epoch [23/90], lter [1611/1752] Loss: 11.7746\n",
      "Epoch [23/90], lter [1621/1752] Loss: 38.0147\n",
      "Epoch [23/90], lter [1631/1752] Loss: 25.2784\n",
      "Epoch [23/90], lter [1641/1752] Loss: 21.5664\n",
      "Epoch [23/90], lter [1651/1752] Loss: 26.9626\n",
      "Epoch [23/90], lter [1661/1752] Loss: 22.8892\n",
      "Epoch [23/90], lter [1671/1752] Loss: 24.8263\n",
      "Epoch [23/90], lter [1681/1752] Loss: 32.8422\n",
      "Epoch [23/90], lter [1691/1752] Loss: 19.4137\n",
      "Epoch [23/90], lter [1701/1752] Loss: 17.0457\n",
      "Epoch [23/90], lter [1711/1752] Loss: 21.1841\n",
      "Epoch [23/90], lter [1721/1752] Loss: 23.9299\n",
      "Epoch [23/90], lter [1731/1752] Loss: 23.8467\n",
      "Epoch [23/90], lter [1741/1752] Loss: 39.3256\n",
      "Epoch [23/90], lter [1751/1752] Loss: 27.2994\n",
      "Epoch:  23 | train loss : 24.5469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 23/90 [67:59:02<16:13:36, 871.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  22 | test loss : 23.1129\n",
      "Epoch [24/90], lter [1/1752] Loss: 26.7755\n",
      "Epoch [24/90], lter [11/1752] Loss: 12.2464\n",
      "Epoch [24/90], lter [21/1752] Loss: 32.3493\n",
      "Epoch [24/90], lter [31/1752] Loss: 32.7860\n",
      "Epoch [24/90], lter [41/1752] Loss: 13.6525\n",
      "Epoch [24/90], lter [51/1752] Loss: 20.6241\n",
      "Epoch [24/90], lter [61/1752] Loss: 22.0938\n",
      "Epoch [24/90], lter [71/1752] Loss: 21.3356\n",
      "Epoch [24/90], lter [81/1752] Loss: 30.0904\n",
      "Epoch [24/90], lter [91/1752] Loss: 16.1058\n",
      "Epoch [24/90], lter [101/1752] Loss: 22.4348\n",
      "Epoch [24/90], lter [111/1752] Loss: 13.1839\n",
      "Epoch [24/90], lter [121/1752] Loss: 17.4016\n",
      "Epoch [24/90], lter [131/1752] Loss: 14.2337\n",
      "Epoch [24/90], lter [141/1752] Loss: 23.5857\n",
      "Epoch [24/90], lter [151/1752] Loss: 21.7117\n",
      "Epoch [24/90], lter [161/1752] Loss: 19.5476\n",
      "Epoch [24/90], lter [171/1752] Loss: 29.6882\n",
      "Epoch [24/90], lter [181/1752] Loss: 13.9250\n",
      "Epoch [24/90], lter [191/1752] Loss: 16.9846\n",
      "Epoch [24/90], lter [201/1752] Loss: 23.3587\n",
      "Epoch [24/90], lter [211/1752] Loss: 19.0591\n",
      "Epoch [24/90], lter [221/1752] Loss: 16.3906\n",
      "Epoch [24/90], lter [231/1752] Loss: 31.7225\n",
      "Epoch [24/90], lter [241/1752] Loss: 12.9315\n",
      "Epoch [24/90], lter [251/1752] Loss: 22.7903\n",
      "Epoch [24/90], lter [261/1752] Loss: 22.5784\n",
      "Epoch [24/90], lter [271/1752] Loss: 29.1519\n",
      "Epoch [24/90], lter [281/1752] Loss: 27.8978\n",
      "Epoch [24/90], lter [291/1752] Loss: 27.9508\n",
      "Epoch [24/90], lter [301/1752] Loss: 16.6953\n",
      "Epoch [24/90], lter [311/1752] Loss: 20.1931\n",
      "Epoch [24/90], lter [321/1752] Loss: 31.1681\n",
      "Epoch [24/90], lter [331/1752] Loss: 27.8908\n",
      "Epoch [24/90], lter [341/1752] Loss: 22.9555\n",
      "Epoch [24/90], lter [351/1752] Loss: 25.8735\n",
      "Epoch [24/90], lter [361/1752] Loss: 28.8159\n",
      "Epoch [24/90], lter [371/1752] Loss: 19.6890\n",
      "Epoch [24/90], lter [381/1752] Loss: 32.1284\n",
      "Epoch [24/90], lter [391/1752] Loss: 21.9057\n",
      "Epoch [24/90], lter [401/1752] Loss: 24.5842\n",
      "Epoch [24/90], lter [411/1752] Loss: 30.3188\n",
      "Epoch [24/90], lter [421/1752] Loss: 16.3503\n",
      "Epoch [24/90], lter [431/1752] Loss: 24.4172\n",
      "Epoch [24/90], lter [441/1752] Loss: 34.1174\n",
      "Epoch [24/90], lter [451/1752] Loss: 24.7591\n",
      "Epoch [24/90], lter [461/1752] Loss: 19.8134\n",
      "Epoch [24/90], lter [471/1752] Loss: 18.2040\n",
      "Epoch [24/90], lter [481/1752] Loss: 40.4076\n",
      "Epoch [24/90], lter [491/1752] Loss: 19.6724\n",
      "Epoch [24/90], lter [501/1752] Loss: 21.1263\n",
      "Epoch [24/90], lter [511/1752] Loss: 17.4540\n",
      "Epoch [24/90], lter [521/1752] Loss: 24.1809\n",
      "Epoch [24/90], lter [531/1752] Loss: 14.2698\n",
      "Epoch [24/90], lter [541/1752] Loss: 29.7839\n",
      "Epoch [24/90], lter [551/1752] Loss: 25.9787\n",
      "Epoch [24/90], lter [561/1752] Loss: 37.6968\n",
      "Epoch [24/90], lter [571/1752] Loss: 8.7818\n",
      "Epoch [24/90], lter [581/1752] Loss: 26.2667\n",
      "Epoch [24/90], lter [591/1752] Loss: 16.1894\n",
      "Epoch [24/90], lter [601/1752] Loss: 12.9485\n",
      "Epoch [24/90], lter [611/1752] Loss: 30.2037\n",
      "Epoch [24/90], lter [621/1752] Loss: 23.7234\n",
      "Epoch [24/90], lter [631/1752] Loss: 26.7222\n",
      "Epoch [24/90], lter [641/1752] Loss: 38.3794\n",
      "Epoch [24/90], lter [651/1752] Loss: 20.8705\n",
      "Epoch [24/90], lter [661/1752] Loss: 22.1469\n",
      "Epoch [24/90], lter [671/1752] Loss: 18.4339\n",
      "Epoch [24/90], lter [681/1752] Loss: 17.9316\n",
      "Epoch [24/90], lter [691/1752] Loss: 40.0853\n",
      "Epoch [24/90], lter [701/1752] Loss: 18.0279\n",
      "Epoch [24/90], lter [711/1752] Loss: 28.8677\n",
      "Epoch [24/90], lter [721/1752] Loss: 9.7679\n",
      "Epoch [24/90], lter [731/1752] Loss: 22.0505\n",
      "Epoch [24/90], lter [741/1752] Loss: 29.0613\n",
      "Epoch [24/90], lter [751/1752] Loss: 20.1874\n",
      "Epoch [24/90], lter [761/1752] Loss: 21.8043\n",
      "Epoch [24/90], lter [771/1752] Loss: 24.1031\n",
      "Epoch [24/90], lter [781/1752] Loss: 20.3916\n",
      "Epoch [24/90], lter [791/1752] Loss: 25.0905\n",
      "Epoch [24/90], lter [801/1752] Loss: 21.7995\n",
      "Epoch [24/90], lter [811/1752] Loss: 22.6031\n",
      "Epoch [24/90], lter [821/1752] Loss: 28.1596\n",
      "Epoch [24/90], lter [831/1752] Loss: 32.5958\n",
      "Epoch [24/90], lter [841/1752] Loss: 22.3186\n",
      "Epoch [24/90], lter [851/1752] Loss: 23.6056\n",
      "Epoch [24/90], lter [861/1752] Loss: 25.7347\n",
      "Epoch [24/90], lter [871/1752] Loss: 31.7094\n",
      "Epoch [24/90], lter [881/1752] Loss: 12.3263\n",
      "Epoch [24/90], lter [891/1752] Loss: 25.3479\n",
      "Epoch [24/90], lter [901/1752] Loss: 27.5539\n",
      "Epoch [24/90], lter [911/1752] Loss: 22.4377\n",
      "Epoch [24/90], lter [921/1752] Loss: 28.9739\n",
      "Epoch [24/90], lter [931/1752] Loss: 20.7457\n",
      "Epoch [24/90], lter [941/1752] Loss: 22.7957\n",
      "Epoch [24/90], lter [951/1752] Loss: 20.1025\n",
      "Epoch [24/90], lter [961/1752] Loss: 21.0994\n",
      "Epoch [24/90], lter [971/1752] Loss: 26.5702\n",
      "Epoch [24/90], lter [981/1752] Loss: 24.9051\n",
      "Epoch [24/90], lter [991/1752] Loss: 20.9063\n",
      "Epoch [24/90], lter [1001/1752] Loss: 47.4697\n",
      "Epoch [24/90], lter [1011/1752] Loss: 35.9858\n",
      "Epoch [24/90], lter [1021/1752] Loss: 18.4842\n",
      "Epoch [24/90], lter [1031/1752] Loss: 10.8094\n",
      "Epoch [24/90], lter [1041/1752] Loss: 18.8076\n",
      "Epoch [24/90], lter [1051/1752] Loss: 18.8462\n",
      "Epoch [24/90], lter [1061/1752] Loss: 13.5025\n",
      "Epoch [24/90], lter [1071/1752] Loss: 36.7873\n",
      "Epoch [24/90], lter [1081/1752] Loss: 42.9922\n",
      "Epoch [24/90], lter [1091/1752] Loss: 25.8798\n",
      "Epoch [24/90], lter [1101/1752] Loss: 16.7600\n",
      "Epoch [24/90], lter [1111/1752] Loss: 26.2852\n",
      "Epoch [24/90], lter [1121/1752] Loss: 29.1271\n",
      "Epoch [24/90], lter [1131/1752] Loss: 23.8407\n",
      "Epoch [24/90], lter [1141/1752] Loss: 23.5025\n",
      "Epoch [24/90], lter [1151/1752] Loss: 17.4248\n",
      "Epoch [24/90], lter [1161/1752] Loss: 19.0034\n",
      "Epoch [24/90], lter [1171/1752] Loss: 25.6318\n",
      "Epoch [24/90], lter [1181/1752] Loss: 30.6688\n",
      "Epoch [24/90], lter [1191/1752] Loss: 18.6847\n",
      "Epoch [24/90], lter [1201/1752] Loss: 28.3533\n",
      "Epoch [24/90], lter [1211/1752] Loss: 17.2847\n",
      "Epoch [24/90], lter [1221/1752] Loss: 23.8665\n",
      "Epoch [24/90], lter [1231/1752] Loss: 14.5819\n",
      "Epoch [24/90], lter [1241/1752] Loss: 21.3216\n",
      "Epoch [24/90], lter [1251/1752] Loss: 13.2137\n",
      "Epoch [24/90], lter [1261/1752] Loss: 26.5972\n",
      "Epoch [24/90], lter [1271/1752] Loss: 43.3714\n",
      "Epoch [24/90], lter [1281/1752] Loss: 29.6734\n",
      "Epoch [24/90], lter [1291/1752] Loss: 11.8815\n",
      "Epoch [24/90], lter [1301/1752] Loss: 26.9717\n",
      "Epoch [24/90], lter [1311/1752] Loss: 21.4328\n",
      "Epoch [24/90], lter [1321/1752] Loss: 27.7904\n",
      "Epoch [24/90], lter [1331/1752] Loss: 54.2671\n",
      "Epoch [24/90], lter [1341/1752] Loss: 33.9292\n",
      "Epoch [24/90], lter [1351/1752] Loss: 21.5127\n",
      "Epoch [24/90], lter [1361/1752] Loss: 14.4991\n",
      "Epoch [24/90], lter [1371/1752] Loss: 40.0853\n",
      "Epoch [24/90], lter [1381/1752] Loss: 28.2735\n",
      "Epoch [24/90], lter [1391/1752] Loss: 25.1811\n",
      "Epoch [24/90], lter [1401/1752] Loss: 33.5317\n",
      "Epoch [24/90], lter [1411/1752] Loss: 20.3316\n",
      "Epoch [24/90], lter [1421/1752] Loss: 23.4078\n",
      "Epoch [24/90], lter [1431/1752] Loss: 23.8675\n",
      "Epoch [24/90], lter [1441/1752] Loss: 25.8446\n",
      "Epoch [24/90], lter [1451/1752] Loss: 24.9098\n",
      "Epoch [24/90], lter [1461/1752] Loss: 22.5513\n",
      "Epoch [24/90], lter [1471/1752] Loss: 21.3544\n",
      "Epoch [24/90], lter [1481/1752] Loss: 20.3305\n",
      "Epoch [24/90], lter [1491/1752] Loss: 29.2463\n",
      "Epoch [24/90], lter [1501/1752] Loss: 21.7790\n",
      "Epoch [24/90], lter [1511/1752] Loss: 15.2832\n",
      "Epoch [24/90], lter [1521/1752] Loss: 29.8026\n",
      "Epoch [24/90], lter [1531/1752] Loss: 24.0469\n",
      "Epoch [24/90], lter [1541/1752] Loss: 21.7471\n",
      "Epoch [24/90], lter [1551/1752] Loss: 23.0444\n",
      "Epoch [24/90], lter [1561/1752] Loss: 14.6106\n",
      "Epoch [24/90], lter [1571/1752] Loss: 24.2310\n",
      "Epoch [24/90], lter [1581/1752] Loss: 26.6663\n",
      "Epoch [24/90], lter [1591/1752] Loss: 28.0819\n",
      "Epoch [24/90], lter [1601/1752] Loss: 29.5904\n",
      "Epoch [24/90], lter [1611/1752] Loss: 17.3502\n",
      "Epoch [24/90], lter [1621/1752] Loss: 11.9535\n",
      "Epoch [24/90], lter [1631/1752] Loss: 22.1175\n",
      "Epoch [24/90], lter [1641/1752] Loss: 34.3391\n",
      "Epoch [24/90], lter [1651/1752] Loss: 17.9274\n",
      "Epoch [24/90], lter [1661/1752] Loss: 23.4070\n",
      "Epoch [24/90], lter [1671/1752] Loss: 33.2243\n",
      "Epoch [24/90], lter [1681/1752] Loss: 21.9550\n",
      "Epoch [24/90], lter [1691/1752] Loss: 30.4471\n",
      "Epoch [24/90], lter [1701/1752] Loss: 33.4101\n",
      "Epoch [24/90], lter [1711/1752] Loss: 22.8318\n",
      "Epoch [24/90], lter [1721/1752] Loss: 21.8984\n",
      "Epoch [24/90], lter [1731/1752] Loss: 29.8564\n",
      "Epoch [24/90], lter [1741/1752] Loss: 27.7426\n",
      "Epoch [24/90], lter [1751/1752] Loss: 11.5372\n",
      "Epoch:  24 | train loss : 24.2046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 24/90 [68:13:11<15:51:15, 864.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23 | test loss : 14.3728\n",
      "Epoch [25/90], lter [1/1752] Loss: 17.6625\n",
      "Epoch [25/90], lter [11/1752] Loss: 19.8536\n",
      "Epoch [25/90], lter [21/1752] Loss: 16.2843\n",
      "Epoch [25/90], lter [31/1752] Loss: 24.2278\n",
      "Epoch [25/90], lter [41/1752] Loss: 10.0437\n",
      "Epoch [25/90], lter [51/1752] Loss: 17.5565\n",
      "Epoch [25/90], lter [61/1752] Loss: 32.0472\n",
      "Epoch [25/90], lter [71/1752] Loss: 27.1284\n",
      "Epoch [25/90], lter [81/1752] Loss: 25.6626\n",
      "Epoch [25/90], lter [91/1752] Loss: 13.5065\n",
      "Epoch [25/90], lter [101/1752] Loss: 11.0844\n",
      "Epoch [25/90], lter [111/1752] Loss: 33.2345\n",
      "Epoch [25/90], lter [121/1752] Loss: 52.7540\n",
      "Epoch [25/90], lter [131/1752] Loss: 24.6536\n",
      "Epoch [25/90], lter [141/1752] Loss: 17.4849\n",
      "Epoch [25/90], lter [151/1752] Loss: 25.7733\n",
      "Epoch [25/90], lter [161/1752] Loss: 22.4089\n",
      "Epoch [25/90], lter [171/1752] Loss: 17.7008\n",
      "Epoch [25/90], lter [181/1752] Loss: 17.3465\n",
      "Epoch [25/90], lter [191/1752] Loss: 24.6816\n",
      "Epoch [25/90], lter [201/1752] Loss: 16.7847\n",
      "Epoch [25/90], lter [211/1752] Loss: 20.9887\n",
      "Epoch [25/90], lter [221/1752] Loss: 17.3362\n",
      "Epoch [25/90], lter [231/1752] Loss: 19.6704\n",
      "Epoch [25/90], lter [241/1752] Loss: 38.9557\n",
      "Epoch [25/90], lter [251/1752] Loss: 23.9394\n",
      "Epoch [25/90], lter [261/1752] Loss: 9.7852\n",
      "Epoch [25/90], lter [271/1752] Loss: 24.1919\n",
      "Epoch [25/90], lter [281/1752] Loss: 29.8265\n",
      "Epoch [25/90], lter [291/1752] Loss: 25.8681\n",
      "Epoch [25/90], lter [301/1752] Loss: 22.5366\n",
      "Epoch [25/90], lter [311/1752] Loss: 25.0933\n",
      "Epoch [25/90], lter [321/1752] Loss: 20.5023\n",
      "Epoch [25/90], lter [331/1752] Loss: 23.0535\n",
      "Epoch [25/90], lter [341/1752] Loss: 25.5351\n",
      "Epoch [25/90], lter [351/1752] Loss: 29.9313\n",
      "Epoch [25/90], lter [361/1752] Loss: 28.5380\n",
      "Epoch [25/90], lter [371/1752] Loss: 22.4768\n",
      "Epoch [25/90], lter [381/1752] Loss: 11.7597\n",
      "Epoch [25/90], lter [391/1752] Loss: 19.7387\n",
      "Epoch [25/90], lter [401/1752] Loss: 17.4417\n",
      "Epoch [25/90], lter [411/1752] Loss: 18.3909\n",
      "Epoch [25/90], lter [421/1752] Loss: 27.6196\n",
      "Epoch [25/90], lter [431/1752] Loss: 14.4681\n",
      "Epoch [25/90], lter [441/1752] Loss: 26.8328\n",
      "Epoch [25/90], lter [451/1752] Loss: 30.9056\n",
      "Epoch [25/90], lter [461/1752] Loss: 16.1997\n",
      "Epoch [25/90], lter [471/1752] Loss: 24.8648\n",
      "Epoch [25/90], lter [481/1752] Loss: 29.9476\n",
      "Epoch [25/90], lter [491/1752] Loss: 21.3648\n",
      "Epoch [25/90], lter [501/1752] Loss: 26.5778\n",
      "Epoch [25/90], lter [511/1752] Loss: 40.8095\n",
      "Epoch [25/90], lter [521/1752] Loss: 30.1154\n",
      "Epoch [25/90], lter [531/1752] Loss: 28.5317\n",
      "Epoch [25/90], lter [541/1752] Loss: 22.3037\n",
      "Epoch [25/90], lter [551/1752] Loss: 5.7729\n",
      "Epoch [25/90], lter [561/1752] Loss: 41.1528\n",
      "Epoch [25/90], lter [571/1752] Loss: 26.1524\n",
      "Epoch [25/90], lter [581/1752] Loss: 41.0464\n",
      "Epoch [25/90], lter [591/1752] Loss: 13.9553\n",
      "Epoch [25/90], lter [601/1752] Loss: 28.5365\n",
      "Epoch [25/90], lter [611/1752] Loss: 17.6626\n",
      "Epoch [25/90], lter [621/1752] Loss: 22.1885\n",
      "Epoch [25/90], lter [631/1752] Loss: 35.1039\n",
      "Epoch [25/90], lter [641/1752] Loss: 24.1224\n",
      "Epoch [25/90], lter [651/1752] Loss: 21.9966\n",
      "Epoch [25/90], lter [661/1752] Loss: 18.5578\n",
      "Epoch [25/90], lter [671/1752] Loss: 16.2653\n",
      "Epoch [25/90], lter [681/1752] Loss: 25.2032\n",
      "Epoch [25/90], lter [691/1752] Loss: 17.0787\n",
      "Epoch [25/90], lter [701/1752] Loss: 27.8604\n",
      "Epoch [25/90], lter [711/1752] Loss: 23.9796\n",
      "Epoch [25/90], lter [721/1752] Loss: 19.0566\n",
      "Epoch [25/90], lter [731/1752] Loss: 16.9773\n",
      "Epoch [25/90], lter [741/1752] Loss: 18.8956\n",
      "Epoch [25/90], lter [751/1752] Loss: 26.1432\n",
      "Epoch [25/90], lter [761/1752] Loss: 30.3572\n",
      "Epoch [25/90], lter [771/1752] Loss: 23.8265\n",
      "Epoch [25/90], lter [781/1752] Loss: 24.8642\n",
      "Epoch [25/90], lter [791/1752] Loss: 19.2754\n",
      "Epoch [25/90], lter [801/1752] Loss: 22.0590\n",
      "Epoch [25/90], lter [811/1752] Loss: 15.7678\n",
      "Epoch [25/90], lter [821/1752] Loss: 27.4924\n",
      "Epoch [25/90], lter [831/1752] Loss: 25.9618\n",
      "Epoch [25/90], lter [841/1752] Loss: 31.7243\n",
      "Epoch [25/90], lter [851/1752] Loss: 19.3277\n",
      "Epoch [25/90], lter [861/1752] Loss: 31.6205\n",
      "Epoch [25/90], lter [871/1752] Loss: 24.7756\n",
      "Epoch [25/90], lter [881/1752] Loss: 23.8228\n",
      "Epoch [25/90], lter [891/1752] Loss: 20.7820\n",
      "Epoch [25/90], lter [901/1752] Loss: 23.1873\n",
      "Epoch [25/90], lter [911/1752] Loss: 27.6652\n",
      "Epoch [25/90], lter [921/1752] Loss: 29.5031\n",
      "Epoch [25/90], lter [931/1752] Loss: 19.7597\n",
      "Epoch [25/90], lter [941/1752] Loss: 30.6622\n",
      "Epoch [25/90], lter [951/1752] Loss: 37.0452\n",
      "Epoch [25/90], lter [961/1752] Loss: 22.6285\n",
      "Epoch [25/90], lter [971/1752] Loss: 43.7284\n",
      "Epoch [25/90], lter [981/1752] Loss: 22.8530\n",
      "Epoch [25/90], lter [991/1752] Loss: 23.9665\n",
      "Epoch [25/90], lter [1001/1752] Loss: 25.8102\n",
      "Epoch [25/90], lter [1011/1752] Loss: 20.7645\n",
      "Epoch [25/90], lter [1021/1752] Loss: 24.2503\n",
      "Epoch [25/90], lter [1031/1752] Loss: 24.8491\n",
      "Epoch [25/90], lter [1041/1752] Loss: 23.2954\n",
      "Epoch [25/90], lter [1051/1752] Loss: 18.6472\n",
      "Epoch [25/90], lter [1061/1752] Loss: 24.3876\n",
      "Epoch [25/90], lter [1071/1752] Loss: 19.5158\n",
      "Epoch [25/90], lter [1081/1752] Loss: 21.9768\n",
      "Epoch [25/90], lter [1091/1752] Loss: 28.4175\n",
      "Epoch [25/90], lter [1101/1752] Loss: 19.8751\n",
      "Epoch [25/90], lter [1111/1752] Loss: 31.9579\n",
      "Epoch [25/90], lter [1121/1752] Loss: 20.9678\n",
      "Epoch [25/90], lter [1131/1752] Loss: 21.4364\n",
      "Epoch [25/90], lter [1141/1752] Loss: 31.5357\n",
      "Epoch [25/90], lter [1151/1752] Loss: 28.7621\n",
      "Epoch [25/90], lter [1161/1752] Loss: 39.0159\n",
      "Epoch [25/90], lter [1171/1752] Loss: 20.8649\n",
      "Epoch [25/90], lter [1181/1752] Loss: 44.9613\n",
      "Epoch [25/90], lter [1191/1752] Loss: 14.6350\n",
      "Epoch [25/90], lter [1201/1752] Loss: 25.3047\n",
      "Epoch [25/90], lter [1211/1752] Loss: 22.2796\n",
      "Epoch [25/90], lter [1221/1752] Loss: 31.3433\n",
      "Epoch [25/90], lter [1231/1752] Loss: 25.8364\n",
      "Epoch [25/90], lter [1241/1752] Loss: 24.1235\n",
      "Epoch [25/90], lter [1251/1752] Loss: 13.3558\n",
      "Epoch [25/90], lter [1261/1752] Loss: 21.4520\n",
      "Epoch [25/90], lter [1271/1752] Loss: 21.9106\n",
      "Epoch [25/90], lter [1281/1752] Loss: 36.3153\n",
      "Epoch [25/90], lter [1291/1752] Loss: 34.0474\n",
      "Epoch [25/90], lter [1301/1752] Loss: 20.7408\n",
      "Epoch [25/90], lter [1311/1752] Loss: 32.1263\n",
      "Epoch [25/90], lter [1321/1752] Loss: 38.0444\n",
      "Epoch [25/90], lter [1331/1752] Loss: 13.4385\n",
      "Epoch [25/90], lter [1341/1752] Loss: 22.4208\n",
      "Epoch [25/90], lter [1351/1752] Loss: 20.7271\n",
      "Epoch [25/90], lter [1361/1752] Loss: 20.8772\n",
      "Epoch [25/90], lter [1371/1752] Loss: 42.8428\n",
      "Epoch [25/90], lter [1381/1752] Loss: 21.1123\n",
      "Epoch [25/90], lter [1391/1752] Loss: 9.9347\n",
      "Epoch [25/90], lter [1401/1752] Loss: 32.8446\n",
      "Epoch [25/90], lter [1411/1752] Loss: 23.4004\n",
      "Epoch [25/90], lter [1421/1752] Loss: 18.9677\n",
      "Epoch [25/90], lter [1431/1752] Loss: 35.5009\n",
      "Epoch [25/90], lter [1441/1752] Loss: 28.7453\n",
      "Epoch [25/90], lter [1451/1752] Loss: 18.2303\n",
      "Epoch [25/90], lter [1461/1752] Loss: 15.5503\n",
      "Epoch [25/90], lter [1471/1752] Loss: 23.8074\n",
      "Epoch [25/90], lter [1481/1752] Loss: 27.7912\n",
      "Epoch [25/90], lter [1491/1752] Loss: 24.7644\n",
      "Epoch [25/90], lter [1501/1752] Loss: 21.9773\n",
      "Epoch [25/90], lter [1511/1752] Loss: 34.7007\n",
      "Epoch [25/90], lter [1521/1752] Loss: 29.7490\n",
      "Epoch [25/90], lter [1531/1752] Loss: 18.5885\n",
      "Epoch [25/90], lter [1541/1752] Loss: 22.5061\n",
      "Epoch [25/90], lter [1551/1752] Loss: 7.4642\n",
      "Epoch [25/90], lter [1561/1752] Loss: 22.3127\n",
      "Epoch [25/90], lter [1571/1752] Loss: 22.3024\n",
      "Epoch [25/90], lter [1581/1752] Loss: 23.1022\n",
      "Epoch [25/90], lter [1591/1752] Loss: 27.2101\n",
      "Epoch [25/90], lter [1601/1752] Loss: 15.1116\n",
      "Epoch [25/90], lter [1611/1752] Loss: 17.6066\n",
      "Epoch [25/90], lter [1621/1752] Loss: 36.1720\n",
      "Epoch [25/90], lter [1631/1752] Loss: 27.7050\n",
      "Epoch [25/90], lter [1641/1752] Loss: 33.9451\n",
      "Epoch [25/90], lter [1651/1752] Loss: 22.6568\n",
      "Epoch [25/90], lter [1661/1752] Loss: 37.2645\n",
      "Epoch [25/90], lter [1671/1752] Loss: 29.1920\n",
      "Epoch [25/90], lter [1681/1752] Loss: 25.0307\n",
      "Epoch [25/90], lter [1691/1752] Loss: 22.1847\n",
      "Epoch [25/90], lter [1701/1752] Loss: 22.3565\n",
      "Epoch [25/90], lter [1711/1752] Loss: 25.6403\n",
      "Epoch [25/90], lter [1721/1752] Loss: 27.7417\n",
      "Epoch [25/90], lter [1731/1752] Loss: 29.3123\n",
      "Epoch [25/90], lter [1741/1752] Loss: 26.5897\n",
      "Epoch [25/90], lter [1751/1752] Loss: 12.4659\n",
      "Epoch:  25 | train loss : 24.2670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 25/90 [68:27:21<15:32:11, 860.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  24 | test loss : 18.5703\n",
      "Epoch [26/90], lter [1/1752] Loss: 15.5016\n",
      "Epoch [26/90], lter [11/1752] Loss: 25.4698\n",
      "Epoch [26/90], lter [21/1752] Loss: 34.4095\n",
      "Epoch [26/90], lter [31/1752] Loss: 23.2267\n",
      "Epoch [26/90], lter [41/1752] Loss: 30.5404\n",
      "Epoch [26/90], lter [51/1752] Loss: 26.4561\n",
      "Epoch [26/90], lter [61/1752] Loss: 33.6093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/90], lter [181/1752] Loss: 9.7751\n",
      "Epoch [33/90], lter [191/1752] Loss: 6.6567\n",
      "Epoch [33/90], lter [201/1752] Loss: 10.7540\n",
      "Epoch [33/90], lter [211/1752] Loss: 9.2518\n",
      "Epoch [33/90], lter [221/1752] Loss: 8.2294\n",
      "Epoch [33/90], lter [231/1752] Loss: 8.2348\n",
      "Epoch [33/90], lter [241/1752] Loss: 8.2455\n",
      "Epoch [33/90], lter [251/1752] Loss: 5.3390\n",
      "Epoch [33/90], lter [261/1752] Loss: 6.4285\n",
      "Epoch [33/90], lter [271/1752] Loss: 7.2771\n",
      "Epoch [33/90], lter [281/1752] Loss: 6.2804\n",
      "Epoch [33/90], lter [291/1752] Loss: 9.0979\n",
      "Epoch [33/90], lter [301/1752] Loss: 8.6935\n",
      "Epoch [33/90], lter [311/1752] Loss: 8.7984\n",
      "Epoch [33/90], lter [321/1752] Loss: 4.2451\n",
      "Epoch [33/90], lter [331/1752] Loss: 4.2673\n",
      "Epoch [33/90], lter [341/1752] Loss: 7.7579\n",
      "Epoch [33/90], lter [351/1752] Loss: 3.8321\n",
      "Epoch [33/90], lter [361/1752] Loss: 7.7120\n",
      "Epoch [33/90], lter [371/1752] Loss: 8.2665\n",
      "Epoch [33/90], lter [381/1752] Loss: 6.1797\n",
      "Epoch [33/90], lter [391/1752] Loss: 8.4613\n",
      "Epoch [33/90], lter [401/1752] Loss: 7.7466\n",
      "Epoch [33/90], lter [411/1752] Loss: 6.8124\n",
      "Epoch [33/90], lter [421/1752] Loss: 11.7413\n",
      "Epoch [33/90], lter [431/1752] Loss: 6.7573\n",
      "Epoch [33/90], lter [441/1752] Loss: 4.1956\n",
      "Epoch [33/90], lter [451/1752] Loss: 3.6615\n",
      "Epoch [33/90], lter [461/1752] Loss: 11.0079\n",
      "Epoch [33/90], lter [471/1752] Loss: 5.4606\n",
      "Epoch [33/90], lter [481/1752] Loss: 6.7271\n",
      "Epoch [33/90], lter [491/1752] Loss: 7.7994\n",
      "Epoch [33/90], lter [501/1752] Loss: 7.8434\n",
      "Epoch [33/90], lter [511/1752] Loss: 8.4038\n",
      "Epoch [33/90], lter [521/1752] Loss: 2.9800\n",
      "Epoch [33/90], lter [531/1752] Loss: 6.4863\n",
      "Epoch [33/90], lter [541/1752] Loss: 6.7543\n",
      "Epoch [33/90], lter [551/1752] Loss: 7.6331\n",
      "Epoch [33/90], lter [561/1752] Loss: 5.3297\n",
      "Epoch [33/90], lter [571/1752] Loss: 5.9246\n",
      "Epoch [33/90], lter [581/1752] Loss: 6.8994\n",
      "Epoch [33/90], lter [591/1752] Loss: 5.1605\n",
      "Epoch [33/90], lter [601/1752] Loss: 5.7763\n",
      "Epoch [33/90], lter [611/1752] Loss: 6.2364\n",
      "Epoch [33/90], lter [621/1752] Loss: 5.4826\n",
      "Epoch [33/90], lter [631/1752] Loss: 3.2804\n",
      "Epoch [33/90], lter [641/1752] Loss: 5.8392\n",
      "Epoch [33/90], lter [651/1752] Loss: 5.7609\n",
      "Epoch [33/90], lter [661/1752] Loss: 4.9109\n",
      "Epoch [33/90], lter [671/1752] Loss: 6.5834\n",
      "Epoch [33/90], lter [681/1752] Loss: 6.0120\n",
      "Epoch [33/90], lter [691/1752] Loss: 5.2247\n",
      "Epoch [33/90], lter [701/1752] Loss: 7.0622\n",
      "Epoch [33/90], lter [711/1752] Loss: 5.5720\n",
      "Epoch [33/90], lter [721/1752] Loss: 8.2976\n",
      "Epoch [33/90], lter [731/1752] Loss: 7.3863\n",
      "Epoch [33/90], lter [741/1752] Loss: 6.2117\n",
      "Epoch [33/90], lter [751/1752] Loss: 5.5214\n",
      "Epoch [33/90], lter [761/1752] Loss: 9.8367\n",
      "Epoch [33/90], lter [771/1752] Loss: 6.3249\n",
      "Epoch [33/90], lter [781/1752] Loss: 4.2096\n",
      "Epoch [33/90], lter [791/1752] Loss: 3.8793\n",
      "Epoch [33/90], lter [801/1752] Loss: 6.3717\n",
      "Epoch [33/90], lter [811/1752] Loss: 4.1816\n",
      "Epoch [33/90], lter [821/1752] Loss: 6.2532\n",
      "Epoch [33/90], lter [831/1752] Loss: 6.7326\n",
      "Epoch [33/90], lter [841/1752] Loss: 6.1113\n",
      "Epoch [33/90], lter [851/1752] Loss: 3.8897\n",
      "Epoch [33/90], lter [861/1752] Loss: 5.3675\n",
      "Epoch [33/90], lter [871/1752] Loss: 5.7408\n",
      "Epoch [33/90], lter [881/1752] Loss: 4.6103\n",
      "Epoch [33/90], lter [891/1752] Loss: 6.0617\n",
      "Epoch [33/90], lter [901/1752] Loss: 7.6204\n",
      "Epoch [33/90], lter [911/1752] Loss: 6.6682\n",
      "Epoch [33/90], lter [921/1752] Loss: 6.0143\n",
      "Epoch [33/90], lter [931/1752] Loss: 5.2649\n",
      "Epoch [33/90], lter [941/1752] Loss: 8.4024\n",
      "Epoch [33/90], lter [951/1752] Loss: 8.3114\n",
      "Epoch [33/90], lter [961/1752] Loss: 5.2975\n",
      "Epoch [33/90], lter [971/1752] Loss: 5.0831\n",
      "Epoch [33/90], lter [981/1752] Loss: 5.8921\n",
      "Epoch [33/90], lter [991/1752] Loss: 4.9202\n",
      "Epoch [33/90], lter [1001/1752] Loss: 6.2043\n",
      "Epoch [33/90], lter [1011/1752] Loss: 6.2419\n",
      "Epoch [33/90], lter [1021/1752] Loss: 5.1426\n",
      "Epoch [33/90], lter [1031/1752] Loss: 4.0207\n",
      "Epoch [33/90], lter [1041/1752] Loss: 6.7268\n",
      "Epoch [33/90], lter [1051/1752] Loss: 6.4420\n",
      "Epoch [33/90], lter [1061/1752] Loss: 5.6576\n",
      "Epoch [33/90], lter [1071/1752] Loss: 4.1621\n",
      "Epoch [33/90], lter [1081/1752] Loss: 7.8771\n",
      "Epoch [33/90], lter [1091/1752] Loss: 6.3381\n",
      "Epoch [33/90], lter [1101/1752] Loss: 8.2897\n",
      "Epoch [33/90], lter [1111/1752] Loss: 5.9510\n",
      "Epoch [33/90], lter [1121/1752] Loss: 6.0015\n",
      "Epoch [33/90], lter [1131/1752] Loss: 5.8003\n",
      "Epoch [33/90], lter [1141/1752] Loss: 7.5952\n",
      "Epoch [33/90], lter [1151/1752] Loss: 4.6909\n",
      "Epoch [33/90], lter [1161/1752] Loss: 5.8437\n",
      "Epoch [33/90], lter [1171/1752] Loss: 3.5546\n",
      "Epoch [33/90], lter [1181/1752] Loss: 8.4774\n",
      "Epoch [33/90], lter [1191/1752] Loss: 6.5562\n",
      "Epoch [33/90], lter [1201/1752] Loss: 3.6577\n",
      "Epoch [33/90], lter [1211/1752] Loss: 3.1196\n",
      "Epoch [33/90], lter [1221/1752] Loss: 5.4410\n",
      "Epoch [33/90], lter [1231/1752] Loss: 9.3738\n",
      "Epoch [33/90], lter [1241/1752] Loss: 3.7342\n",
      "Epoch [33/90], lter [1251/1752] Loss: 5.4405\n",
      "Epoch [33/90], lter [1261/1752] Loss: 6.6285\n",
      "Epoch [33/90], lter [1271/1752] Loss: 9.1201\n",
      "Epoch [33/90], lter [1281/1752] Loss: 3.1806\n",
      "Epoch [33/90], lter [1291/1752] Loss: 5.8377\n",
      "Epoch [33/90], lter [1301/1752] Loss: 5.4575\n",
      "Epoch [33/90], lter [1311/1752] Loss: 6.4568\n",
      "Epoch [33/90], lter [1321/1752] Loss: 4.7792\n",
      "Epoch [33/90], lter [1331/1752] Loss: 4.5897\n",
      "Epoch [33/90], lter [1341/1752] Loss: 9.8202\n",
      "Epoch [33/90], lter [1351/1752] Loss: 4.9855\n",
      "Epoch [33/90], lter [1361/1752] Loss: 7.3952\n",
      "Epoch [33/90], lter [1371/1752] Loss: 7.5982\n",
      "Epoch [33/90], lter [1381/1752] Loss: 4.4852\n",
      "Epoch [33/90], lter [1391/1752] Loss: 6.6940\n",
      "Epoch [33/90], lter [1401/1752] Loss: 4.3077\n",
      "Epoch [33/90], lter [1411/1752] Loss: 3.8480\n",
      "Epoch [33/90], lter [1421/1752] Loss: 2.3900\n",
      "Epoch [33/90], lter [1431/1752] Loss: 6.3622\n",
      "Epoch [33/90], lter [1441/1752] Loss: 4.1733\n",
      "Epoch [33/90], lter [1451/1752] Loss: 3.7885\n",
      "Epoch [33/90], lter [1461/1752] Loss: 4.9147\n",
      "Epoch [33/90], lter [1471/1752] Loss: 6.1955\n",
      "Epoch [33/90], lter [1481/1752] Loss: 4.1006\n",
      "Epoch [33/90], lter [1491/1752] Loss: 8.4433\n",
      "Epoch [33/90], lter [1501/1752] Loss: 6.2380\n",
      "Epoch [33/90], lter [1511/1752] Loss: 5.3901\n",
      "Epoch [33/90], lter [1521/1752] Loss: 2.5503\n",
      "Epoch [33/90], lter [1531/1752] Loss: 4.4401\n",
      "Epoch [33/90], lter [1541/1752] Loss: 5.6621\n",
      "Epoch [33/90], lter [1551/1752] Loss: 6.2394\n",
      "Epoch [33/90], lter [1561/1752] Loss: 4.2241\n",
      "Epoch [33/90], lter [1571/1752] Loss: 2.5210\n",
      "Epoch [33/90], lter [1581/1752] Loss: 5.2979\n",
      "Epoch [33/90], lter [1591/1752] Loss: 1.6576\n",
      "Epoch [33/90], lter [1601/1752] Loss: 2.7095\n",
      "Epoch [33/90], lter [1611/1752] Loss: 6.6066\n",
      "Epoch [33/90], lter [1621/1752] Loss: 3.8177\n",
      "Epoch [33/90], lter [1631/1752] Loss: 5.1390\n",
      "Epoch [33/90], lter [1641/1752] Loss: 6.4038\n",
      "Epoch [33/90], lter [1651/1752] Loss: 3.3687\n",
      "Epoch [33/90], lter [1661/1752] Loss: 6.0206\n",
      "Epoch [33/90], lter [1671/1752] Loss: 4.9222\n",
      "Epoch [33/90], lter [1681/1752] Loss: 5.8015\n",
      "Epoch [33/90], lter [1691/1752] Loss: 5.0288\n",
      "Epoch [33/90], lter [1701/1752] Loss: 5.6054\n",
      "Epoch [33/90], lter [1711/1752] Loss: 6.6006\n",
      "Epoch [33/90], lter [1721/1752] Loss: 4.2490\n",
      "Epoch [33/90], lter [1731/1752] Loss: 5.4438\n",
      "Epoch [33/90], lter [1741/1752] Loss: 4.9102\n",
      "Epoch [33/90], lter [1751/1752] Loss: 6.4830\n",
      "Epoch:  33 | train loss : 6.2916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 33/90 [70:22:04<13:53:33, 877.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  32 | test loss : 3.3447\n",
      "Epoch [34/90], lter [1/1752] Loss: 3.6700\n",
      "Epoch [34/90], lter [11/1752] Loss: 3.9610\n",
      "Epoch [34/90], lter [21/1752] Loss: 3.5822\n",
      "Epoch [34/90], lter [31/1752] Loss: 6.6400\n",
      "Epoch [34/90], lter [41/1752] Loss: 4.5799\n",
      "Epoch [34/90], lter [51/1752] Loss: 5.7734\n",
      "Epoch [34/90], lter [61/1752] Loss: 6.4170\n",
      "Epoch [34/90], lter [71/1752] Loss: 7.0863\n",
      "Epoch [34/90], lter [81/1752] Loss: 4.6083\n",
      "Epoch [34/90], lter [91/1752] Loss: 6.9184\n",
      "Epoch [34/90], lter [101/1752] Loss: 3.8883\n",
      "Epoch [34/90], lter [111/1752] Loss: 4.5531\n",
      "Epoch [34/90], lter [121/1752] Loss: 4.7245\n",
      "Epoch [34/90], lter [131/1752] Loss: 3.8316\n",
      "Epoch [34/90], lter [141/1752] Loss: 4.8985\n",
      "Epoch [34/90], lter [151/1752] Loss: 3.1011\n",
      "Epoch [34/90], lter [161/1752] Loss: 5.8293\n",
      "Epoch [34/90], lter [171/1752] Loss: 3.0419\n",
      "Epoch [34/90], lter [181/1752] Loss: 3.9986\n",
      "Epoch [34/90], lter [191/1752] Loss: 3.0132\n",
      "Epoch [34/90], lter [201/1752] Loss: 5.3022\n",
      "Epoch [34/90], lter [211/1752] Loss: 4.2027\n",
      "Epoch [34/90], lter [221/1752] Loss: 3.4769\n",
      "Epoch [34/90], lter [231/1752] Loss: 5.7803\n",
      "Epoch [34/90], lter [241/1752] Loss: 4.3456\n",
      "Epoch [34/90], lter [251/1752] Loss: 1.9604\n",
      "Epoch [34/90], lter [261/1752] Loss: 4.2362\n",
      "Epoch [34/90], lter [271/1752] Loss: 3.7866\n",
      "Epoch [34/90], lter [281/1752] Loss: 4.4783\n",
      "Epoch [34/90], lter [291/1752] Loss: 4.3246\n",
      "Epoch [34/90], lter [301/1752] Loss: 5.3428\n",
      "Epoch [34/90], lter [311/1752] Loss: 6.8241\n",
      "Epoch [34/90], lter [321/1752] Loss: 3.9529\n",
      "Epoch [34/90], lter [331/1752] Loss: 6.9729\n",
      "Epoch [34/90], lter [341/1752] Loss: 2.7192\n",
      "Epoch [34/90], lter [351/1752] Loss: 3.9395\n",
      "Epoch [34/90], lter [361/1752] Loss: 5.8162\n",
      "Epoch [34/90], lter [371/1752] Loss: 3.7456\n",
      "Epoch [34/90], lter [381/1752] Loss: 4.7441\n",
      "Epoch [34/90], lter [391/1752] Loss: 3.7078\n",
      "Epoch [34/90], lter [401/1752] Loss: 4.9595\n",
      "Epoch [34/90], lter [411/1752] Loss: 5.1174\n",
      "Epoch [34/90], lter [421/1752] Loss: 2.5937\n",
      "Epoch [34/90], lter [431/1752] Loss: 3.6900\n",
      "Epoch [34/90], lter [441/1752] Loss: 5.1820\n",
      "Epoch [34/90], lter [451/1752] Loss: 5.9006\n",
      "Epoch [34/90], lter [461/1752] Loss: 3.1408\n",
      "Epoch [34/90], lter [471/1752] Loss: 3.0355\n",
      "Epoch [34/90], lter [481/1752] Loss: 3.9612\n",
      "Epoch [34/90], lter [491/1752] Loss: 3.3358\n",
      "Epoch [34/90], lter [501/1752] Loss: 3.6890\n",
      "Epoch [34/90], lter [511/1752] Loss: 3.4980\n",
      "Epoch [34/90], lter [521/1752] Loss: 3.1236\n",
      "Epoch [34/90], lter [531/1752] Loss: 4.1010\n",
      "Epoch [34/90], lter [541/1752] Loss: 4.6744\n",
      "Epoch [34/90], lter [551/1752] Loss: 7.1183\n",
      "Epoch [34/90], lter [561/1752] Loss: 5.2629\n",
      "Epoch [34/90], lter [571/1752] Loss: 4.6869\n",
      "Epoch [34/90], lter [581/1752] Loss: 3.4317\n",
      "Epoch [34/90], lter [591/1752] Loss: 4.4212\n",
      "Epoch [34/90], lter [601/1752] Loss: 5.6084\n",
      "Epoch [34/90], lter [611/1752] Loss: 3.0000\n",
      "Epoch [34/90], lter [621/1752] Loss: 4.5853\n",
      "Epoch [34/90], lter [631/1752] Loss: 3.1232\n",
      "Epoch [34/90], lter [641/1752] Loss: 3.0106\n",
      "Epoch [34/90], lter [651/1752] Loss: 4.4013\n",
      "Epoch [34/90], lter [661/1752] Loss: 3.9521\n",
      "Epoch [34/90], lter [671/1752] Loss: 3.2687\n",
      "Epoch [34/90], lter [681/1752] Loss: 3.0655\n",
      "Epoch [34/90], lter [691/1752] Loss: 3.1346\n",
      "Epoch [34/90], lter [701/1752] Loss: 2.9064\n",
      "Epoch [34/90], lter [711/1752] Loss: 3.3749\n",
      "Epoch [34/90], lter [721/1752] Loss: 2.9363\n",
      "Epoch [34/90], lter [731/1752] Loss: 3.7176\n",
      "Epoch [34/90], lter [741/1752] Loss: 2.4139\n",
      "Epoch [34/90], lter [751/1752] Loss: 4.9750\n",
      "Epoch [34/90], lter [761/1752] Loss: 4.3296\n",
      "Epoch [34/90], lter [771/1752] Loss: 3.6961\n",
      "Epoch [34/90], lter [781/1752] Loss: 6.2348\n",
      "Epoch [34/90], lter [791/1752] Loss: 2.8592\n",
      "Epoch [34/90], lter [801/1752] Loss: 2.3405\n",
      "Epoch [34/90], lter [811/1752] Loss: 3.9389\n",
      "Epoch [34/90], lter [821/1752] Loss: 4.8139\n",
      "Epoch [34/90], lter [831/1752] Loss: 4.3844\n",
      "Epoch [34/90], lter [841/1752] Loss: 3.4396\n",
      "Epoch [34/90], lter [851/1752] Loss: 5.0274\n",
      "Epoch [34/90], lter [861/1752] Loss: 4.7869\n",
      "Epoch [34/90], lter [871/1752] Loss: 3.6870\n",
      "Epoch [34/90], lter [881/1752] Loss: 4.2357\n",
      "Epoch [34/90], lter [891/1752] Loss: 3.2090\n",
      "Epoch [34/90], lter [901/1752] Loss: 4.7298\n",
      "Epoch [34/90], lter [911/1752] Loss: 5.6918\n",
      "Epoch [34/90], lter [921/1752] Loss: 3.6690\n",
      "Epoch [34/90], lter [931/1752] Loss: 4.8168\n",
      "Epoch [34/90], lter [941/1752] Loss: 5.1924\n",
      "Epoch [34/90], lter [951/1752] Loss: 4.7024\n",
      "Epoch [34/90], lter [961/1752] Loss: 4.4317\n",
      "Epoch [34/90], lter [971/1752] Loss: 3.7198\n",
      "Epoch [34/90], lter [981/1752] Loss: 3.5477\n",
      "Epoch [34/90], lter [991/1752] Loss: 3.1263\n",
      "Epoch [34/90], lter [1001/1752] Loss: 4.0344\n",
      "Epoch [34/90], lter [1011/1752] Loss: 5.2384\n",
      "Epoch [34/90], lter [1021/1752] Loss: 4.4961\n",
      "Epoch [34/90], lter [1031/1752] Loss: 4.3369\n",
      "Epoch [34/90], lter [1041/1752] Loss: 3.7943\n",
      "Epoch [34/90], lter [1051/1752] Loss: 5.7464\n",
      "Epoch [34/90], lter [1061/1752] Loss: 5.0474\n",
      "Epoch [34/90], lter [1071/1752] Loss: 3.2359\n",
      "Epoch [34/90], lter [1081/1752] Loss: 8.0902\n",
      "Epoch [34/90], lter [1091/1752] Loss: 3.0647\n",
      "Epoch [34/90], lter [1101/1752] Loss: 5.5231\n",
      "Epoch [34/90], lter [1111/1752] Loss: 3.3117\n",
      "Epoch [34/90], lter [1121/1752] Loss: 3.4307\n",
      "Epoch [34/90], lter [1131/1752] Loss: 3.8174\n",
      "Epoch [34/90], lter [1141/1752] Loss: 5.7380\n",
      "Epoch [34/90], lter [1151/1752] Loss: 4.9898\n",
      "Epoch [34/90], lter [1161/1752] Loss: 5.0748\n",
      "Epoch [34/90], lter [1171/1752] Loss: 3.7824\n",
      "Epoch [34/90], lter [1181/1752] Loss: 6.3831\n",
      "Epoch [34/90], lter [1191/1752] Loss: 3.7576\n",
      "Epoch [34/90], lter [1201/1752] Loss: 3.2940\n",
      "Epoch [34/90], lter [1211/1752] Loss: 3.3922\n",
      "Epoch [34/90], lter [1221/1752] Loss: 2.7220\n",
      "Epoch [34/90], lter [1231/1752] Loss: 1.5626\n",
      "Epoch [34/90], lter [1241/1752] Loss: 3.1120\n",
      "Epoch [34/90], lter [1251/1752] Loss: 2.0583\n",
      "Epoch [34/90], lter [1261/1752] Loss: 4.2427\n",
      "Epoch [34/90], lter [1271/1752] Loss: 2.7544\n",
      "Epoch [34/90], lter [1281/1752] Loss: 2.5963\n",
      "Epoch [34/90], lter [1291/1752] Loss: 3.4807\n",
      "Epoch [34/90], lter [1301/1752] Loss: 2.7699\n",
      "Epoch [34/90], lter [1311/1752] Loss: 3.0367\n",
      "Epoch [34/90], lter [1321/1752] Loss: 5.8259\n",
      "Epoch [34/90], lter [1331/1752] Loss: 3.9188\n",
      "Epoch [34/90], lter [1341/1752] Loss: 5.1994\n",
      "Epoch [34/90], lter [1351/1752] Loss: 3.6472\n",
      "Epoch [34/90], lter [1361/1752] Loss: 2.6483\n",
      "Epoch [34/90], lter [1371/1752] Loss: 4.0291\n",
      "Epoch [34/90], lter [1381/1752] Loss: 3.9786\n",
      "Epoch [34/90], lter [1391/1752] Loss: 5.5094\n",
      "Epoch [34/90], lter [1401/1752] Loss: 3.5384\n",
      "Epoch [34/90], lter [1411/1752] Loss: 3.7823\n",
      "Epoch [34/90], lter [1421/1752] Loss: 4.3399\n",
      "Epoch [34/90], lter [1431/1752] Loss: 3.3348\n",
      "Epoch [34/90], lter [1441/1752] Loss: 3.0327\n",
      "Epoch [34/90], lter [1451/1752] Loss: 3.6524\n",
      "Epoch [34/90], lter [1461/1752] Loss: 5.0438\n",
      "Epoch [34/90], lter [1471/1752] Loss: 5.0103\n",
      "Epoch [34/90], lter [1481/1752] Loss: 3.5126\n",
      "Epoch [34/90], lter [1491/1752] Loss: 1.7372\n",
      "Epoch [34/90], lter [1501/1752] Loss: 3.9659\n",
      "Epoch [34/90], lter [1511/1752] Loss: 2.5826\n",
      "Epoch [34/90], lter [1521/1752] Loss: 5.3860\n",
      "Epoch [34/90], lter [1531/1752] Loss: 3.0721\n",
      "Epoch [34/90], lter [1541/1752] Loss: 2.5837\n",
      "Epoch [34/90], lter [1551/1752] Loss: 2.6611\n",
      "Epoch [34/90], lter [1561/1752] Loss: 2.9767\n",
      "Epoch [34/90], lter [1571/1752] Loss: 2.6213\n",
      "Epoch [34/90], lter [1581/1752] Loss: 2.0832\n",
      "Epoch [34/90], lter [1591/1752] Loss: 3.2570\n",
      "Epoch [34/90], lter [1601/1752] Loss: 2.5965\n",
      "Epoch [34/90], lter [1611/1752] Loss: 3.8224\n",
      "Epoch [34/90], lter [1621/1752] Loss: 1.4987\n",
      "Epoch [34/90], lter [1631/1752] Loss: 4.1240\n",
      "Epoch [34/90], lter [1641/1752] Loss: 4.2714\n",
      "Epoch [34/90], lter [1651/1752] Loss: 4.7395\n",
      "Epoch [34/90], lter [1661/1752] Loss: 2.0138\n",
      "Epoch [34/90], lter [1671/1752] Loss: 3.7148\n",
      "Epoch [34/90], lter [1681/1752] Loss: 3.5366\n",
      "Epoch [34/90], lter [1691/1752] Loss: 3.6458\n",
      "Epoch [34/90], lter [1701/1752] Loss: 2.9994\n",
      "Epoch [34/90], lter [1711/1752] Loss: 2.8882\n",
      "Epoch [34/90], lter [1721/1752] Loss: 2.6441\n",
      "Epoch [34/90], lter [1731/1752] Loss: 2.7883\n",
      "Epoch [34/90], lter [1741/1752] Loss: 3.9150\n",
      "Epoch [34/90], lter [1751/1752] Loss: 3.1526\n",
      "Epoch:  34 | train loss : 3.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 34/90 [70:42:25<15:15:07, 980.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  33 | test loss : 3.0993\n",
      "Epoch [35/90], lter [1/1752] Loss: 5.1149\n",
      "Epoch [35/90], lter [11/1752] Loss: 2.4875\n",
      "Epoch [35/90], lter [21/1752] Loss: 3.5429\n",
      "Epoch [35/90], lter [31/1752] Loss: 2.3489\n",
      "Epoch [35/90], lter [41/1752] Loss: 4.2594\n",
      "Epoch [35/90], lter [51/1752] Loss: 2.9886\n",
      "Epoch [35/90], lter [61/1752] Loss: 2.5260\n",
      "Epoch [35/90], lter [71/1752] Loss: 3.0343\n",
      "Epoch [35/90], lter [81/1752] Loss: 2.4351\n",
      "Epoch [35/90], lter [91/1752] Loss: 2.0132\n",
      "Epoch [35/90], lter [101/1752] Loss: 4.0760\n",
      "Epoch [35/90], lter [111/1752] Loss: 3.1930\n",
      "Epoch [35/90], lter [121/1752] Loss: 2.3748\n",
      "Epoch [35/90], lter [131/1752] Loss: 2.4359\n",
      "Epoch [35/90], lter [141/1752] Loss: 2.9164\n",
      "Epoch [35/90], lter [151/1752] Loss: 3.3981\n",
      "Epoch [35/90], lter [161/1752] Loss: 3.9764\n",
      "Epoch [35/90], lter [171/1752] Loss: 2.5785\n",
      "Epoch [35/90], lter [181/1752] Loss: 3.6164\n",
      "Epoch [35/90], lter [191/1752] Loss: 2.6777\n",
      "Epoch [35/90], lter [201/1752] Loss: 4.1594\n",
      "Epoch [35/90], lter [211/1752] Loss: 2.6367\n",
      "Epoch [35/90], lter [221/1752] Loss: 3.8527\n",
      "Epoch [35/90], lter [231/1752] Loss: 3.2741\n",
      "Epoch [35/90], lter [241/1752] Loss: 4.1108\n",
      "Epoch [35/90], lter [251/1752] Loss: 2.5438\n",
      "Epoch [35/90], lter [261/1752] Loss: 3.0812\n",
      "Epoch [35/90], lter [271/1752] Loss: 4.1288\n",
      "Epoch [35/90], lter [281/1752] Loss: 2.5768\n",
      "Epoch [35/90], lter [291/1752] Loss: 2.0760\n",
      "Epoch [35/90], lter [301/1752] Loss: 2.9881\n",
      "Epoch [35/90], lter [311/1752] Loss: 3.4803\n",
      "Epoch [35/90], lter [321/1752] Loss: 1.8862\n",
      "Epoch [35/90], lter [331/1752] Loss: 3.5352\n",
      "Epoch [35/90], lter [341/1752] Loss: 2.8901\n",
      "Epoch [35/90], lter [351/1752] Loss: 3.0881\n",
      "Epoch [35/90], lter [361/1752] Loss: 3.1296\n",
      "Epoch [35/90], lter [371/1752] Loss: 4.0699\n",
      "Epoch [35/90], lter [381/1752] Loss: 2.3197\n",
      "Epoch [35/90], lter [391/1752] Loss: 3.0281\n",
      "Epoch [35/90], lter [401/1752] Loss: 1.6816\n",
      "Epoch [35/90], lter [411/1752] Loss: 3.8514\n",
      "Epoch [35/90], lter [421/1752] Loss: 2.2486\n",
      "Epoch [35/90], lter [431/1752] Loss: 2.4961\n",
      "Epoch [35/90], lter [441/1752] Loss: 3.0105\n",
      "Epoch [35/90], lter [451/1752] Loss: 3.5165\n",
      "Epoch [35/90], lter [461/1752] Loss: 5.4335\n",
      "Epoch [35/90], lter [471/1752] Loss: 5.8019\n",
      "Epoch [35/90], lter [481/1752] Loss: 2.6843\n",
      "Epoch [35/90], lter [491/1752] Loss: 3.7762\n",
      "Epoch [35/90], lter [501/1752] Loss: 3.3147\n",
      "Epoch [35/90], lter [511/1752] Loss: 2.6444\n",
      "Epoch [35/90], lter [521/1752] Loss: 3.2332\n",
      "Epoch [35/90], lter [531/1752] Loss: 2.5677\n",
      "Epoch [35/90], lter [541/1752] Loss: 3.1892\n",
      "Epoch [35/90], lter [551/1752] Loss: 3.3961\n",
      "Epoch [35/90], lter [561/1752] Loss: 2.6642\n",
      "Epoch [35/90], lter [571/1752] Loss: 3.9390\n",
      "Epoch [35/90], lter [581/1752] Loss: 1.7475\n",
      "Epoch [35/90], lter [591/1752] Loss: 3.1986\n",
      "Epoch [35/90], lter [601/1752] Loss: 3.0411\n",
      "Epoch [35/90], lter [611/1752] Loss: 3.1449\n",
      "Epoch [35/90], lter [621/1752] Loss: 3.7083\n",
      "Epoch [35/90], lter [631/1752] Loss: 4.0477\n",
      "Epoch [35/90], lter [641/1752] Loss: 4.2695\n",
      "Epoch [35/90], lter [651/1752] Loss: 7.6617\n",
      "Epoch [35/90], lter [661/1752] Loss: 3.0896\n",
      "Epoch [35/90], lter [671/1752] Loss: 2.3806\n",
      "Epoch [35/90], lter [681/1752] Loss: 2.3421\n",
      "Epoch [35/90], lter [691/1752] Loss: 3.1349\n",
      "Epoch [35/90], lter [701/1752] Loss: 2.0886\n",
      "Epoch [35/90], lter [711/1752] Loss: 1.7032\n",
      "Epoch [35/90], lter [721/1752] Loss: 4.9046\n",
      "Epoch [35/90], lter [731/1752] Loss: 3.6600\n",
      "Epoch [35/90], lter [741/1752] Loss: 5.7033\n",
      "Epoch [35/90], lter [751/1752] Loss: 3.1779\n",
      "Epoch [35/90], lter [761/1752] Loss: 1.6747\n",
      "Epoch [35/90], lter [771/1752] Loss: 2.3450\n",
      "Epoch [35/90], lter [781/1752] Loss: 3.4710\n",
      "Epoch [35/90], lter [791/1752] Loss: 2.1728\n",
      "Epoch [35/90], lter [801/1752] Loss: 2.4934\n",
      "Epoch [35/90], lter [811/1752] Loss: 2.9048\n",
      "Epoch [35/90], lter [821/1752] Loss: 3.9046\n",
      "Epoch [35/90], lter [831/1752] Loss: 2.8587\n",
      "Epoch [35/90], lter [841/1752] Loss: 2.5664\n",
      "Epoch [35/90], lter [851/1752] Loss: 2.8224\n",
      "Epoch [35/90], lter [861/1752] Loss: 1.7809\n",
      "Epoch [35/90], lter [871/1752] Loss: 5.6111\n",
      "Epoch [35/90], lter [881/1752] Loss: 4.1053\n",
      "Epoch [35/90], lter [891/1752] Loss: 2.1885\n",
      "Epoch [35/90], lter [901/1752] Loss: 4.9962\n",
      "Epoch [35/90], lter [911/1752] Loss: 2.9457\n",
      "Epoch [35/90], lter [921/1752] Loss: 3.4833\n",
      "Epoch [35/90], lter [931/1752] Loss: 2.8818\n",
      "Epoch [35/90], lter [941/1752] Loss: 2.6672\n",
      "Epoch [35/90], lter [951/1752] Loss: 3.7811\n",
      "Epoch [35/90], lter [961/1752] Loss: 3.0531\n",
      "Epoch [35/90], lter [971/1752] Loss: 3.6444\n",
      "Epoch [35/90], lter [981/1752] Loss: 3.9191\n",
      "Epoch [35/90], lter [991/1752] Loss: 2.8182\n",
      "Epoch [35/90], lter [1001/1752] Loss: 2.2848\n",
      "Epoch [35/90], lter [1011/1752] Loss: 2.6703\n",
      "Epoch [35/90], lter [1021/1752] Loss: 3.4060\n",
      "Epoch [35/90], lter [1031/1752] Loss: 2.6521\n",
      "Epoch [35/90], lter [1041/1752] Loss: 2.3860\n",
      "Epoch [35/90], lter [1051/1752] Loss: 2.8259\n",
      "Epoch [35/90], lter [1061/1752] Loss: 4.0227\n",
      "Epoch [35/90], lter [1071/1752] Loss: 3.0155\n",
      "Epoch [35/90], lter [1081/1752] Loss: 3.4235\n",
      "Epoch [35/90], lter [1091/1752] Loss: 3.0566\n",
      "Epoch [35/90], lter [1101/1752] Loss: 2.9818\n",
      "Epoch [35/90], lter [1111/1752] Loss: 3.2947\n",
      "Epoch [35/90], lter [1121/1752] Loss: 3.0240\n",
      "Epoch [35/90], lter [1131/1752] Loss: 3.6484\n",
      "Epoch [35/90], lter [1141/1752] Loss: 3.7174\n",
      "Epoch [35/90], lter [1151/1752] Loss: 2.6987\n",
      "Epoch [35/90], lter [1161/1752] Loss: 3.5007\n",
      "Epoch [35/90], lter [1171/1752] Loss: 1.6284\n",
      "Epoch [35/90], lter [1181/1752] Loss: 3.2508\n",
      "Epoch [35/90], lter [1191/1752] Loss: 3.0988\n",
      "Epoch [35/90], lter [1201/1752] Loss: 2.9984\n",
      "Epoch [35/90], lter [1211/1752] Loss: 4.5869\n",
      "Epoch [35/90], lter [1221/1752] Loss: 2.9341\n",
      "Epoch [35/90], lter [1231/1752] Loss: 3.3473\n",
      "Epoch [35/90], lter [1241/1752] Loss: 3.3370\n",
      "Epoch [35/90], lter [1251/1752] Loss: 3.4722\n",
      "Epoch [35/90], lter [1261/1752] Loss: 2.5155\n",
      "Epoch [35/90], lter [1271/1752] Loss: 2.3030\n",
      "Epoch [35/90], lter [1281/1752] Loss: 2.5871\n",
      "Epoch [35/90], lter [1291/1752] Loss: 2.7951\n",
      "Epoch [35/90], lter [1301/1752] Loss: 2.5388\n",
      "Epoch [35/90], lter [1311/1752] Loss: 2.3928\n",
      "Epoch [35/90], lter [1321/1752] Loss: 2.7397\n",
      "Epoch [35/90], lter [1331/1752] Loss: 2.8441\n",
      "Epoch [35/90], lter [1341/1752] Loss: 3.4049\n",
      "Epoch [35/90], lter [1351/1752] Loss: 5.0670\n",
      "Epoch [35/90], lter [1361/1752] Loss: 4.0549\n",
      "Epoch [35/90], lter [1371/1752] Loss: 2.0890\n",
      "Epoch [35/90], lter [1381/1752] Loss: 2.3993\n",
      "Epoch [35/90], lter [1391/1752] Loss: 2.7956\n",
      "Epoch [35/90], lter [1401/1752] Loss: 2.3764\n",
      "Epoch [35/90], lter [1411/1752] Loss: 2.1996\n",
      "Epoch [35/90], lter [1421/1752] Loss: 3.3552\n",
      "Epoch [35/90], lter [1431/1752] Loss: 2.5195\n",
      "Epoch [35/90], lter [1441/1752] Loss: 2.6654\n",
      "Epoch [35/90], lter [1451/1752] Loss: 1.6873\n",
      "Epoch [35/90], lter [1461/1752] Loss: 3.8963\n",
      "Epoch [35/90], lter [1471/1752] Loss: 4.6828\n",
      "Epoch [35/90], lter [1481/1752] Loss: 3.4882\n",
      "Epoch [35/90], lter [1491/1752] Loss: 2.3737\n",
      "Epoch [35/90], lter [1501/1752] Loss: 4.9960\n",
      "Epoch [35/90], lter [1511/1752] Loss: 3.0718\n",
      "Epoch [35/90], lter [1521/1752] Loss: 3.6450\n",
      "Epoch [35/90], lter [1531/1752] Loss: 1.9452\n",
      "Epoch [35/90], lter [1541/1752] Loss: 2.0603\n",
      "Epoch [35/90], lter [1551/1752] Loss: 3.0465\n",
      "Epoch [35/90], lter [1561/1752] Loss: 2.3954\n",
      "Epoch [35/90], lter [1571/1752] Loss: 3.0880\n",
      "Epoch [35/90], lter [1581/1752] Loss: 3.0911\n",
      "Epoch [35/90], lter [1591/1752] Loss: 2.4055\n",
      "Epoch [35/90], lter [1601/1752] Loss: 3.9670\n",
      "Epoch [35/90], lter [1611/1752] Loss: 2.6602\n",
      "Epoch [35/90], lter [1621/1752] Loss: 2.1330\n",
      "Epoch [35/90], lter [1631/1752] Loss: 3.8429\n",
      "Epoch [35/90], lter [1641/1752] Loss: 3.7404\n",
      "Epoch [35/90], lter [1651/1752] Loss: 2.9759\n",
      "Epoch [35/90], lter [1661/1752] Loss: 3.1937\n",
      "Epoch [35/90], lter [1671/1752] Loss: 2.5593\n",
      "Epoch [35/90], lter [1681/1752] Loss: 2.3770\n",
      "Epoch [35/90], lter [1691/1752] Loss: 3.1668\n",
      "Epoch [35/90], lter [1701/1752] Loss: 1.9254\n",
      "Epoch [35/90], lter [1711/1752] Loss: 2.8912\n",
      "Epoch [35/90], lter [1721/1752] Loss: 2.2670\n",
      "Epoch [35/90], lter [1731/1752] Loss: 2.7227\n",
      "Epoch [35/90], lter [1741/1752] Loss: 1.9386\n",
      "Epoch [35/90], lter [1751/1752] Loss: 3.2719\n",
      "Epoch:  35 | train loss : 3.2119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 35/90 [71:02:45<16:04:40, 1052.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  34 | test loss : 2.1891\n",
      "Epoch [36/90], lter [1/1752] Loss: 3.9035\n",
      "Epoch [36/90], lter [11/1752] Loss: 2.4014\n",
      "Epoch [36/90], lter [21/1752] Loss: 3.0493\n",
      "Epoch [36/90], lter [31/1752] Loss: 3.9373\n",
      "Epoch [36/90], lter [41/1752] Loss: 2.1237\n",
      "Epoch [36/90], lter [51/1752] Loss: 3.9973\n",
      "Epoch [36/90], lter [61/1752] Loss: 4.6750\n",
      "Epoch [36/90], lter [71/1752] Loss: 3.8713\n",
      "Epoch [36/90], lter [81/1752] Loss: 3.1982\n",
      "Epoch [36/90], lter [91/1752] Loss: 2.0917\n",
      "Epoch [36/90], lter [101/1752] Loss: 2.4164\n",
      "Epoch [36/90], lter [111/1752] Loss: 4.1790\n",
      "Epoch [36/90], lter [121/1752] Loss: 4.8455\n",
      "Epoch [36/90], lter [131/1752] Loss: 2.7841\n",
      "Epoch [36/90], lter [141/1752] Loss: 3.5696\n",
      "Epoch [36/90], lter [151/1752] Loss: 2.4367\n",
      "Epoch [36/90], lter [161/1752] Loss: 3.0674\n",
      "Epoch [36/90], lter [171/1752] Loss: 3.1252\n",
      "Epoch [36/90], lter [181/1752] Loss: 3.0515\n",
      "Epoch [36/90], lter [191/1752] Loss: 2.2458\n",
      "Epoch [36/90], lter [201/1752] Loss: 4.8770\n",
      "Epoch [36/90], lter [211/1752] Loss: 2.2396\n",
      "Epoch [36/90], lter [221/1752] Loss: 2.5266\n",
      "Epoch [36/90], lter [231/1752] Loss: 1.7805\n",
      "Epoch [36/90], lter [241/1752] Loss: 2.8721\n",
      "Epoch [36/90], lter [251/1752] Loss: 2.8192\n",
      "Epoch [36/90], lter [261/1752] Loss: 2.9457\n",
      "Epoch [36/90], lter [271/1752] Loss: 3.2291\n",
      "Epoch [36/90], lter [281/1752] Loss: 2.9477\n",
      "Epoch [36/90], lter [291/1752] Loss: 2.9671\n",
      "Epoch [36/90], lter [301/1752] Loss: 3.4879\n",
      "Epoch [36/90], lter [311/1752] Loss: 2.1939\n",
      "Epoch [36/90], lter [321/1752] Loss: 2.9715\n",
      "Epoch [36/90], lter [331/1752] Loss: 2.7786\n",
      "Epoch [36/90], lter [341/1752] Loss: 3.6253\n",
      "Epoch [36/90], lter [351/1752] Loss: 2.5835\n",
      "Epoch [36/90], lter [361/1752] Loss: 2.4986\n",
      "Epoch [36/90], lter [371/1752] Loss: 4.2724\n",
      "Epoch [36/90], lter [381/1752] Loss: 2.7414\n",
      "Epoch [36/90], lter [391/1752] Loss: 3.0642\n",
      "Epoch [36/90], lter [401/1752] Loss: 2.5214\n",
      "Epoch [36/90], lter [411/1752] Loss: 4.0588\n",
      "Epoch [36/90], lter [421/1752] Loss: 2.6939\n",
      "Epoch [36/90], lter [431/1752] Loss: 3.3898\n",
      "Epoch [36/90], lter [441/1752] Loss: 2.8441\n",
      "Epoch [36/90], lter [451/1752] Loss: 3.6145\n",
      "Epoch [36/90], lter [461/1752] Loss: 2.2773\n",
      "Epoch [36/90], lter [471/1752] Loss: 2.8563\n",
      "Epoch [36/90], lter [481/1752] Loss: 2.3420\n",
      "Epoch [36/90], lter [491/1752] Loss: 3.0185\n",
      "Epoch [36/90], lter [501/1752] Loss: 3.0544\n",
      "Epoch [36/90], lter [511/1752] Loss: 2.8848\n",
      "Epoch [36/90], lter [521/1752] Loss: 2.3547\n",
      "Epoch [36/90], lter [531/1752] Loss: 2.8576\n",
      "Epoch [36/90], lter [541/1752] Loss: 2.9029\n",
      "Epoch [36/90], lter [551/1752] Loss: 2.0805\n",
      "Epoch [36/90], lter [561/1752] Loss: 2.7776\n",
      "Epoch [36/90], lter [571/1752] Loss: 3.1587\n",
      "Epoch [36/90], lter [581/1752] Loss: 2.3880\n",
      "Epoch [36/90], lter [591/1752] Loss: 2.9044\n",
      "Epoch [36/90], lter [601/1752] Loss: 2.4191\n",
      "Epoch [36/90], lter [611/1752] Loss: 4.2429\n",
      "Epoch [36/90], lter [621/1752] Loss: 2.5682\n",
      "Epoch [36/90], lter [631/1752] Loss: 3.9873\n",
      "Epoch [36/90], lter [641/1752] Loss: 2.7883\n",
      "Epoch [36/90], lter [651/1752] Loss: 3.2287\n",
      "Epoch [36/90], lter [661/1752] Loss: 2.9800\n",
      "Epoch [36/90], lter [671/1752] Loss: 5.7863\n",
      "Epoch [36/90], lter [681/1752] Loss: 4.0373\n",
      "Epoch [36/90], lter [691/1752] Loss: 3.0092\n",
      "Epoch [36/90], lter [701/1752] Loss: 2.4164\n",
      "Epoch [36/90], lter [711/1752] Loss: 3.6111\n",
      "Epoch [36/90], lter [721/1752] Loss: 2.6844\n",
      "Epoch [36/90], lter [731/1752] Loss: 3.3213\n",
      "Epoch [36/90], lter [741/1752] Loss: 1.6558\n",
      "Epoch [36/90], lter [751/1752] Loss: 2.3721\n",
      "Epoch [36/90], lter [761/1752] Loss: 3.3336\n",
      "Epoch [36/90], lter [771/1752] Loss: 2.6596\n",
      "Epoch [36/90], lter [781/1752] Loss: 3.0695\n",
      "Epoch [36/90], lter [791/1752] Loss: 3.2721\n",
      "Epoch [36/90], lter [801/1752] Loss: 2.8281\n",
      "Epoch [36/90], lter [811/1752] Loss: 3.4815\n",
      "Epoch [36/90], lter [821/1752] Loss: 2.5080\n",
      "Epoch [36/90], lter [831/1752] Loss: 2.2501\n",
      "Epoch [36/90], lter [841/1752] Loss: 2.6153\n",
      "Epoch [36/90], lter [851/1752] Loss: 2.5632\n",
      "Epoch [36/90], lter [861/1752] Loss: 4.0203\n",
      "Epoch [36/90], lter [871/1752] Loss: 2.9431\n",
      "Epoch [36/90], lter [881/1752] Loss: 3.2389\n",
      "Epoch [36/90], lter [891/1752] Loss: 4.1542\n",
      "Epoch [36/90], lter [901/1752] Loss: 3.0218\n",
      "Epoch [36/90], lter [911/1752] Loss: 2.5808\n",
      "Epoch [36/90], lter [921/1752] Loss: 4.2683\n",
      "Epoch [36/90], lter [931/1752] Loss: 3.0718\n",
      "Epoch [36/90], lter [941/1752] Loss: 2.1257\n",
      "Epoch [36/90], lter [951/1752] Loss: 3.5951\n",
      "Epoch [36/90], lter [961/1752] Loss: 2.0297\n",
      "Epoch [36/90], lter [971/1752] Loss: 3.4061\n",
      "Epoch [36/90], lter [981/1752] Loss: 2.4818\n",
      "Epoch [36/90], lter [991/1752] Loss: 3.8197\n",
      "Epoch [36/90], lter [1001/1752] Loss: 3.0943\n",
      "Epoch [36/90], lter [1011/1752] Loss: 2.7355\n",
      "Epoch [36/90], lter [1021/1752] Loss: 2.0500\n",
      "Epoch [36/90], lter [1031/1752] Loss: 2.6551\n",
      "Epoch [36/90], lter [1041/1752] Loss: 1.8156\n",
      "Epoch [36/90], lter [1051/1752] Loss: 3.8136\n",
      "Epoch [36/90], lter [1061/1752] Loss: 2.8995\n",
      "Epoch [36/90], lter [1071/1752] Loss: 2.5982\n",
      "Epoch [36/90], lter [1081/1752] Loss: 3.5940\n",
      "Epoch [36/90], lter [1091/1752] Loss: 2.9371\n",
      "Epoch [36/90], lter [1101/1752] Loss: 2.7694\n",
      "Epoch [36/90], lter [1111/1752] Loss: 4.2697\n",
      "Epoch [36/90], lter [1121/1752] Loss: 2.4104\n",
      "Epoch [36/90], lter [1131/1752] Loss: 3.2455\n",
      "Epoch [36/90], lter [1141/1752] Loss: 3.3185\n",
      "Epoch [36/90], lter [1151/1752] Loss: 3.1969\n",
      "Epoch [36/90], lter [1161/1752] Loss: 3.1834\n",
      "Epoch [36/90], lter [1171/1752] Loss: 1.9982\n",
      "Epoch [36/90], lter [1181/1752] Loss: 3.5340\n",
      "Epoch [36/90], lter [1191/1752] Loss: 2.3190\n",
      "Epoch [36/90], lter [1201/1752] Loss: 2.9119\n",
      "Epoch [36/90], lter [1211/1752] Loss: 3.2277\n",
      "Epoch [36/90], lter [1221/1752] Loss: 2.3531\n",
      "Epoch [36/90], lter [1231/1752] Loss: 3.0636\n",
      "Epoch [36/90], lter [1241/1752] Loss: 2.4521\n",
      "Epoch [36/90], lter [1251/1752] Loss: 1.8240\n",
      "Epoch [36/90], lter [1261/1752] Loss: 4.1869\n",
      "Epoch [36/90], lter [1271/1752] Loss: 3.0408\n",
      "Epoch [36/90], lter [1281/1752] Loss: 3.2924\n",
      "Epoch [36/90], lter [1291/1752] Loss: 3.8250\n",
      "Epoch [36/90], lter [1301/1752] Loss: 1.9342\n",
      "Epoch [36/90], lter [1311/1752] Loss: 3.3864\n",
      "Epoch [36/90], lter [1321/1752] Loss: 1.9179\n",
      "Epoch [36/90], lter [1331/1752] Loss: 4.3481\n",
      "Epoch [36/90], lter [1341/1752] Loss: 2.4980\n",
      "Epoch [36/90], lter [1351/1752] Loss: 3.1379\n",
      "Epoch [36/90], lter [1361/1752] Loss: 3.8073\n",
      "Epoch [36/90], lter [1371/1752] Loss: 3.1088\n",
      "Epoch [36/90], lter [1381/1752] Loss: 2.6236\n",
      "Epoch [36/90], lter [1391/1752] Loss: 2.2169\n",
      "Epoch [36/90], lter [1401/1752] Loss: 4.0254\n",
      "Epoch [36/90], lter [1411/1752] Loss: 3.5433\n",
      "Epoch [36/90], lter [1421/1752] Loss: 2.2572\n",
      "Epoch [36/90], lter [1431/1752] Loss: 2.4246\n",
      "Epoch [36/90], lter [1441/1752] Loss: 1.7275\n",
      "Epoch [36/90], lter [1451/1752] Loss: 3.1461\n",
      "Epoch [36/90], lter [1461/1752] Loss: 3.1868\n",
      "Epoch [36/90], lter [1471/1752] Loss: 2.4538\n",
      "Epoch [36/90], lter [1481/1752] Loss: 2.9212\n",
      "Epoch [36/90], lter [1491/1752] Loss: 3.0067\n",
      "Epoch [36/90], lter [1501/1752] Loss: 4.4635\n",
      "Epoch [36/90], lter [1511/1752] Loss: 4.5907\n",
      "Epoch [36/90], lter [1521/1752] Loss: 1.9818\n",
      "Epoch [36/90], lter [1531/1752] Loss: 3.0198\n",
      "Epoch [36/90], lter [1541/1752] Loss: 3.4132\n",
      "Epoch [36/90], lter [1551/1752] Loss: 3.1859\n",
      "Epoch [36/90], lter [1561/1752] Loss: 4.1362\n",
      "Epoch [36/90], lter [1571/1752] Loss: 3.6361\n",
      "Epoch [36/90], lter [1581/1752] Loss: 2.7166\n",
      "Epoch [36/90], lter [1591/1752] Loss: 2.8137\n",
      "Epoch [36/90], lter [1601/1752] Loss: 2.8305\n",
      "Epoch [36/90], lter [1611/1752] Loss: 2.7977\n",
      "Epoch [36/90], lter [1621/1752] Loss: 2.4157\n",
      "Epoch [36/90], lter [1631/1752] Loss: 2.6255\n",
      "Epoch [36/90], lter [1641/1752] Loss: 1.8870\n",
      "Epoch [36/90], lter [1651/1752] Loss: 2.2004\n",
      "Epoch [36/90], lter [1661/1752] Loss: 3.4070\n",
      "Epoch [36/90], lter [1671/1752] Loss: 2.2911\n",
      "Epoch [36/90], lter [1681/1752] Loss: 3.1244\n",
      "Epoch [36/90], lter [1691/1752] Loss: 3.2502\n",
      "Epoch [36/90], lter [1701/1752] Loss: 2.9609\n",
      "Epoch [36/90], lter [1711/1752] Loss: 2.3726\n",
      "Epoch [36/90], lter [1721/1752] Loss: 3.5056\n",
      "Epoch [36/90], lter [1731/1752] Loss: 3.5626\n",
      "Epoch [36/90], lter [1741/1752] Loss: 2.4767\n",
      "Epoch [36/90], lter [1751/1752] Loss: 2.7519\n",
      "Epoch:  36 | train loss : 2.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 36/90 [71:23:09<16:33:13, 1103.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  35 | test loss : 1.9252\n",
      "Epoch [37/90], lter [1/1752] Loss: 2.6643\n",
      "Epoch [37/90], lter [11/1752] Loss: 3.0692\n",
      "Epoch [37/90], lter [21/1752] Loss: 3.1108\n",
      "Epoch [37/90], lter [31/1752] Loss: 1.9618\n",
      "Epoch [37/90], lter [41/1752] Loss: 2.8503\n",
      "Epoch [37/90], lter [51/1752] Loss: 3.2097\n",
      "Epoch [37/90], lter [61/1752] Loss: 2.2594\n",
      "Epoch [37/90], lter [71/1752] Loss: 3.0894\n",
      "Epoch [37/90], lter [81/1752] Loss: 1.7407\n",
      "Epoch [37/90], lter [91/1752] Loss: 2.7049\n",
      "Epoch [37/90], lter [101/1752] Loss: 2.7788\n",
      "Epoch [37/90], lter [111/1752] Loss: 2.9259\n",
      "Epoch [37/90], lter [121/1752] Loss: 2.3825\n",
      "Epoch [37/90], lter [131/1752] Loss: 2.5944\n",
      "Epoch [37/90], lter [141/1752] Loss: 2.2277\n",
      "Epoch [37/90], lter [151/1752] Loss: 2.7466\n",
      "Epoch [37/90], lter [161/1752] Loss: 2.4239\n",
      "Epoch [37/90], lter [171/1752] Loss: 4.3613\n",
      "Epoch [37/90], lter [181/1752] Loss: 2.5226\n",
      "Epoch [37/90], lter [191/1752] Loss: 2.5077\n",
      "Epoch [37/90], lter [201/1752] Loss: 1.8260\n",
      "Epoch [37/90], lter [211/1752] Loss: 2.4556\n",
      "Epoch [37/90], lter [221/1752] Loss: 3.3220\n",
      "Epoch [37/90], lter [231/1752] Loss: 2.8115\n",
      "Epoch [37/90], lter [241/1752] Loss: 3.6798\n",
      "Epoch [37/90], lter [251/1752] Loss: 2.6153\n",
      "Epoch [37/90], lter [261/1752] Loss: 3.3601\n",
      "Epoch [37/90], lter [271/1752] Loss: 1.9339\n",
      "Epoch [37/90], lter [281/1752] Loss: 3.4215\n",
      "Epoch [37/90], lter [291/1752] Loss: 2.8002\n",
      "Epoch [37/90], lter [301/1752] Loss: 3.4898\n",
      "Epoch [37/90], lter [311/1752] Loss: 2.5092\n",
      "Epoch [37/90], lter [321/1752] Loss: 2.5433\n",
      "Epoch [37/90], lter [331/1752] Loss: 1.8825\n",
      "Epoch [37/90], lter [341/1752] Loss: 2.1372\n",
      "Epoch [37/90], lter [351/1752] Loss: 2.1641\n",
      "Epoch [37/90], lter [361/1752] Loss: 2.6106\n",
      "Epoch [37/90], lter [371/1752] Loss: 2.2779\n",
      "Epoch [37/90], lter [381/1752] Loss: 3.1687\n",
      "Epoch [37/90], lter [391/1752] Loss: 2.3516\n",
      "Epoch [37/90], lter [401/1752] Loss: 1.1572\n",
      "Epoch [37/90], lter [411/1752] Loss: 2.9750\n",
      "Epoch [37/90], lter [421/1752] Loss: 2.5896\n",
      "Epoch [37/90], lter [431/1752] Loss: 3.0758\n",
      "Epoch [37/90], lter [441/1752] Loss: 3.4367\n",
      "Epoch [37/90], lter [451/1752] Loss: 2.7742\n",
      "Epoch [37/90], lter [461/1752] Loss: 2.9108\n",
      "Epoch [37/90], lter [471/1752] Loss: 1.7985\n",
      "Epoch [37/90], lter [481/1752] Loss: 2.3662\n",
      "Epoch [37/90], lter [491/1752] Loss: 2.8435\n",
      "Epoch [37/90], lter [501/1752] Loss: 3.2427\n",
      "Epoch [37/90], lter [511/1752] Loss: 3.7172\n",
      "Epoch [37/90], lter [521/1752] Loss: 2.9359\n",
      "Epoch [37/90], lter [531/1752] Loss: 2.7670\n",
      "Epoch [37/90], lter [541/1752] Loss: 1.6595\n",
      "Epoch [37/90], lter [551/1752] Loss: 2.7940\n",
      "Epoch [37/90], lter [561/1752] Loss: 2.3224\n",
      "Epoch [37/90], lter [571/1752] Loss: 3.6495\n",
      "Epoch [37/90], lter [581/1752] Loss: 2.9198\n",
      "Epoch [37/90], lter [591/1752] Loss: 3.0076\n",
      "Epoch [37/90], lter [601/1752] Loss: 4.1103\n",
      "Epoch [37/90], lter [611/1752] Loss: 1.5966\n",
      "Epoch [37/90], lter [621/1752] Loss: 3.1897\n",
      "Epoch [37/90], lter [631/1752] Loss: 3.2211\n",
      "Epoch [37/90], lter [641/1752] Loss: 2.5877\n",
      "Epoch [37/90], lter [651/1752] Loss: 2.3959\n",
      "Epoch [37/90], lter [661/1752] Loss: 3.0087\n",
      "Epoch [37/90], lter [671/1752] Loss: 3.3238\n",
      "Epoch [37/90], lter [681/1752] Loss: 2.4856\n",
      "Epoch [37/90], lter [691/1752] Loss: 1.7610\n",
      "Epoch [37/90], lter [701/1752] Loss: 2.7245\n",
      "Epoch [37/90], lter [711/1752] Loss: 3.7418\n",
      "Epoch [37/90], lter [721/1752] Loss: 2.5203\n",
      "Epoch [37/90], lter [731/1752] Loss: 3.9723\n",
      "Epoch [37/90], lter [741/1752] Loss: 3.3759\n",
      "Epoch [37/90], lter [751/1752] Loss: 2.3671\n",
      "Epoch [37/90], lter [761/1752] Loss: 1.8388\n",
      "Epoch [37/90], lter [771/1752] Loss: 3.8898\n",
      "Epoch [37/90], lter [781/1752] Loss: 2.9552\n",
      "Epoch [37/90], lter [791/1752] Loss: 3.3303\n",
      "Epoch [37/90], lter [801/1752] Loss: 3.2311\n",
      "Epoch [37/90], lter [811/1752] Loss: 3.2652\n",
      "Epoch [37/90], lter [821/1752] Loss: 2.5516\n",
      "Epoch [37/90], lter [831/1752] Loss: 2.9629\n",
      "Epoch [37/90], lter [841/1752] Loss: 2.9643\n",
      "Epoch [37/90], lter [851/1752] Loss: 2.5368\n",
      "Epoch [37/90], lter [861/1752] Loss: 3.4711\n",
      "Epoch [37/90], lter [871/1752] Loss: 2.1547\n",
      "Epoch [37/90], lter [881/1752] Loss: 2.6212\n",
      "Epoch [37/90], lter [891/1752] Loss: 2.6548\n",
      "Epoch [37/90], lter [901/1752] Loss: 1.5818\n",
      "Epoch [37/90], lter [911/1752] Loss: 2.4696\n",
      "Epoch [37/90], lter [921/1752] Loss: 3.0196\n",
      "Epoch [37/90], lter [931/1752] Loss: 3.1640\n",
      "Epoch [37/90], lter [941/1752] Loss: 2.7523\n",
      "Epoch [37/90], lter [951/1752] Loss: 2.0338\n",
      "Epoch [37/90], lter [961/1752] Loss: 2.3936\n",
      "Epoch [37/90], lter [971/1752] Loss: 2.6370\n",
      "Epoch [37/90], lter [981/1752] Loss: 2.5355\n",
      "Epoch [37/90], lter [991/1752] Loss: 4.3438\n",
      "Epoch [37/90], lter [1001/1752] Loss: 2.1393\n",
      "Epoch [37/90], lter [1011/1752] Loss: 2.3996\n",
      "Epoch [37/90], lter [1021/1752] Loss: 1.8306\n",
      "Epoch [37/90], lter [1031/1752] Loss: 3.5174\n",
      "Epoch [37/90], lter [1041/1752] Loss: 3.1428\n",
      "Epoch [37/90], lter [1051/1752] Loss: 2.4786\n",
      "Epoch [37/90], lter [1061/1752] Loss: 2.3736\n",
      "Epoch [37/90], lter [1071/1752] Loss: 3.3224\n",
      "Epoch [37/90], lter [1081/1752] Loss: 2.0309\n",
      "Epoch [37/90], lter [1091/1752] Loss: 2.7165\n",
      "Epoch [37/90], lter [1101/1752] Loss: 2.9063\n",
      "Epoch [37/90], lter [1111/1752] Loss: 3.4454\n",
      "Epoch [37/90], lter [1121/1752] Loss: 2.0897\n",
      "Epoch [37/90], lter [1131/1752] Loss: 1.9360\n",
      "Epoch [37/90], lter [1141/1752] Loss: 2.5449\n",
      "Epoch [37/90], lter [1151/1752] Loss: 3.1063\n",
      "Epoch [37/90], lter [1161/1752] Loss: 1.9932\n",
      "Epoch [37/90], lter [1171/1752] Loss: 2.9046\n",
      "Epoch [37/90], lter [1181/1752] Loss: 1.9832\n",
      "Epoch [37/90], lter [1191/1752] Loss: 4.8362\n",
      "Epoch [37/90], lter [1201/1752] Loss: 2.9497\n",
      "Epoch [37/90], lter [1211/1752] Loss: 3.6551\n",
      "Epoch [37/90], lter [1221/1752] Loss: 1.9919\n",
      "Epoch [37/90], lter [1231/1752] Loss: 3.2096\n",
      "Epoch [37/90], lter [1241/1752] Loss: 4.4868\n",
      "Epoch [37/90], lter [1251/1752] Loss: 3.8993\n",
      "Epoch [37/90], lter [1261/1752] Loss: 2.9935\n",
      "Epoch [37/90], lter [1271/1752] Loss: 2.5567\n",
      "Epoch [37/90], lter [1281/1752] Loss: 2.5711\n",
      "Epoch [37/90], lter [1291/1752] Loss: 2.9279\n",
      "Epoch [37/90], lter [1301/1752] Loss: 2.1136\n",
      "Epoch [37/90], lter [1311/1752] Loss: 2.6546\n",
      "Epoch [37/90], lter [1321/1752] Loss: 2.3827\n",
      "Epoch [37/90], lter [1331/1752] Loss: 3.1167\n",
      "Epoch [37/90], lter [1341/1752] Loss: 1.7197\n",
      "Epoch [37/90], lter [1351/1752] Loss: 3.0866\n",
      "Epoch [37/90], lter [1361/1752] Loss: 2.9564\n",
      "Epoch [37/90], lter [1371/1752] Loss: 3.6514\n",
      "Epoch [37/90], lter [1381/1752] Loss: 3.4407\n",
      "Epoch [37/90], lter [1391/1752] Loss: 3.8865\n",
      "Epoch [37/90], lter [1401/1752] Loss: 3.0887\n",
      "Epoch [37/90], lter [1411/1752] Loss: 2.5961\n",
      "Epoch [37/90], lter [1421/1752] Loss: 2.6933\n",
      "Epoch [37/90], lter [1431/1752] Loss: 2.9975\n",
      "Epoch [37/90], lter [1441/1752] Loss: 3.1239\n",
      "Epoch [37/90], lter [1451/1752] Loss: 2.6050\n",
      "Epoch [37/90], lter [1461/1752] Loss: 1.7832\n",
      "Epoch [37/90], lter [1471/1752] Loss: 2.9477\n",
      "Epoch [37/90], lter [1481/1752] Loss: 1.4338\n",
      "Epoch [37/90], lter [1491/1752] Loss: 3.1220\n",
      "Epoch [37/90], lter [1501/1752] Loss: 1.8252\n",
      "Epoch [37/90], lter [1511/1752] Loss: 2.8362\n",
      "Epoch [37/90], lter [1521/1752] Loss: 2.3644\n",
      "Epoch [37/90], lter [1531/1752] Loss: 2.6776\n",
      "Epoch [37/90], lter [1541/1752] Loss: 3.2821\n",
      "Epoch [37/90], lter [1551/1752] Loss: 3.2110\n",
      "Epoch [37/90], lter [1561/1752] Loss: 2.2993\n",
      "Epoch [37/90], lter [1571/1752] Loss: 3.7236\n",
      "Epoch [37/90], lter [1581/1752] Loss: 2.6609\n",
      "Epoch [37/90], lter [1591/1752] Loss: 1.9420\n",
      "Epoch [37/90], lter [1601/1752] Loss: 3.0789\n",
      "Epoch [37/90], lter [1611/1752] Loss: 2.7400\n",
      "Epoch [37/90], lter [1621/1752] Loss: 4.6818\n",
      "Epoch [37/90], lter [1631/1752] Loss: 2.7657\n",
      "Epoch [37/90], lter [1641/1752] Loss: 4.3529\n",
      "Epoch [37/90], lter [1651/1752] Loss: 3.3951\n",
      "Epoch [37/90], lter [1661/1752] Loss: 1.2816\n",
      "Epoch [37/90], lter [1671/1752] Loss: 2.9185\n",
      "Epoch [37/90], lter [1681/1752] Loss: 2.0153\n",
      "Epoch [37/90], lter [1691/1752] Loss: 2.2794\n",
      "Epoch [37/90], lter [1701/1752] Loss: 2.7926\n",
      "Epoch [37/90], lter [1711/1752] Loss: 1.9194\n",
      "Epoch [37/90], lter [1721/1752] Loss: 2.5613\n",
      "Epoch [37/90], lter [1731/1752] Loss: 3.3368\n",
      "Epoch [37/90], lter [1741/1752] Loss: 2.9011\n",
      "Epoch [37/90], lter [1751/1752] Loss: 3.1743\n",
      "Epoch:  37 | train loss : 2.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 37/90 [71:43:28<16:45:30, 1138.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  36 | test loss : 2.1522\n",
      "Epoch [38/90], lter [1/1752] Loss: 3.1321\n",
      "Epoch [38/90], lter [11/1752] Loss: 2.6342\n",
      "Epoch [38/90], lter [21/1752] Loss: 2.9721\n",
      "Epoch [38/90], lter [31/1752] Loss: 2.0365\n",
      "Epoch [38/90], lter [41/1752] Loss: 3.8425\n",
      "Epoch [38/90], lter [51/1752] Loss: 1.8607\n",
      "Epoch [38/90], lter [61/1752] Loss: 2.8703\n",
      "Epoch [38/90], lter [71/1752] Loss: 3.4377\n",
      "Epoch [38/90], lter [81/1752] Loss: 2.8984\n",
      "Epoch [38/90], lter [91/1752] Loss: 2.5985\n",
      "Epoch [38/90], lter [101/1752] Loss: 3.2057\n",
      "Epoch [38/90], lter [111/1752] Loss: 2.7577\n",
      "Epoch [38/90], lter [121/1752] Loss: 2.9531\n",
      "Epoch [38/90], lter [131/1752] Loss: 2.1246\n",
      "Epoch [38/90], lter [141/1752] Loss: 2.6520\n",
      "Epoch [38/90], lter [151/1752] Loss: 3.3533\n",
      "Epoch [38/90], lter [161/1752] Loss: 2.7634\n",
      "Epoch [38/90], lter [171/1752] Loss: 2.3109\n",
      "Epoch [38/90], lter [181/1752] Loss: 2.7926\n",
      "Epoch [38/90], lter [191/1752] Loss: 3.1782\n",
      "Epoch [38/90], lter [201/1752] Loss: 2.6464\n",
      "Epoch [38/90], lter [211/1752] Loss: 2.8396\n",
      "Epoch [38/90], lter [221/1752] Loss: 2.5727\n",
      "Epoch [38/90], lter [231/1752] Loss: 3.0507\n",
      "Epoch [38/90], lter [241/1752] Loss: 2.6871\n",
      "Epoch [38/90], lter [251/1752] Loss: 2.0137\n",
      "Epoch [38/90], lter [261/1752] Loss: 2.6372\n",
      "Epoch [38/90], lter [271/1752] Loss: 2.8664\n",
      "Epoch [38/90], lter [281/1752] Loss: 1.9044\n",
      "Epoch [38/90], lter [291/1752] Loss: 2.8642\n",
      "Epoch [38/90], lter [301/1752] Loss: 1.6892\n",
      "Epoch [38/90], lter [311/1752] Loss: 1.7264\n",
      "Epoch [38/90], lter [321/1752] Loss: 4.4512\n",
      "Epoch [38/90], lter [331/1752] Loss: 2.9186\n",
      "Epoch [38/90], lter [341/1752] Loss: 1.5019\n",
      "Epoch [38/90], lter [351/1752] Loss: 2.8673\n",
      "Epoch [38/90], lter [361/1752] Loss: 2.3799\n",
      "Epoch [38/90], lter [371/1752] Loss: 1.8965\n",
      "Epoch [38/90], lter [381/1752] Loss: 2.7112\n",
      "Epoch [38/90], lter [391/1752] Loss: 3.1148\n",
      "Epoch [38/90], lter [401/1752] Loss: 3.3372\n",
      "Epoch [38/90], lter [411/1752] Loss: 3.2700\n",
      "Epoch [38/90], lter [421/1752] Loss: 1.8537\n",
      "Epoch [38/90], lter [431/1752] Loss: 1.1487\n",
      "Epoch [38/90], lter [441/1752] Loss: 1.8731\n",
      "Epoch [38/90], lter [451/1752] Loss: 2.4993\n",
      "Epoch [38/90], lter [461/1752] Loss: 2.6606\n",
      "Epoch [38/90], lter [471/1752] Loss: 3.3641\n",
      "Epoch [38/90], lter [481/1752] Loss: 2.9405\n",
      "Epoch [38/90], lter [491/1752] Loss: 3.3837\n",
      "Epoch [38/90], lter [501/1752] Loss: 4.1618\n",
      "Epoch [38/90], lter [511/1752] Loss: 2.4354\n",
      "Epoch [38/90], lter [521/1752] Loss: 3.0441\n",
      "Epoch [38/90], lter [531/1752] Loss: 3.1619\n",
      "Epoch [38/90], lter [541/1752] Loss: 2.4696\n",
      "Epoch [38/90], lter [551/1752] Loss: 3.3785\n",
      "Epoch [38/90], lter [561/1752] Loss: 3.1491\n",
      "Epoch [38/90], lter [571/1752] Loss: 3.1014\n",
      "Epoch [38/90], lter [581/1752] Loss: 3.2035\n",
      "Epoch [38/90], lter [591/1752] Loss: 2.8558\n",
      "Epoch [38/90], lter [601/1752] Loss: 3.7045\n",
      "Epoch [38/90], lter [611/1752] Loss: 3.4532\n",
      "Epoch [38/90], lter [621/1752] Loss: 3.3655\n",
      "Epoch [38/90], lter [631/1752] Loss: 2.0056\n",
      "Epoch [38/90], lter [641/1752] Loss: 2.9245\n",
      "Epoch [38/90], lter [651/1752] Loss: 3.1810\n",
      "Epoch [38/90], lter [661/1752] Loss: 1.7202\n",
      "Epoch [38/90], lter [671/1752] Loss: 2.9316\n",
      "Epoch [38/90], lter [681/1752] Loss: 2.7668\n",
      "Epoch [38/90], lter [691/1752] Loss: 2.2072\n",
      "Epoch [38/90], lter [701/1752] Loss: 2.2011\n",
      "Epoch [38/90], lter [711/1752] Loss: 2.3278\n",
      "Epoch [38/90], lter [721/1752] Loss: 3.6054\n",
      "Epoch [38/90], lter [731/1752] Loss: 3.4313\n",
      "Epoch [38/90], lter [741/1752] Loss: 3.1758\n",
      "Epoch [38/90], lter [751/1752] Loss: 4.0580\n",
      "Epoch [38/90], lter [761/1752] Loss: 3.2696\n",
      "Epoch [38/90], lter [771/1752] Loss: 3.7064\n",
      "Epoch [38/90], lter [781/1752] Loss: 2.2647\n",
      "Epoch [38/90], lter [791/1752] Loss: 1.6875\n",
      "Epoch [38/90], lter [801/1752] Loss: 3.6356\n",
      "Epoch [38/90], lter [811/1752] Loss: 2.7458\n",
      "Epoch [38/90], lter [821/1752] Loss: 2.2892\n",
      "Epoch [38/90], lter [831/1752] Loss: 2.3877\n",
      "Epoch [38/90], lter [841/1752] Loss: 2.7018\n",
      "Epoch [38/90], lter [851/1752] Loss: 2.1548\n",
      "Epoch [38/90], lter [861/1752] Loss: 2.6396\n",
      "Epoch [38/90], lter [871/1752] Loss: 3.1942\n",
      "Epoch [38/90], lter [881/1752] Loss: 2.5926\n",
      "Epoch [38/90], lter [891/1752] Loss: 2.2367\n",
      "Epoch [38/90], lter [901/1752] Loss: 4.7163\n",
      "Epoch [38/90], lter [911/1752] Loss: 2.5592\n",
      "Epoch [38/90], lter [921/1752] Loss: 3.1046\n",
      "Epoch [38/90], lter [931/1752] Loss: 4.2055\n",
      "Epoch [38/90], lter [941/1752] Loss: 2.7186\n",
      "Epoch [38/90], lter [951/1752] Loss: 2.8291\n",
      "Epoch [38/90], lter [961/1752] Loss: 2.4212\n",
      "Epoch [38/90], lter [971/1752] Loss: 3.7883\n",
      "Epoch [38/90], lter [981/1752] Loss: 2.8637\n",
      "Epoch [38/90], lter [991/1752] Loss: 2.3795\n",
      "Epoch [38/90], lter [1001/1752] Loss: 3.6887\n",
      "Epoch [38/90], lter [1011/1752] Loss: 3.0613\n",
      "Epoch [38/90], lter [1021/1752] Loss: 3.0271\n",
      "Epoch [38/90], lter [1031/1752] Loss: 3.1921\n",
      "Epoch [38/90], lter [1041/1752] Loss: 3.5736\n",
      "Epoch [38/90], lter [1051/1752] Loss: 3.6707\n",
      "Epoch [38/90], lter [1061/1752] Loss: 3.7455\n",
      "Epoch [38/90], lter [1071/1752] Loss: 4.0537\n",
      "Epoch [38/90], lter [1081/1752] Loss: 4.5625\n",
      "Epoch [38/90], lter [1091/1752] Loss: 1.5053\n",
      "Epoch [38/90], lter [1101/1752] Loss: 2.5740\n",
      "Epoch [38/90], lter [1111/1752] Loss: 1.8994\n",
      "Epoch [38/90], lter [1121/1752] Loss: 2.9310\n",
      "Epoch [38/90], lter [1131/1752] Loss: 3.8230\n",
      "Epoch [38/90], lter [1141/1752] Loss: 3.6685\n",
      "Epoch [38/90], lter [1151/1752] Loss: 3.7441\n",
      "Epoch [38/90], lter [1161/1752] Loss: 1.4319\n",
      "Epoch [38/90], lter [1171/1752] Loss: 2.2527\n",
      "Epoch [38/90], lter [1181/1752] Loss: 1.8468\n",
      "Epoch [38/90], lter [1191/1752] Loss: 2.9384\n",
      "Epoch [38/90], lter [1201/1752] Loss: 2.8058\n",
      "Epoch [38/90], lter [1211/1752] Loss: 2.4187\n",
      "Epoch [38/90], lter [1221/1752] Loss: 2.1831\n",
      "Epoch [38/90], lter [1231/1752] Loss: 3.0894\n",
      "Epoch [38/90], lter [1241/1752] Loss: 2.6923\n",
      "Epoch [38/90], lter [1251/1752] Loss: 2.7238\n",
      "Epoch [38/90], lter [1261/1752] Loss: 2.9723\n",
      "Epoch [38/90], lter [1271/1752] Loss: 3.4341\n",
      "Epoch [38/90], lter [1281/1752] Loss: 3.0183\n",
      "Epoch [38/90], lter [1291/1752] Loss: 1.9852\n",
      "Epoch [38/90], lter [1301/1752] Loss: 3.1445\n",
      "Epoch [38/90], lter [1311/1752] Loss: 4.0448\n",
      "Epoch [38/90], lter [1321/1752] Loss: 2.9416\n",
      "Epoch [38/90], lter [1331/1752] Loss: 3.1760\n",
      "Epoch [38/90], lter [1341/1752] Loss: 3.3823\n",
      "Epoch [38/90], lter [1351/1752] Loss: 3.5695\n",
      "Epoch [38/90], lter [1361/1752] Loss: 2.9269\n",
      "Epoch [38/90], lter [1371/1752] Loss: 2.8629\n",
      "Epoch [38/90], lter [1381/1752] Loss: 4.2751\n",
      "Epoch [38/90], lter [1391/1752] Loss: 2.3385\n",
      "Epoch [38/90], lter [1401/1752] Loss: 4.2107\n",
      "Epoch [38/90], lter [1411/1752] Loss: 2.6204\n",
      "Epoch [38/90], lter [1421/1752] Loss: 3.0197\n",
      "Epoch [38/90], lter [1431/1752] Loss: 3.2880\n",
      "Epoch [38/90], lter [1441/1752] Loss: 3.3478\n",
      "Epoch [38/90], lter [1451/1752] Loss: 2.7900\n",
      "Epoch [38/90], lter [1461/1752] Loss: 2.9436\n",
      "Epoch [38/90], lter [1661/1752] Loss: 4.0833\n",
      "Epoch [38/90], lter [1671/1752] Loss: 1.9360\n",
      "Epoch [38/90], lter [1681/1752] Loss: 3.2317\n",
      "Epoch [38/90], lter [1691/1752] Loss: 3.3831\n",
      "Epoch [38/90], lter [1701/1752] Loss: 2.9572\n",
      "Epoch [38/90], lter [1711/1752] Loss: 2.8877\n",
      "Epoch [38/90], lter [1721/1752] Loss: 2.5681\n",
      "Epoch [38/90], lter [1731/1752] Loss: 2.9047\n",
      "Epoch [38/90], lter [1741/1752] Loss: 3.7981\n",
      "Epoch [38/90], lter [1751/1752] Loss: 2.3795\n",
      "Epoch:  38 | train loss : 2.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 38/90 [72:03:55<16:49:45, 1165.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  37 | test loss : 3.1033\n",
      "Epoch [39/90], lter [1/1752] Loss: 2.5235\n",
      "Epoch [39/90], lter [11/1752] Loss: 2.9727\n",
      "Epoch [39/90], lter [21/1752] Loss: 2.7275\n",
      "Epoch [39/90], lter [31/1752] Loss: 1.9715\n",
      "Epoch [39/90], lter [41/1752] Loss: 2.5208\n",
      "Epoch [39/90], lter [51/1752] Loss: 2.4712\n",
      "Epoch [39/90], lter [61/1752] Loss: 2.3537\n",
      "Epoch [39/90], lter [71/1752] Loss: 2.2681\n",
      "Epoch [39/90], lter [81/1752] Loss: 2.3197\n",
      "Epoch [39/90], lter [91/1752] Loss: 2.5279\n",
      "Epoch [39/90], lter [101/1752] Loss: 4.0730\n",
      "Epoch [39/90], lter [111/1752] Loss: 1.8743\n",
      "Epoch [39/90], lter [121/1752] Loss: 3.3986\n",
      "Epoch [39/90], lter [131/1752] Loss: 2.5107\n",
      "Epoch [39/90], lter [141/1752] Loss: 2.8659\n",
      "Epoch [39/90], lter [151/1752] Loss: 3.5441\n",
      "Epoch [39/90], lter [161/1752] Loss: 3.6815\n",
      "Epoch [39/90], lter [171/1752] Loss: 3.2036\n",
      "Epoch [39/90], lter [181/1752] Loss: 4.8534\n",
      "Epoch [39/90], lter [191/1752] Loss: 1.9151\n",
      "Epoch [39/90], lter [201/1752] Loss: 2.3656\n",
      "Epoch [39/90], lter [211/1752] Loss: 2.7672\n",
      "Epoch [39/90], lter [221/1752] Loss: 2.2688\n",
      "Epoch [39/90], lter [231/1752] Loss: 2.9190\n",
      "Epoch [39/90], lter [241/1752] Loss: 3.5606\n",
      "Epoch [39/90], lter [251/1752] Loss: 1.8119\n",
      "Epoch [39/90], lter [261/1752] Loss: 3.7479\n",
      "Epoch [39/90], lter [271/1752] Loss: 2.8872\n",
      "Epoch [39/90], lter [281/1752] Loss: 2.4489\n",
      "Epoch [39/90], lter [291/1752] Loss: 2.3996\n",
      "Epoch [39/90], lter [301/1752] Loss: 3.2240\n",
      "Epoch [39/90], lter [311/1752] Loss: 3.3065\n",
      "Epoch [39/90], lter [321/1752] Loss: 3.5013\n",
      "Epoch [39/90], lter [331/1752] Loss: 3.5173\n",
      "Epoch [39/90], lter [341/1752] Loss: 2.4416\n",
      "Epoch [39/90], lter [351/1752] Loss: 2.5900\n",
      "Epoch [39/90], lter [361/1752] Loss: 2.9437\n",
      "Epoch [39/90], lter [371/1752] Loss: 2.1266\n",
      "Epoch [39/90], lter [381/1752] Loss: 2.8200\n",
      "Epoch [39/90], lter [391/1752] Loss: 1.7848\n",
      "Epoch [39/90], lter [401/1752] Loss: 3.7837\n",
      "Epoch [39/90], lter [411/1752] Loss: 2.8073\n",
      "Epoch [39/90], lter [421/1752] Loss: 2.5968\n",
      "Epoch [39/90], lter [431/1752] Loss: 2.3817\n",
      "Epoch [39/90], lter [441/1752] Loss: 1.7786\n",
      "Epoch [39/90], lter [451/1752] Loss: 3.1257\n",
      "Epoch [39/90], lter [461/1752] Loss: 3.6909\n",
      "Epoch [39/90], lter [471/1752] Loss: 3.4464\n",
      "Epoch [39/90], lter [481/1752] Loss: 2.0644\n",
      "Epoch [39/90], lter [491/1752] Loss: 3.5645\n",
      "Epoch [39/90], lter [501/1752] Loss: 3.8509\n",
      "Epoch [39/90], lter [511/1752] Loss: 3.1789\n",
      "Epoch [39/90], lter [521/1752] Loss: 2.2002\n",
      "Epoch [39/90], lter [531/1752] Loss: 2.5622\n",
      "Epoch [39/90], lter [541/1752] Loss: 1.9544\n",
      "Epoch [39/90], lter [551/1752] Loss: 4.1198\n",
      "Epoch [39/90], lter [561/1752] Loss: 2.7516\n",
      "Epoch [39/90], lter [571/1752] Loss: 3.1588\n",
      "Epoch [39/90], lter [581/1752] Loss: 2.1361\n",
      "Epoch [39/90], lter [591/1752] Loss: 3.1717\n",
      "Epoch [39/90], lter [601/1752] Loss: 2.2109\n",
      "Epoch [39/90], lter [611/1752] Loss: 3.1972\n",
      "Epoch [39/90], lter [621/1752] Loss: 3.3998\n",
      "Epoch [39/90], lter [631/1752] Loss: 3.5882\n",
      "Epoch [39/90], lter [641/1752] Loss: 3.6350\n",
      "Epoch [39/90], lter [651/1752] Loss: 2.2007\n",
      "Epoch [39/90], lter [661/1752] Loss: 3.1341\n",
      "Epoch [39/90], lter [671/1752] Loss: 1.8955\n",
      "Epoch [39/90], lter [681/1752] Loss: 1.0461\n",
      "Epoch [39/90], lter [691/1752] Loss: 2.7621\n",
      "Epoch [39/90], lter [701/1752] Loss: 3.9185\n",
      "Epoch [39/90], lter [711/1752] Loss: 2.7129\n",
      "Epoch [39/90], lter [721/1752] Loss: 1.7668\n",
      "Epoch [39/90], lter [731/1752] Loss: 2.6501\n",
      "Epoch [39/90], lter [741/1752] Loss: 2.7939\n",
      "Epoch [39/90], lter [751/1752] Loss: 2.6380\n",
      "Epoch [39/90], lter [761/1752] Loss: 1.9966\n",
      "Epoch [39/90], lter [771/1752] Loss: 2.8589\n",
      "Epoch [39/90], lter [781/1752] Loss: 1.6008\n",
      "Epoch [39/90], lter [791/1752] Loss: 2.6938\n",
      "Epoch [39/90], lter [801/1752] Loss: 2.1062\n",
      "Epoch [39/90], lter [811/1752] Loss: 2.6500\n",
      "Epoch [39/90], lter [821/1752] Loss: 3.1112\n",
      "Epoch [39/90], lter [831/1752] Loss: 4.4566\n",
      "Epoch [39/90], lter [841/1752] Loss: 2.5428\n",
      "Epoch [39/90], lter [851/1752] Loss: 2.9025\n",
      "Epoch [39/90], lter [861/1752] Loss: 2.8270\n",
      "Epoch [39/90], lter [871/1752] Loss: 2.7414\n",
      "Epoch [39/90], lter [881/1752] Loss: 5.3492\n",
      "Epoch [39/90], lter [891/1752] Loss: 2.4109\n",
      "Epoch [39/90], lter [901/1752] Loss: 3.0459\n",
      "Epoch [39/90], lter [911/1752] Loss: 3.0705\n",
      "Epoch [39/90], lter [921/1752] Loss: 2.8298\n",
      "Epoch [39/90], lter [931/1752] Loss: 2.2875\n",
      "Epoch [39/90], lter [941/1752] Loss: 2.6814\n",
      "Epoch [39/90], lter [951/1752] Loss: 2.4366\n",
      "Epoch [39/90], lter [961/1752] Loss: 2.2357\n",
      "Epoch [39/90], lter [971/1752] Loss: 2.1743\n",
      "Epoch [39/90], lter [981/1752] Loss: 2.4044\n",
      "Epoch [39/90], lter [991/1752] Loss: 2.4121\n",
      "Epoch [39/90], lter [1001/1752] Loss: 2.2680\n",
      "Epoch [39/90], lter [1011/1752] Loss: 1.7176\n",
      "Epoch [39/90], lter [1021/1752] Loss: 2.9599\n",
      "Epoch [39/90], lter [1031/1752] Loss: 2.1270\n",
      "Epoch [39/90], lter [1041/1752] Loss: 1.7508\n",
      "Epoch [39/90], lter [1051/1752] Loss: 2.1599\n",
      "Epoch [39/90], lter [1061/1752] Loss: 3.3300\n",
      "Epoch [39/90], lter [1071/1752] Loss: 1.7871\n",
      "Epoch [39/90], lter [1081/1752] Loss: 3.0207\n",
      "Epoch [39/90], lter [1091/1752] Loss: 2.6994\n",
      "Epoch [39/90], lter [1101/1752] Loss: 2.2346\n",
      "Epoch [39/90], lter [1111/1752] Loss: 4.2208\n",
      "Epoch [39/90], lter [1121/1752] Loss: 4.2350\n",
      "Epoch [39/90], lter [1131/1752] Loss: 3.3276\n",
      "Epoch [39/90], lter [1141/1752] Loss: 4.5112\n",
      "Epoch [39/90], lter [1151/1752] Loss: 5.2515\n",
      "Epoch [39/90], lter [1161/1752] Loss: 2.3833\n",
      "Epoch [39/90], lter [1171/1752] Loss: 2.4520\n",
      "Epoch [39/90], lter [1181/1752] Loss: 3.1228\n",
      "Epoch [39/90], lter [1191/1752] Loss: 1.6883\n",
      "Epoch [39/90], lter [1201/1752] Loss: 3.6829\n",
      "Epoch [39/90], lter [1211/1752] Loss: 3.0011\n",
      "Epoch [39/90], lter [1221/1752] Loss: 2.3269\n",
      "Epoch [39/90], lter [1231/1752] Loss: 2.5009\n",
      "Epoch [39/90], lter [1241/1752] Loss: 3.1006\n",
      "Epoch [39/90], lter [1251/1752] Loss: 2.2772\n",
      "Epoch [39/90], lter [1261/1752] Loss: 2.7795\n",
      "Epoch [39/90], lter [1271/1752] Loss: 3.4073\n",
      "Epoch [39/90], lter [1281/1752] Loss: 2.4676\n",
      "Epoch [39/90], lter [1291/1752] Loss: 3.3378\n",
      "Epoch [39/90], lter [1301/1752] Loss: 2.4702\n",
      "Epoch [39/90], lter [1311/1752] Loss: 3.5790\n",
      "Epoch [39/90], lter [1321/1752] Loss: 2.7017\n",
      "Epoch [39/90], lter [1331/1752] Loss: 3.7207\n",
      "Epoch [39/90], lter [1341/1752] Loss: 2.2150\n",
      "Epoch [39/90], lter [1351/1752] Loss: 2.1403\n",
      "Epoch [39/90], lter [1361/1752] Loss: 4.6789\n",
      "Epoch [39/90], lter [1371/1752] Loss: 4.7314\n",
      "Epoch [39/90], lter [1381/1752] Loss: 2.2957\n",
      "Epoch [39/90], lter [1391/1752] Loss: 1.8550\n",
      "Epoch [39/90], lter [1401/1752] Loss: 3.1782\n",
      "Epoch [39/90], lter [1411/1752] Loss: 2.5474\n",
      "Epoch [39/90], lter [1421/1752] Loss: 2.5498\n",
      "Epoch [39/90], lter [1431/1752] Loss: 3.8813\n",
      "Epoch [39/90], lter [1441/1752] Loss: 3.5050\n",
      "Epoch [39/90], lter [1451/1752] Loss: 3.4953\n",
      "Epoch [39/90], lter [1461/1752] Loss: 2.0115\n",
      "Epoch [39/90], lter [1471/1752] Loss: 3.0885\n",
      "Epoch [39/90], lter [1481/1752] Loss: 2.8557\n",
      "Epoch [39/90], lter [1491/1752] Loss: 1.9780\n",
      "Epoch [39/90], lter [1501/1752] Loss: 3.0665\n",
      "Epoch [39/90], lter [1511/1752] Loss: 2.5309\n",
      "Epoch [39/90], lter [1521/1752] Loss: 1.8565\n",
      "Epoch [39/90], lter [1531/1752] Loss: 2.5519\n",
      "Epoch [39/90], lter [1541/1752] Loss: 2.8790\n",
      "Epoch [39/90], lter [1551/1752] Loss: 3.0670\n",
      "Epoch [39/90], lter [1561/1752] Loss: 3.6131\n",
      "Epoch [39/90], lter [1571/1752] Loss: 3.3180\n",
      "Epoch [39/90], lter [1581/1752] Loss: 2.5257\n",
      "Epoch [39/90], lter [1591/1752] Loss: 3.8557\n",
      "Epoch [39/90], lter [1601/1752] Loss: 3.2082\n",
      "Epoch [39/90], lter [1611/1752] Loss: 2.4399\n",
      "Epoch [39/90], lter [1621/1752] Loss: 2.7134\n",
      "Epoch [39/90], lter [1631/1752] Loss: 3.0149\n",
      "Epoch [39/90], lter [1641/1752] Loss: 3.5001\n",
      "Epoch [39/90], lter [1651/1752] Loss: 2.5556\n",
      "Epoch [39/90], lter [1661/1752] Loss: 2.8547\n",
      "Epoch [39/90], lter [1671/1752] Loss: 3.0514\n",
      "Epoch [39/90], lter [1681/1752] Loss: 2.3412\n",
      "Epoch [39/90], lter [1691/1752] Loss: 2.5235\n",
      "Epoch [39/90], lter [1701/1752] Loss: 3.7439\n",
      "Epoch [39/90], lter [1711/1752] Loss: 2.9057\n",
      "Epoch [39/90], lter [1721/1752] Loss: 3.5255\n",
      "Epoch [39/90], lter [1731/1752] Loss: 3.1887\n",
      "Epoch [39/90], lter [1741/1752] Loss: 2.5641\n",
      "Epoch [39/90], lter [1751/1752] Loss: 2.2444\n",
      "Epoch:  39 | train loss : 2.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 39/90 [72:24:15<16:44:07, 1181.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  38 | test loss : 2.2390\n",
      "Epoch [40/90], lter [1/1752] Loss: 2.6595\n",
      "Epoch [40/90], lter [11/1752] Loss: 2.3664\n",
      "Epoch [40/90], lter [21/1752] Loss: 2.5132\n",
      "Epoch [40/90], lter [31/1752] Loss: 2.9244\n",
      "Epoch [40/90], lter [41/1752] Loss: 2.3407\n",
      "Epoch [40/90], lter [51/1752] Loss: 2.6316\n",
      "Epoch [40/90], lter [61/1752] Loss: 2.1541\n",
      "Epoch [40/90], lter [71/1752] Loss: 3.4213\n",
      "Epoch [40/90], lter [81/1752] Loss: 1.9668\n",
      "Epoch [40/90], lter [91/1752] Loss: 3.7929\n",
      "Epoch [40/90], lter [101/1752] Loss: 3.2980\n",
      "Epoch [40/90], lter [111/1752] Loss: 2.9782\n",
      "Epoch [40/90], lter [121/1752] Loss: 2.5444\n",
      "Epoch [40/90], lter [131/1752] Loss: 3.1835\n",
      "Epoch [40/90], lter [141/1752] Loss: 1.2568\n",
      "Epoch [40/90], lter [151/1752] Loss: 3.4894\n",
      "Epoch [40/90], lter [161/1752] Loss: 3.2988\n",
      "Epoch [40/90], lter [171/1752] Loss: 3.9502\n",
      "Epoch [40/90], lter [181/1752] Loss: 2.7616\n",
      "Epoch [40/90], lter [191/1752] Loss: 2.0171\n",
      "Epoch [40/90], lter [201/1752] Loss: 2.1679\n",
      "Epoch [40/90], lter [211/1752] Loss: 3.0922\n",
      "Epoch [40/90], lter [221/1752] Loss: 4.7623\n",
      "Epoch [40/90], lter [231/1752] Loss: 2.5680\n",
      "Epoch [40/90], lter [241/1752] Loss: 3.1372\n",
      "Epoch [40/90], lter [251/1752] Loss: 3.3337\n",
      "Epoch [40/90], lter [261/1752] Loss: 3.0455\n",
      "Epoch [40/90], lter [271/1752] Loss: 2.9428\n",
      "Epoch [40/90], lter [281/1752] Loss: 2.3951\n",
      "Epoch [40/90], lter [291/1752] Loss: 2.6736\n",
      "Epoch [40/90], lter [301/1752] Loss: 2.0326\n",
      "Epoch [40/90], lter [311/1752] Loss: 2.5083\n",
      "Epoch [40/90], lter [321/1752] Loss: 2.4534\n",
      "Epoch [40/90], lter [331/1752] Loss: 2.4545\n",
      "Epoch [40/90], lter [341/1752] Loss: 3.1953\n",
      "Epoch [40/90], lter [351/1752] Loss: 2.3948\n",
      "Epoch [40/90], lter [361/1752] Loss: 3.4566\n",
      "Epoch [40/90], lter [371/1752] Loss: 3.4877\n",
      "Epoch [40/90], lter [381/1752] Loss: 3.3471\n",
      "Epoch [40/90], lter [391/1752] Loss: 3.6092\n",
      "Epoch [40/90], lter [401/1752] Loss: 3.5147\n",
      "Epoch [40/90], lter [411/1752] Loss: 4.0802\n",
      "Epoch [40/90], lter [421/1752] Loss: 3.2864\n",
      "Epoch [40/90], lter [431/1752] Loss: 3.1922\n",
      "Epoch [40/90], lter [441/1752] Loss: 2.9263\n",
      "Epoch [40/90], lter [451/1752] Loss: 2.8689\n",
      "Epoch [40/90], lter [461/1752] Loss: 3.0180\n",
      "Epoch [40/90], lter [471/1752] Loss: 4.2407\n",
      "Epoch [40/90], lter [481/1752] Loss: 2.3295\n",
      "Epoch [40/90], lter [491/1752] Loss: 2.4247\n",
      "Epoch [40/90], lter [501/1752] Loss: 1.8741\n",
      "Epoch [40/90], lter [511/1752] Loss: 2.9454\n",
      "Epoch [40/90], lter [521/1752] Loss: 3.3475\n",
      "Epoch [40/90], lter [531/1752] Loss: 2.4881\n",
      "Epoch [40/90], lter [541/1752] Loss: 3.0829\n",
      "Epoch [40/90], lter [551/1752] Loss: 3.0688\n",
      "Epoch [40/90], lter [561/1752] Loss: 3.1325\n",
      "Epoch [40/90], lter [571/1752] Loss: 3.2208\n",
      "Epoch [40/90], lter [581/1752] Loss: 2.2876\n",
      "Epoch [40/90], lter [591/1752] Loss: 3.8668\n",
      "Epoch [40/90], lter [601/1752] Loss: 1.8866\n",
      "Epoch [40/90], lter [611/1752] Loss: 2.7642\n",
      "Epoch [40/90], lter [621/1752] Loss: 1.6873\n",
      "Epoch [40/90], lter [631/1752] Loss: 2.6949\n",
      "Epoch [40/90], lter [641/1752] Loss: 3.1681\n",
      "Epoch [40/90], lter [651/1752] Loss: 2.2013\n",
      "Epoch [40/90], lter [661/1752] Loss: 1.9355\n",
      "Epoch [40/90], lter [671/1752] Loss: 4.2147\n",
      "Epoch [40/90], lter [681/1752] Loss: 3.9064\n",
      "Epoch [40/90], lter [691/1752] Loss: 3.5274\n",
      "Epoch [40/90], lter [701/1752] Loss: 3.2077\n",
      "Epoch [40/90], lter [711/1752] Loss: 2.4143\n",
      "Epoch [40/90], lter [721/1752] Loss: 2.1234\n",
      "Epoch [40/90], lter [731/1752] Loss: 2.7365\n",
      "Epoch [40/90], lter [741/1752] Loss: 2.0254\n",
      "Epoch [40/90], lter [751/1752] Loss: 2.1371\n",
      "Epoch [40/90], lter [761/1752] Loss: 1.8214\n",
      "Epoch [40/90], lter [771/1752] Loss: 2.0795\n",
      "Epoch [40/90], lter [781/1752] Loss: 2.1617\n",
      "Epoch [40/90], lter [791/1752] Loss: 3.8653\n",
      "Epoch [40/90], lter [801/1752] Loss: 2.9204\n",
      "Epoch [40/90], lter [811/1752] Loss: 3.4963\n",
      "Epoch [40/90], lter [821/1752] Loss: 2.5630\n",
      "Epoch [40/90], lter [831/1752] Loss: 2.2618\n",
      "Epoch [40/90], lter [841/1752] Loss: 3.4358\n",
      "Epoch [40/90], lter [851/1752] Loss: 3.7330\n",
      "Epoch [40/90], lter [861/1752] Loss: 2.5027\n",
      "Epoch [40/90], lter [871/1752] Loss: 2.8423\n",
      "Epoch [40/90], lter [881/1752] Loss: 2.5598\n",
      "Epoch [40/90], lter [891/1752] Loss: 2.6038\n",
      "Epoch [40/90], lter [901/1752] Loss: 2.7336\n",
      "Epoch [40/90], lter [911/1752] Loss: 3.2766\n",
      "Epoch [40/90], lter [921/1752] Loss: 3.6032\n",
      "Epoch [40/90], lter [931/1752] Loss: 2.1266\n",
      "Epoch [40/90], lter [941/1752] Loss: 4.4105\n",
      "Epoch [40/90], lter [951/1752] Loss: 3.2091\n",
      "Epoch [40/90], lter [961/1752] Loss: 2.2075\n",
      "Epoch [40/90], lter [971/1752] Loss: 2.7404\n",
      "Epoch [40/90], lter [981/1752] Loss: 3.8290\n",
      "Epoch [40/90], lter [991/1752] Loss: 2.4851\n",
      "Epoch [40/90], lter [1001/1752] Loss: 3.0871\n",
      "Epoch [40/90], lter [1011/1752] Loss: 3.3736\n",
      "Epoch [40/90], lter [1021/1752] Loss: 3.6455\n",
      "Epoch [40/90], lter [1031/1752] Loss: 3.7053\n",
      "Epoch [40/90], lter [1041/1752] Loss: 2.1130\n",
      "Epoch [40/90], lter [1051/1752] Loss: 3.8219\n",
      "Epoch [40/90], lter [1061/1752] Loss: 3.1275\n",
      "Epoch [40/90], lter [1071/1752] Loss: 3.6288\n",
      "Epoch [40/90], lter [1081/1752] Loss: 3.0174\n",
      "Epoch [40/90], lter [1091/1752] Loss: 2.4456\n",
      "Epoch [40/90], lter [1101/1752] Loss: 2.1422\n",
      "Epoch [40/90], lter [1111/1752] Loss: 3.7106\n",
      "Epoch [40/90], lter [1121/1752] Loss: 2.4514\n",
      "Epoch [40/90], lter [1131/1752] Loss: 2.5236\n",
      "Epoch [40/90], lter [1141/1752] Loss: 3.2414\n",
      "Epoch [40/90], lter [1151/1752] Loss: 2.7069\n",
      "Epoch [40/90], lter [1161/1752] Loss: 2.6346\n",
      "Epoch [40/90], lter [1171/1752] Loss: 2.3435\n",
      "Epoch [40/90], lter [1181/1752] Loss: 2.5413\n",
      "Epoch [40/90], lter [1191/1752] Loss: 2.3335\n",
      "Epoch [40/90], lter [1201/1752] Loss: 2.6958\n",
      "Epoch [40/90], lter [1211/1752] Loss: 2.3311\n",
      "Epoch [40/90], lter [1221/1752] Loss: 3.2491\n",
      "Epoch [40/90], lter [1231/1752] Loss: 3.2763\n",
      "Epoch [40/90], lter [1241/1752] Loss: 4.9760\n",
      "Epoch [40/90], lter [1251/1752] Loss: 3.6696\n",
      "Epoch [40/90], lter [1261/1752] Loss: 2.6120\n",
      "Epoch [40/90], lter [1271/1752] Loss: 3.4096\n",
      "Epoch [40/90], lter [1281/1752] Loss: 2.7489\n",
      "Epoch [40/90], lter [1291/1752] Loss: 2.6434\n",
      "Epoch [40/90], lter [1301/1752] Loss: 2.0046\n",
      "Epoch [40/90], lter [1311/1752] Loss: 3.3075\n",
      "Epoch [40/90], lter [1321/1752] Loss: 3.3013\n",
      "Epoch [40/90], lter [1331/1752] Loss: 3.0189\n",
      "Epoch [40/90], lter [1341/1752] Loss: 1.8714\n",
      "Epoch [40/90], lter [1351/1752] Loss: 1.6720\n",
      "Epoch [40/90], lter [1361/1752] Loss: 2.7958\n",
      "Epoch [40/90], lter [1371/1752] Loss: 2.9103\n",
      "Epoch [40/90], lter [1381/1752] Loss: 3.7703\n",
      "Epoch [40/90], lter [1391/1752] Loss: 3.4274\n",
      "Epoch [40/90], lter [1401/1752] Loss: 3.4455\n",
      "Epoch [40/90], lter [1411/1752] Loss: 2.6463\n",
      "Epoch [40/90], lter [1421/1752] Loss: 3.7390\n",
      "Epoch [40/90], lter [1431/1752] Loss: 2.4999\n",
      "Epoch [40/90], lter [1441/1752] Loss: 3.1936\n",
      "Epoch [40/90], lter [1451/1752] Loss: 2.1743\n",
      "Epoch [40/90], lter [1461/1752] Loss: 2.9670\n",
      "Epoch [40/90], lter [1471/1752] Loss: 2.0848\n",
      "Epoch [40/90], lter [1481/1752] Loss: 3.3144\n",
      "Epoch [40/90], lter [1491/1752] Loss: 2.0349\n",
      "Epoch [40/90], lter [1501/1752] Loss: 2.6257\n",
      "Epoch [40/90], lter [1511/1752] Loss: 2.1109\n",
      "Epoch [40/90], lter [1521/1752] Loss: 2.7578\n",
      "Epoch [40/90], lter [1531/1752] Loss: 3.1990\n",
      "Epoch [40/90], lter [1541/1752] Loss: 2.1239\n",
      "Epoch [40/90], lter [1551/1752] Loss: 2.8604\n",
      "Epoch [40/90], lter [1561/1752] Loss: 2.6199\n",
      "Epoch [40/90], lter [1571/1752] Loss: 2.7911\n",
      "Epoch [40/90], lter [1581/1752] Loss: 3.2094\n",
      "Epoch [40/90], lter [1591/1752] Loss: 3.4194\n",
      "Epoch [40/90], lter [1601/1752] Loss: 3.0699\n",
      "Epoch [40/90], lter [1611/1752] Loss: 2.4946\n",
      "Epoch [40/90], lter [1621/1752] Loss: 2.8184\n",
      "Epoch [40/90], lter [1631/1752] Loss: 2.8712\n",
      "Epoch [40/90], lter [1641/1752] Loss: 2.3906\n",
      "Epoch [40/90], lter [1651/1752] Loss: 3.2029\n",
      "Epoch [40/90], lter [1661/1752] Loss: 2.9780\n",
      "Epoch [40/90], lter [1671/1752] Loss: 2.9375\n",
      "Epoch [40/90], lter [1681/1752] Loss: 2.1922\n",
      "Epoch [40/90], lter [1691/1752] Loss: 3.1975\n",
      "Epoch [40/90], lter [1701/1752] Loss: 2.6809\n",
      "Epoch [40/90], lter [1711/1752] Loss: 2.2876\n",
      "Epoch [40/90], lter [1721/1752] Loss: 2.7805\n",
      "Epoch [40/90], lter [1731/1752] Loss: 3.0190\n",
      "Epoch [40/90], lter [1741/1752] Loss: 2.0890\n",
      "Epoch [40/90], lter [1751/1752] Loss: 2.0902\n",
      "Epoch:  40 | train loss : 2.8335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 40/90 [72:44:39<16:35:07, 1194.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  39 | test loss : 2.1058\n",
      "Epoch [41/90], lter [1/1752] Loss: 2.5842\n",
      "Epoch [41/90], lter [11/1752] Loss: 3.5515\n",
      "Epoch [41/90], lter [21/1752] Loss: 2.5377\n",
      "Epoch [41/90], lter [31/1752] Loss: 1.8702\n",
      "Epoch [41/90], lter [41/1752] Loss: 2.8982\n",
      "Epoch [41/90], lter [51/1752] Loss: 2.4293\n",
      "Epoch [41/90], lter [61/1752] Loss: 2.8090\n",
      "Epoch [41/90], lter [71/1752] Loss: 3.2137\n",
      "Epoch [41/90], lter [81/1752] Loss: 1.6088\n",
      "Epoch [41/90], lter [91/1752] Loss: 2.3522\n",
      "Epoch [41/90], lter [101/1752] Loss: 2.1203\n",
      "Epoch [41/90], lter [111/1752] Loss: 3.3240\n",
      "Epoch [41/90], lter [121/1752] Loss: 2.8369\n",
      "Epoch [41/90], lter [131/1752] Loss: 3.0002\n",
      "Epoch [41/90], lter [141/1752] Loss: 2.3844\n",
      "Epoch [41/90], lter [151/1752] Loss: 2.9527\n",
      "Epoch [41/90], lter [161/1752] Loss: 3.1010\n",
      "Epoch [41/90], lter [171/1752] Loss: 3.0547\n",
      "Epoch [41/90], lter [181/1752] Loss: 2.9997\n",
      "Epoch [41/90], lter [191/1752] Loss: 2.9227\n",
      "Epoch [41/90], lter [201/1752] Loss: 2.5107\n",
      "Epoch [41/90], lter [211/1752] Loss: 3.1740\n",
      "Epoch [41/90], lter [221/1752] Loss: 2.3788\n",
      "Epoch [41/90], lter [231/1752] Loss: 2.2898\n",
      "Epoch [41/90], lter [241/1752] Loss: 2.7763\n",
      "Epoch [41/90], lter [251/1752] Loss: 2.0907\n",
      "Epoch [41/90], lter [261/1752] Loss: 2.4947\n",
      "Epoch [41/90], lter [271/1752] Loss: 2.2157\n",
      "Epoch [41/90], lter [281/1752] Loss: 3.3670\n",
      "Epoch [41/90], lter [291/1752] Loss: 4.4716\n",
      "Epoch [41/90], lter [301/1752] Loss: 2.7289\n",
      "Epoch [41/90], lter [311/1752] Loss: 3.0948\n",
      "Epoch [41/90], lter [321/1752] Loss: 3.0134\n",
      "Epoch [41/90], lter [331/1752] Loss: 3.0663\n",
      "Epoch [41/90], lter [341/1752] Loss: 3.1093\n",
      "Epoch [41/90], lter [351/1752] Loss: 2.6049\n",
      "Epoch [41/90], lter [361/1752] Loss: 2.4891\n",
      "Epoch [41/90], lter [371/1752] Loss: 2.6543\n",
      "Epoch [41/90], lter [381/1752] Loss: 3.1719\n",
      "Epoch [41/90], lter [391/1752] Loss: 2.4275\n",
      "Epoch [41/90], lter [401/1752] Loss: 1.9243\n",
      "Epoch [41/90], lter [411/1752] Loss: 2.4853\n",
      "Epoch [41/90], lter [421/1752] Loss: 2.4582\n",
      "Epoch [41/90], lter [431/1752] Loss: 2.7408\n",
      "Epoch [41/90], lter [441/1752] Loss: 2.1048\n",
      "Epoch [41/90], lter [451/1752] Loss: 1.7641\n",
      "Epoch [41/90], lter [461/1752] Loss: 1.7449\n",
      "Epoch [41/90], lter [471/1752] Loss: 3.0932\n",
      "Epoch [41/90], lter [481/1752] Loss: 2.0802\n",
      "Epoch [41/90], lter [491/1752] Loss: 3.0965\n",
      "Epoch [41/90], lter [501/1752] Loss: 1.8539\n",
      "Epoch [41/90], lter [511/1752] Loss: 3.4195\n",
      "Epoch [41/90], lter [521/1752] Loss: 2.6456\n",
      "Epoch [41/90], lter [531/1752] Loss: 2.3860\n",
      "Epoch [41/90], lter [541/1752] Loss: 2.5265\n",
      "Epoch [41/90], lter [551/1752] Loss: 3.5997\n",
      "Epoch [41/90], lter [561/1752] Loss: 3.7289\n",
      "Epoch [41/90], lter [571/1752] Loss: 3.4780\n",
      "Epoch [41/90], lter [581/1752] Loss: 2.1160\n",
      "Epoch [41/90], lter [591/1752] Loss: 2.2917\n",
      "Epoch [41/90], lter [601/1752] Loss: 1.8609\n",
      "Epoch [41/90], lter [611/1752] Loss: 2.5378\n",
      "Epoch [41/90], lter [621/1752] Loss: 2.8513\n",
      "Epoch [41/90], lter [631/1752] Loss: 3.3120\n",
      "Epoch [41/90], lter [641/1752] Loss: 2.7978\n",
      "Epoch [41/90], lter [651/1752] Loss: 2.4851\n",
      "Epoch [41/90], lter [661/1752] Loss: 3.6180\n",
      "Epoch [41/90], lter [671/1752] Loss: 2.4612\n",
      "Epoch [41/90], lter [681/1752] Loss: 3.6576\n",
      "Epoch [41/90], lter [691/1752] Loss: 3.6692\n",
      "Epoch [41/90], lter [701/1752] Loss: 2.6968\n",
      "Epoch [41/90], lter [711/1752] Loss: 2.6624\n",
      "Epoch [41/90], lter [721/1752] Loss: 1.8370\n",
      "Epoch [41/90], lter [731/1752] Loss: 1.9091\n",
      "Epoch [41/90], lter [741/1752] Loss: 3.6968\n",
      "Epoch [41/90], lter [751/1752] Loss: 2.1533\n",
      "Epoch [41/90], lter [761/1752] Loss: 3.4015\n",
      "Epoch [41/90], lter [771/1752] Loss: 2.0647\n",
      "Epoch [41/90], lter [781/1752] Loss: 3.3418\n",
      "Epoch [41/90], lter [791/1752] Loss: 3.2012\n",
      "Epoch [41/90], lter [801/1752] Loss: 2.4043\n",
      "Epoch [41/90], lter [811/1752] Loss: 2.7609\n",
      "Epoch [41/90], lter [821/1752] Loss: 3.1773\n",
      "Epoch [41/90], lter [831/1752] Loss: 3.6411\n",
      "Epoch [41/90], lter [841/1752] Loss: 2.8412\n",
      "Epoch [41/90], lter [851/1752] Loss: 2.6750\n",
      "Epoch [41/90], lter [861/1752] Loss: 2.4596\n",
      "Epoch [41/90], lter [871/1752] Loss: 2.6481\n",
      "Epoch [41/90], lter [881/1752] Loss: 3.5324\n",
      "Epoch [41/90], lter [891/1752] Loss: 3.5858\n",
      "Epoch [41/90], lter [901/1752] Loss: 2.8885\n",
      "Epoch [41/90], lter [911/1752] Loss: 2.9301\n",
      "Epoch [41/90], lter [921/1752] Loss: 2.7947\n",
      "Epoch [41/90], lter [931/1752] Loss: 2.6129\n",
      "Epoch [41/90], lter [941/1752] Loss: 2.2584\n",
      "Epoch [41/90], lter [951/1752] Loss: 2.4567\n",
      "Epoch [41/90], lter [961/1752] Loss: 2.5963\n",
      "Epoch [41/90], lter [971/1752] Loss: 2.0645\n",
      "Epoch [41/90], lter [981/1752] Loss: 2.7185\n",
      "Epoch [41/90], lter [991/1752] Loss: 2.3391\n",
      "Epoch [41/90], lter [1001/1752] Loss: 3.4329\n",
      "Epoch [41/90], lter [1011/1752] Loss: 2.3277\n",
      "Epoch [41/90], lter [1021/1752] Loss: 2.4613\n",
      "Epoch [41/90], lter [1031/1752] Loss: 2.4583\n",
      "Epoch [41/90], lter [1041/1752] Loss: 3.0414\n",
      "Epoch [41/90], lter [1051/1752] Loss: 2.4773\n",
      "Epoch [41/90], lter [1061/1752] Loss: 3.7018\n",
      "Epoch [41/90], lter [1071/1752] Loss: 2.3857\n",
      "Epoch [41/90], lter [1081/1752] Loss: 2.3027\n",
      "Epoch [41/90], lter [1091/1752] Loss: 3.3883\n",
      "Epoch [41/90], lter [1101/1752] Loss: 2.2834\n",
      "Epoch [41/90], lter [1111/1752] Loss: 2.8252\n",
      "Epoch [41/90], lter [1121/1752] Loss: 3.6594\n",
      "Epoch [41/90], lter [1131/1752] Loss: 2.6700\n",
      "Epoch [41/90], lter [1141/1752] Loss: 2.1840\n",
      "Epoch [41/90], lter [1151/1752] Loss: 3.5184\n",
      "Epoch [41/90], lter [1161/1752] Loss: 2.0929\n",
      "Epoch [41/90], lter [1171/1752] Loss: 3.0938\n",
      "Epoch [41/90], lter [1181/1752] Loss: 2.3480\n",
      "Epoch [41/90], lter [1191/1752] Loss: 2.6968\n",
      "Epoch [41/90], lter [1201/1752] Loss: 3.3229\n",
      "Epoch [41/90], lter [1211/1752] Loss: 1.8537\n",
      "Epoch [41/90], lter [1221/1752] Loss: 3.1018\n",
      "Epoch [41/90], lter [1231/1752] Loss: 2.9808\n",
      "Epoch [41/90], lter [1241/1752] Loss: 2.4275\n",
      "Epoch [41/90], lter [1251/1752] Loss: 2.4375\n",
      "Epoch [41/90], lter [1261/1752] Loss: 2.7892\n",
      "Epoch [41/90], lter [1271/1752] Loss: 2.4235\n",
      "Epoch [41/90], lter [1281/1752] Loss: 2.5565\n",
      "Epoch [41/90], lter [1291/1752] Loss: 2.6200\n",
      "Epoch [41/90], lter [1301/1752] Loss: 2.1749\n",
      "Epoch [41/90], lter [1311/1752] Loss: 1.6720\n",
      "Epoch [41/90], lter [1321/1752] Loss: 1.8908\n",
      "Epoch [41/90], lter [1331/1752] Loss: 3.0412\n",
      "Epoch [41/90], lter [1341/1752] Loss: 3.1565\n",
      "Epoch [41/90], lter [1351/1752] Loss: 2.4288\n",
      "Epoch [41/90], lter [1361/1752] Loss: 2.3698\n",
      "Epoch [41/90], lter [1371/1752] Loss: 2.2122\n",
      "Epoch [41/90], lter [1381/1752] Loss: 2.5494\n",
      "Epoch [41/90], lter [1391/1752] Loss: 3.0448\n",
      "Epoch [41/90], lter [1401/1752] Loss: 2.5401\n",
      "Epoch [41/90], lter [1411/1752] Loss: 2.7131\n",
      "Epoch [41/90], lter [1421/1752] Loss: 2.4398\n",
      "Epoch [41/90], lter [1431/1752] Loss: 2.5145\n",
      "Epoch [41/90], lter [1441/1752] Loss: 2.7078\n",
      "Epoch [41/90], lter [1451/1752] Loss: 1.9803\n",
      "Epoch [41/90], lter [1461/1752] Loss: 3.3867\n",
      "Epoch [41/90], lter [1471/1752] Loss: 2.4877\n",
      "Epoch [41/90], lter [1481/1752] Loss: 1.9483\n",
      "Epoch [41/90], lter [1491/1752] Loss: 3.3095\n",
      "Epoch [41/90], lter [1501/1752] Loss: 2.4948\n",
      "Epoch [41/90], lter [1511/1752] Loss: 2.2432\n",
      "Epoch [41/90], lter [1521/1752] Loss: 2.4946\n",
      "Epoch [41/90], lter [1531/1752] Loss: 3.0159\n",
      "Epoch [41/90], lter [1541/1752] Loss: 2.4511\n",
      "Epoch [41/90], lter [1551/1752] Loss: 2.1698\n",
      "Epoch [41/90], lter [1561/1752] Loss: 3.0934\n",
      "Epoch [41/90], lter [1571/1752] Loss: 2.8484\n",
      "Epoch [41/90], lter [1581/1752] Loss: 3.1563\n",
      "Epoch [41/90], lter [1591/1752] Loss: 3.7486\n",
      "Epoch [41/90], lter [1601/1752] Loss: 2.4548\n",
      "Epoch [41/90], lter [1611/1752] Loss: 4.3419\n",
      "Epoch [41/90], lter [1621/1752] Loss: 3.7235\n",
      "Epoch [41/90], lter [1631/1752] Loss: 1.9568\n",
      "Epoch [41/90], lter [1641/1752] Loss: 4.4175\n",
      "Epoch [41/90], lter [1651/1752] Loss: 2.4552\n",
      "Epoch [41/90], lter [1661/1752] Loss: 3.0545\n",
      "Epoch [41/90], lter [1671/1752] Loss: 2.4737\n",
      "Epoch [41/90], lter [1681/1752] Loss: 2.7639\n",
      "Epoch [41/90], lter [1691/1752] Loss: 2.7704\n",
      "Epoch [41/90], lter [1701/1752] Loss: 1.8425\n",
      "Epoch [41/90], lter [1711/1752] Loss: 3.4219\n",
      "Epoch [41/90], lter [1721/1752] Loss: 1.6989\n",
      "Epoch [41/90], lter [1731/1752] Loss: 2.1595\n",
      "Epoch [41/90], lter [1741/1752] Loss: 2.9788\n",
      "Epoch [41/90], lter [1751/1752] Loss: 2.8020\n",
      "Epoch:  41 | train loss : 2.7152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 41/90 [73:04:58<16:21:21, 1201.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  40 | test loss : 2.0775\n",
      "Epoch [42/90], lter [1/1752] Loss: 2.2463\n",
      "Epoch [42/90], lter [11/1752] Loss: 3.3001\n",
      "Epoch [42/90], lter [21/1752] Loss: 2.7208\n",
      "Epoch [42/90], lter [31/1752] Loss: 2.0945\n",
      "Epoch [42/90], lter [41/1752] Loss: 2.6652\n",
      "Epoch [42/90], lter [51/1752] Loss: 3.4544\n",
      "Epoch [42/90], lter [61/1752] Loss: 2.9386\n",
      "Epoch [42/90], lter [71/1752] Loss: 3.0272\n",
      "Epoch [42/90], lter [81/1752] Loss: 3.0629\n",
      "Epoch [42/90], lter [91/1752] Loss: 2.4524\n",
      "Epoch [42/90], lter [101/1752] Loss: 2.8539\n",
      "Epoch [42/90], lter [111/1752] Loss: 2.8584\n",
      "Epoch [42/90], lter [121/1752] Loss: 2.2397\n",
      "Epoch [42/90], lter [131/1752] Loss: 2.0967\n",
      "Epoch [42/90], lter [141/1752] Loss: 2.7340\n",
      "Epoch [42/90], lter [151/1752] Loss: 2.4351\n",
      "Epoch [42/90], lter [161/1752] Loss: 2.4969\n",
      "Epoch [42/90], lter [171/1752] Loss: 2.9835\n",
      "Epoch [42/90], lter [181/1752] Loss: 2.1689\n",
      "Epoch [42/90], lter [191/1752] Loss: 4.0469\n",
      "Epoch [42/90], lter [201/1752] Loss: 1.6193\n",
      "Epoch [42/90], lter [211/1752] Loss: 2.2572\n",
      "Epoch [42/90], lter [221/1752] Loss: 2.1335\n",
      "Epoch [42/90], lter [231/1752] Loss: 2.6313\n",
      "Epoch [42/90], lter [241/1752] Loss: 3.2130\n",
      "Epoch [42/90], lter [251/1752] Loss: 3.0200\n",
      "Epoch [42/90], lter [261/1752] Loss: 2.4083\n",
      "Epoch [42/90], lter [271/1752] Loss: 2.9752\n",
      "Epoch [42/90], lter [281/1752] Loss: 2.4338\n",
      "Epoch [42/90], lter [291/1752] Loss: 2.1325\n",
      "Epoch [42/90], lter [301/1752] Loss: 1.8059\n",
      "Epoch [42/90], lter [311/1752] Loss: 2.9076\n",
      "Epoch [42/90], lter [321/1752] Loss: 2.4354\n",
      "Epoch [42/90], lter [331/1752] Loss: 2.4164\n",
      "Epoch [42/90], lter [341/1752] Loss: 2.4892\n",
      "Epoch [42/90], lter [351/1752] Loss: 3.8142\n",
      "Epoch [42/90], lter [361/1752] Loss: 2.9143\n",
      "Epoch [42/90], lter [371/1752] Loss: 2.7935\n",
      "Epoch [42/90], lter [381/1752] Loss: 2.2129\n",
      "Epoch [42/90], lter [391/1752] Loss: 1.7630\n",
      "Epoch [42/90], lter [401/1752] Loss: 3.0209\n",
      "Epoch [42/90], lter [411/1752] Loss: 2.2427\n",
      "Epoch [42/90], lter [421/1752] Loss: 3.3641\n",
      "Epoch [42/90], lter [431/1752] Loss: 3.0519\n",
      "Epoch [42/90], lter [441/1752] Loss: 2.9424\n",
      "Epoch [42/90], lter [451/1752] Loss: 2.6685\n",
      "Epoch [42/90], lter [461/1752] Loss: 1.9349\n",
      "Epoch [42/90], lter [471/1752] Loss: 2.6875\n",
      "Epoch [42/90], lter [481/1752] Loss: 2.7187\n",
      "Epoch [42/90], lter [491/1752] Loss: 3.0494\n",
      "Epoch [42/90], lter [501/1752] Loss: 2.7324\n",
      "Epoch [42/90], lter [511/1752] Loss: 2.3049\n",
      "Epoch [42/90], lter [521/1752] Loss: 2.9451\n",
      "Epoch [42/90], lter [531/1752] Loss: 3.0509\n",
      "Epoch [42/90], lter [541/1752] Loss: 2.2158\n",
      "Epoch [42/90], lter [551/1752] Loss: 2.5114\n",
      "Epoch [42/90], lter [561/1752] Loss: 3.3730\n",
      "Epoch [42/90], lter [571/1752] Loss: 2.5540\n",
      "Epoch [42/90], lter [581/1752] Loss: 2.7494\n",
      "Epoch [42/90], lter [591/1752] Loss: 3.4010\n",
      "Epoch [42/90], lter [601/1752] Loss: 3.2752\n",
      "Epoch [42/90], lter [611/1752] Loss: 3.8650\n",
      "Epoch [42/90], lter [621/1752] Loss: 1.7290\n",
      "Epoch [42/90], lter [631/1752] Loss: 3.1699\n",
      "Epoch [42/90], lter [641/1752] Loss: 2.0629\n",
      "Epoch [42/90], lter [651/1752] Loss: 2.7858\n",
      "Epoch [42/90], lter [661/1752] Loss: 3.0444\n",
      "Epoch [42/90], lter [671/1752] Loss: 2.1067\n",
      "Epoch [42/90], lter [681/1752] Loss: 1.8613\n",
      "Epoch [42/90], lter [691/1752] Loss: 2.9243\n",
      "Epoch [42/90], lter [701/1752] Loss: 2.9714\n",
      "Epoch [42/90], lter [711/1752] Loss: 2.5233\n",
      "Epoch [42/90], lter [721/1752] Loss: 1.8538\n",
      "Epoch [42/90], lter [731/1752] Loss: 3.3037\n",
      "Epoch [42/90], lter [741/1752] Loss: 2.9149\n",
      "Epoch [42/90], lter [751/1752] Loss: 2.4473\n",
      "Epoch [42/90], lter [761/1752] Loss: 2.0823\n",
      "Epoch [42/90], lter [771/1752] Loss: 2.9442\n",
      "Epoch [42/90], lter [781/1752] Loss: 2.2085\n",
      "Epoch [42/90], lter [791/1752] Loss: 1.9132\n",
      "Epoch [42/90], lter [801/1752] Loss: 3.8434\n",
      "Epoch [42/90], lter [811/1752] Loss: 3.1150\n",
      "Epoch [42/90], lter [821/1752] Loss: 2.5891\n",
      "Epoch [42/90], lter [831/1752] Loss: 1.9598\n",
      "Epoch [42/90], lter [841/1752] Loss: 2.7310\n",
      "Epoch [42/90], lter [851/1752] Loss: 1.9299\n",
      "Epoch [42/90], lter [861/1752] Loss: 2.8760\n",
      "Epoch [42/90], lter [871/1752] Loss: 3.0101\n",
      "Epoch [42/90], lter [881/1752] Loss: 2.7084\n",
      "Epoch [42/90], lter [891/1752] Loss: 2.4716\n",
      "Epoch [42/90], lter [901/1752] Loss: 2.8001\n",
      "Epoch [42/90], lter [911/1752] Loss: 2.5289\n",
      "Epoch [42/90], lter [921/1752] Loss: 3.0084\n",
      "Epoch [42/90], lter [931/1752] Loss: 2.8553\n",
      "Epoch [42/90], lter [941/1752] Loss: 2.4690\n",
      "Epoch [42/90], lter [951/1752] Loss: 3.4162\n",
      "Epoch [42/90], lter [961/1752] Loss: 3.0492\n",
      "Epoch [42/90], lter [971/1752] Loss: 3.2469\n",
      "Epoch [42/90], lter [981/1752] Loss: 3.9257\n",
      "Epoch [42/90], lter [991/1752] Loss: 2.4188\n",
      "Epoch [42/90], lter [1001/1752] Loss: 2.4698\n",
      "Epoch [42/90], lter [1011/1752] Loss: 2.5489\n",
      "Epoch [42/90], lter [1021/1752] Loss: 1.3583\n",
      "Epoch [42/90], lter [1031/1752] Loss: 2.8761\n",
      "Epoch [42/90], lter [1041/1752] Loss: 2.8028\n",
      "Epoch [42/90], lter [1051/1752] Loss: 3.4880\n",
      "Epoch [42/90], lter [1061/1752] Loss: 2.9176\n",
      "Epoch [42/90], lter [1071/1752] Loss: 4.1693\n",
      "Epoch [42/90], lter [1081/1752] Loss: 2.1240\n",
      "Epoch [42/90], lter [1091/1752] Loss: 3.2046\n",
      "Epoch [42/90], lter [1101/1752] Loss: 2.6858\n",
      "Epoch [42/90], lter [1111/1752] Loss: 2.3600\n",
      "Epoch [42/90], lter [1121/1752] Loss: 2.2131\n",
      "Epoch [42/90], lter [1131/1752] Loss: 4.3980\n",
      "Epoch [42/90], lter [1141/1752] Loss: 1.8365\n",
      "Epoch [42/90], lter [1151/1752] Loss: 2.8376\n",
      "Epoch [42/90], lter [1161/1752] Loss: 2.8140\n",
      "Epoch [42/90], lter [1171/1752] Loss: 3.0773\n",
      "Epoch [42/90], lter [1181/1752] Loss: 3.3279\n",
      "Epoch [42/90], lter [1191/1752] Loss: 2.5776\n",
      "Epoch [42/90], lter [1201/1752] Loss: 2.1515\n",
      "Epoch [42/90], lter [1211/1752] Loss: 3.0827\n",
      "Epoch [42/90], lter [1221/1752] Loss: 4.0948\n",
      "Epoch [42/90], lter [1231/1752] Loss: 2.8284\n",
      "Epoch [42/90], lter [1241/1752] Loss: 2.7499\n",
      "Epoch [42/90], lter [1251/1752] Loss: 2.7359\n",
      "Epoch [42/90], lter [1261/1752] Loss: 1.9494\n",
      "Epoch [42/90], lter [1271/1752] Loss: 3.6591\n",
      "Epoch [42/90], lter [1281/1752] Loss: 3.4569\n",
      "Epoch [42/90], lter [1291/1752] Loss: 2.1960\n",
      "Epoch [42/90], lter [1301/1752] Loss: 2.0169\n",
      "Epoch [42/90], lter [1311/1752] Loss: 2.5665\n",
      "Epoch [42/90], lter [1321/1752] Loss: 2.5633\n",
      "Epoch [42/90], lter [1331/1752] Loss: 3.5774\n",
      "Epoch [42/90], lter [1341/1752] Loss: 2.0748\n",
      "Epoch [42/90], lter [1351/1752] Loss: 3.3984\n",
      "Epoch [42/90], lter [1361/1752] Loss: 2.9641\n",
      "Epoch [42/90], lter [1371/1752] Loss: 2.1484\n",
      "Epoch [42/90], lter [1381/1752] Loss: 2.3327\n",
      "Epoch [42/90], lter [1391/1752] Loss: 3.9110\n",
      "Epoch [42/90], lter [1401/1752] Loss: 2.3111\n",
      "Epoch [42/90], lter [1411/1752] Loss: 3.6406\n",
      "Epoch [42/90], lter [1421/1752] Loss: 2.7223\n",
      "Epoch [42/90], lter [1431/1752] Loss: 3.4317\n",
      "Epoch [42/90], lter [1441/1752] Loss: 2.6866\n",
      "Epoch [42/90], lter [1451/1752] Loss: 2.3171\n",
      "Epoch [42/90], lter [1461/1752] Loss: 3.3998\n",
      "Epoch [42/90], lter [1471/1752] Loss: 3.0415\n",
      "Epoch [42/90], lter [1481/1752] Loss: 2.1003\n",
      "Epoch [42/90], lter [1491/1752] Loss: 2.1536\n",
      "Epoch [42/90], lter [1501/1752] Loss: 3.1253\n",
      "Epoch [42/90], lter [1511/1752] Loss: 3.0135\n",
      "Epoch [42/90], lter [1521/1752] Loss: 1.8701\n",
      "Epoch [42/90], lter [1531/1752] Loss: 2.2228\n",
      "Epoch [42/90], lter [1541/1752] Loss: 2.9598\n",
      "Epoch [42/90], lter [1551/1752] Loss: 2.6413\n",
      "Epoch [42/90], lter [1561/1752] Loss: 2.9889\n",
      "Epoch [42/90], lter [1571/1752] Loss: 3.4663\n",
      "Epoch [42/90], lter [1581/1752] Loss: 1.5848\n",
      "Epoch [42/90], lter [1591/1752] Loss: 2.2434\n",
      "Epoch [42/90], lter [1601/1752] Loss: 3.1747\n",
      "Epoch [42/90], lter [1611/1752] Loss: 3.6209\n",
      "Epoch [42/90], lter [1621/1752] Loss: 1.5020\n",
      "Epoch [42/90], lter [1631/1752] Loss: 2.2125\n",
      "Epoch [42/90], lter [1641/1752] Loss: 2.2040\n",
      "Epoch [42/90], lter [1651/1752] Loss: 2.6308\n",
      "Epoch [42/90], lter [1661/1752] Loss: 2.7671\n",
      "Epoch [42/90], lter [1671/1752] Loss: 2.3314\n",
      "Epoch [42/90], lter [1681/1752] Loss: 4.1940\n",
      "Epoch [42/90], lter [1691/1752] Loss: 3.5879\n",
      "Epoch [42/90], lter [1701/1752] Loss: 2.5707\n",
      "Epoch [42/90], lter [1711/1752] Loss: 1.9350\n",
      "Epoch [42/90], lter [1721/1752] Loss: 2.6814\n",
      "Epoch [42/90], lter [1731/1752] Loss: 2.6993\n",
      "Epoch [42/90], lter [1741/1752] Loss: 3.8066\n",
      "Epoch [42/90], lter [1751/1752] Loss: 3.1823\n",
      "Epoch:  42 | train loss : 2.7432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 42/90 [73:25:17<16:05:34, 1206.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  41 | test loss : 2.3382\n",
      "Epoch [43/90], lter [1/1752] Loss: 2.1182\n",
      "Epoch [43/90], lter [11/1752] Loss: 3.5179\n",
      "Epoch [43/90], lter [21/1752] Loss: 2.9059\n",
      "Epoch [43/90], lter [31/1752] Loss: 2.7879\n",
      "Epoch [43/90], lter [41/1752] Loss: 2.9184\n",
      "Epoch [43/90], lter [51/1752] Loss: 1.9016\n",
      "Epoch [43/90], lter [61/1752] Loss: 2.8625\n",
      "Epoch [43/90], lter [71/1752] Loss: 2.6420\n",
      "Epoch [43/90], lter [81/1752] Loss: 3.0919\n",
      "Epoch [43/90], lter [91/1752] Loss: 2.6461\n",
      "Epoch [43/90], lter [101/1752] Loss: 3.0857\n",
      "Epoch [43/90], lter [111/1752] Loss: 2.8348\n",
      "Epoch [43/90], lter [121/1752] Loss: 2.7635\n",
      "Epoch [43/90], lter [131/1752] Loss: 2.9647\n",
      "Epoch [43/90], lter [141/1752] Loss: 2.2143\n",
      "Epoch [43/90], lter [151/1752] Loss: 2.3134\n",
      "Epoch [43/90], lter [161/1752] Loss: 3.8190\n",
      "Epoch [43/90], lter [171/1752] Loss: 2.4889\n",
      "Epoch [43/90], lter [181/1752] Loss: 3.4092\n",
      "Epoch [43/90], lter [191/1752] Loss: 2.6257\n",
      "Epoch [43/90], lter [201/1752] Loss: 2.5489\n",
      "Epoch [43/90], lter [211/1752] Loss: 4.0020\n",
      "Epoch [43/90], lter [221/1752] Loss: 2.2458\n",
      "Epoch [43/90], lter [231/1752] Loss: 3.3739\n",
      "Epoch [43/90], lter [241/1752] Loss: 2.4538\n",
      "Epoch [43/90], lter [251/1752] Loss: 2.6227\n",
      "Epoch [43/90], lter [261/1752] Loss: 3.1429\n",
      "Epoch [43/90], lter [271/1752] Loss: 3.5646\n",
      "Epoch [43/90], lter [281/1752] Loss: 3.7036\n",
      "Epoch [43/90], lter [291/1752] Loss: 2.7614\n",
      "Epoch [43/90], lter [301/1752] Loss: 2.1530\n",
      "Epoch [43/90], lter [311/1752] Loss: 2.6489\n",
      "Epoch [43/90], lter [321/1752] Loss: 3.8886\n",
      "Epoch [43/90], lter [331/1752] Loss: 2.4943\n",
      "Epoch [43/90], lter [341/1752] Loss: 2.7135\n",
      "Epoch [43/90], lter [351/1752] Loss: 2.9126\n",
      "Epoch [43/90], lter [361/1752] Loss: 1.8343\n",
      "Epoch [43/90], lter [371/1752] Loss: 2.3631\n",
      "Epoch [43/90], lter [381/1752] Loss: 2.2419\n",
      "Epoch [43/90], lter [391/1752] Loss: 2.9797\n",
      "Epoch [43/90], lter [401/1752] Loss: 1.8307\n",
      "Epoch [43/90], lter [411/1752] Loss: 1.7748\n",
      "Epoch [43/90], lter [421/1752] Loss: 4.0345\n",
      "Epoch [43/90], lter [431/1752] Loss: 3.3744\n",
      "Epoch [43/90], lter [441/1752] Loss: 3.9557\n",
      "Epoch [43/90], lter [451/1752] Loss: 3.7150\n",
      "Epoch [43/90], lter [461/1752] Loss: 3.2014\n",
      "Epoch [43/90], lter [471/1752] Loss: 3.5994\n",
      "Epoch [43/90], lter [481/1752] Loss: 3.9154\n",
      "Epoch [43/90], lter [491/1752] Loss: 2.6529\n",
      "Epoch [43/90], lter [501/1752] Loss: 3.9440\n",
      "Epoch [43/90], lter [511/1752] Loss: 2.3066\n",
      "Epoch [43/90], lter [521/1752] Loss: 2.7922\n",
      "Epoch [43/90], lter [531/1752] Loss: 2.7417\n",
      "Epoch [43/90], lter [541/1752] Loss: 2.1928\n",
      "Epoch [43/90], lter [551/1752] Loss: 3.1503\n",
      "Epoch [43/90], lter [561/1752] Loss: 2.5968\n",
      "Epoch [43/90], lter [571/1752] Loss: 2.6002\n",
      "Epoch [43/90], lter [581/1752] Loss: 1.8143\n",
      "Epoch [43/90], lter [591/1752] Loss: 2.4357\n",
      "Epoch [43/90], lter [601/1752] Loss: 1.6899\n",
      "Epoch [43/90], lter [611/1752] Loss: 2.6688\n",
      "Epoch [43/90], lter [621/1752] Loss: 2.2412\n",
      "Epoch [43/90], lter [631/1752] Loss: 3.6500\n",
      "Epoch [43/90], lter [641/1752] Loss: 1.0545\n",
      "Epoch [43/90], lter [651/1752] Loss: 2.1804\n",
      "Epoch [43/90], lter [661/1752] Loss: 3.7899\n",
      "Epoch [43/90], lter [671/1752] Loss: 2.7925\n",
      "Epoch [43/90], lter [681/1752] Loss: 3.3713\n",
      "Epoch [43/90], lter [691/1752] Loss: 2.0642\n",
      "Epoch [43/90], lter [701/1752] Loss: 2.9842\n",
      "Epoch [43/90], lter [711/1752] Loss: 2.6101\n",
      "Epoch [43/90], lter [721/1752] Loss: 3.0139\n",
      "Epoch [43/90], lter [731/1752] Loss: 2.2254\n",
      "Epoch [43/90], lter [741/1752] Loss: 2.0942\n",
      "Epoch [43/90], lter [751/1752] Loss: 3.9951\n",
      "Epoch [43/90], lter [761/1752] Loss: 4.3450\n",
      "Epoch [43/90], lter [771/1752] Loss: 3.5153\n",
      "Epoch [43/90], lter [781/1752] Loss: 2.4282\n",
      "Epoch [43/90], lter [791/1752] Loss: 2.9217\n",
      "Epoch [43/90], lter [801/1752] Loss: 3.0020\n",
      "Epoch [43/90], lter [811/1752] Loss: 3.1355\n",
      "Epoch [43/90], lter [821/1752] Loss: 3.3053\n",
      "Epoch [43/90], lter [831/1752] Loss: 3.9538\n",
      "Epoch [43/90], lter [841/1752] Loss: 2.2021\n",
      "Epoch [43/90], lter [851/1752] Loss: 2.7414\n",
      "Epoch [43/90], lter [861/1752] Loss: 2.2275\n",
      "Epoch [43/90], lter [871/1752] Loss: 2.1583\n",
      "Epoch [43/90], lter [881/1752] Loss: 2.8117\n",
      "Epoch [43/90], lter [891/1752] Loss: 3.0707\n",
      "Epoch [43/90], lter [901/1752] Loss: 2.6989\n",
      "Epoch [43/90], lter [911/1752] Loss: 1.8259\n",
      "Epoch [43/90], lter [921/1752] Loss: 2.2462\n",
      "Epoch [43/90], lter [931/1752] Loss: 2.8733\n",
      "Epoch [43/90], lter [941/1752] Loss: 2.8425\n",
      "Epoch [43/90], lter [951/1752] Loss: 4.0964\n",
      "Epoch [43/90], lter [961/1752] Loss: 3.5697\n",
      "Epoch [43/90], lter [971/1752] Loss: 2.1920\n",
      "Epoch [43/90], lter [981/1752] Loss: 2.5223\n",
      "Epoch [43/90], lter [991/1752] Loss: 2.4973\n",
      "Epoch [43/90], lter [1001/1752] Loss: 2.7183\n",
      "Epoch [43/90], lter [1011/1752] Loss: 1.9482\n",
      "Epoch [43/90], lter [1021/1752] Loss: 2.0823\n",
      "Epoch [43/90], lter [1031/1752] Loss: 2.4910\n",
      "Epoch [43/90], lter [1041/1752] Loss: 2.7440\n",
      "Epoch [43/90], lter [1051/1752] Loss: 2.3750\n",
      "Epoch [43/90], lter [1061/1752] Loss: 2.3514\n",
      "Epoch [43/90], lter [1071/1752] Loss: 2.3227\n",
      "Epoch [43/90], lter [1081/1752] Loss: 2.4788\n",
      "Epoch [43/90], lter [1091/1752] Loss: 3.6491\n",
      "Epoch [43/90], lter [1101/1752] Loss: 2.3852\n",
      "Epoch [43/90], lter [1111/1752] Loss: 2.9776\n",
      "Epoch [43/90], lter [1121/1752] Loss: 3.9418\n",
      "Epoch [43/90], lter [1131/1752] Loss: 2.4461\n",
      "Epoch [43/90], lter [1141/1752] Loss: 2.0281\n",
      "Epoch [43/90], lter [1151/1752] Loss: 1.7731\n",
      "Epoch [43/90], lter [1161/1752] Loss: 2.3660\n",
      "Epoch [43/90], lter [1171/1752] Loss: 2.4978\n",
      "Epoch [43/90], lter [1181/1752] Loss: 2.4002\n",
      "Epoch [43/90], lter [1191/1752] Loss: 2.7109\n",
      "Epoch [43/90], lter [1201/1752] Loss: 2.0916\n",
      "Epoch [43/90], lter [1211/1752] Loss: 2.4497\n",
      "Epoch [43/90], lter [1221/1752] Loss: 2.7782\n",
      "Epoch [43/90], lter [1231/1752] Loss: 3.3606\n",
      "Epoch [43/90], lter [1241/1752] Loss: 3.6850\n",
      "Epoch [43/90], lter [1251/1752] Loss: 3.1018\n",
      "Epoch [43/90], lter [1261/1752] Loss: 3.0322\n",
      "Epoch [43/90], lter [1271/1752] Loss: 2.5907\n",
      "Epoch [43/90], lter [1281/1752] Loss: 2.2688\n",
      "Epoch [43/90], lter [1291/1752] Loss: 2.8815\n",
      "Epoch [43/90], lter [1301/1752] Loss: 3.3978\n",
      "Epoch [43/90], lter [1311/1752] Loss: 2.1832\n",
      "Epoch [43/90], lter [1321/1752] Loss: 2.0041\n",
      "Epoch [43/90], lter [1331/1752] Loss: 3.0826\n",
      "Epoch [43/90], lter [1341/1752] Loss: 2.0922\n",
      "Epoch [43/90], lter [1351/1752] Loss: 2.3227\n",
      "Epoch [43/90], lter [1361/1752] Loss: 2.6330\n",
      "Epoch [43/90], lter [1371/1752] Loss: 2.0733\n",
      "Epoch [43/90], lter [1381/1752] Loss: 2.6151\n",
      "Epoch [43/90], lter [1391/1752] Loss: 3.7295\n",
      "Epoch [43/90], lter [1401/1752] Loss: 1.9122\n",
      "Epoch [43/90], lter [1411/1752] Loss: 3.5524\n",
      "Epoch [43/90], lter [1421/1752] Loss: 2.1425\n",
      "Epoch [43/90], lter [1431/1752] Loss: 2.2990\n",
      "Epoch [43/90], lter [1441/1752] Loss: 3.9095\n",
      "Epoch [43/90], lter [1451/1752] Loss: 3.0222\n",
      "Epoch [43/90], lter [1461/1752] Loss: 2.5693\n",
      "Epoch [43/90], lter [1471/1752] Loss: 3.7776\n",
      "Epoch [43/90], lter [1481/1752] Loss: 2.6748\n",
      "Epoch [43/90], lter [1491/1752] Loss: 2.0438\n",
      "Epoch [43/90], lter [1501/1752] Loss: 2.6565\n",
      "Epoch [43/90], lter [1511/1752] Loss: 2.4923\n",
      "Epoch [43/90], lter [1521/1752] Loss: 1.6637\n",
      "Epoch [43/90], lter [1531/1752] Loss: 2.2788\n",
      "Epoch [43/90], lter [1541/1752] Loss: 1.9195\n",
      "Epoch [43/90], lter [1551/1752] Loss: 2.5366\n",
      "Epoch [43/90], lter [1561/1752] Loss: 2.6831\n",
      "Epoch [43/90], lter [1571/1752] Loss: 2.3080\n",
      "Epoch [43/90], lter [1581/1752] Loss: 2.0789\n",
      "Epoch [43/90], lter [1591/1752] Loss: 3.4571\n",
      "Epoch [43/90], lter [1601/1752] Loss: 2.7301\n",
      "Epoch [43/90], lter [1611/1752] Loss: 3.2691\n",
      "Epoch [43/90], lter [1621/1752] Loss: 3.1572\n",
      "Epoch [43/90], lter [1631/1752] Loss: 2.2690\n",
      "Epoch [43/90], lter [1641/1752] Loss: 2.9570\n",
      "Epoch [43/90], lter [1651/1752] Loss: 2.6677\n",
      "Epoch [43/90], lter [1661/1752] Loss: 2.7400\n",
      "Epoch [43/90], lter [1671/1752] Loss: 3.0474\n",
      "Epoch [43/90], lter [1681/1752] Loss: 1.7841\n",
      "Epoch [43/90], lter [1691/1752] Loss: 3.3703\n",
      "Epoch [43/90], lter [1701/1752] Loss: 3.9186\n",
      "Epoch [43/90], lter [1711/1752] Loss: 1.5250\n",
      "Epoch [43/90], lter [1721/1752] Loss: 2.6007\n",
      "Epoch [43/90], lter [1731/1752] Loss: 3.5395\n",
      "Epoch [43/90], lter [1741/1752] Loss: 2.3840\n",
      "Epoch [43/90], lter [1751/1752] Loss: 3.2073\n",
      "Epoch:  43 | train loss : 2.8306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 43/90 [73:45:46<15:50:30, 1213.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  42 | test loss : 1.9031\n",
      "Epoch [44/90], lter [1/1752] Loss: 2.0115\n",
      "Epoch [44/90], lter [11/1752] Loss: 2.5655\n",
      "Epoch [44/90], lter [21/1752] Loss: 1.8665\n",
      "Epoch [44/90], lter [31/1752] Loss: 2.6803\n",
      "Epoch [44/90], lter [41/1752] Loss: 3.1261\n",
      "Epoch [44/90], lter [51/1752] Loss: 2.6313\n",
      "Epoch [44/90], lter [61/1752] Loss: 3.1417\n",
      "Epoch [44/90], lter [71/1752] Loss: 2.2732\n",
      "Epoch [44/90], lter [81/1752] Loss: 2.1051\n",
      "Epoch [44/90], lter [91/1752] Loss: 3.6440\n",
      "Epoch [44/90], lter [101/1752] Loss: 2.5380\n",
      "Epoch [44/90], lter [111/1752] Loss: 3.6472\n",
      "Epoch [44/90], lter [121/1752] Loss: 2.8643\n",
      "Epoch [44/90], lter [131/1752] Loss: 2.6074\n",
      "Epoch [44/90], lter [141/1752] Loss: 1.5311\n",
      "Epoch [44/90], lter [151/1752] Loss: 2.8837\n",
      "Epoch [44/90], lter [161/1752] Loss: 2.2623\n",
      "Epoch [44/90], lter [171/1752] Loss: 3.5311\n",
      "Epoch [44/90], lter [181/1752] Loss: 3.0566\n",
      "Epoch [44/90], lter [191/1752] Loss: 3.6693\n",
      "Epoch [44/90], lter [201/1752] Loss: 3.2370\n",
      "Epoch [44/90], lter [211/1752] Loss: 2.8873\n",
      "Epoch [44/90], lter [221/1752] Loss: 4.2334\n",
      "Epoch [44/90], lter [231/1752] Loss: 2.6612\n",
      "Epoch [44/90], lter [241/1752] Loss: 2.0725\n",
      "Epoch [44/90], lter [251/1752] Loss: 3.9683\n",
      "Epoch [44/90], lter [261/1752] Loss: 2.9667\n",
      "Epoch [44/90], lter [271/1752] Loss: 2.1456\n",
      "Epoch [44/90], lter [281/1752] Loss: 2.3326\n",
      "Epoch [44/90], lter [291/1752] Loss: 2.9133\n",
      "Epoch [44/90], lter [301/1752] Loss: 3.2367\n",
      "Epoch [44/90], lter [311/1752] Loss: 2.3699\n",
      "Epoch [44/90], lter [321/1752] Loss: 3.4430\n",
      "Epoch [44/90], lter [331/1752] Loss: 1.8860\n",
      "Epoch [44/90], lter [341/1752] Loss: 2.6031\n",
      "Epoch [44/90], lter [351/1752] Loss: 3.3363\n",
      "Epoch [44/90], lter [361/1752] Loss: 2.0887\n",
      "Epoch [44/90], lter [371/1752] Loss: 2.1607\n",
      "Epoch [44/90], lter [381/1752] Loss: 2.6710\n",
      "Epoch [44/90], lter [391/1752] Loss: 3.0315\n",
      "Epoch [44/90], lter [401/1752] Loss: 2.9825\n",
      "Epoch [44/90], lter [411/1752] Loss: 2.9581\n",
      "Epoch [44/90], lter [421/1752] Loss: 2.3298\n",
      "Epoch [44/90], lter [431/1752] Loss: 1.9825\n",
      "Epoch [44/90], lter [441/1752] Loss: 2.3440\n",
      "Epoch [44/90], lter [451/1752] Loss: 2.5035\n",
      "Epoch [44/90], lter [461/1752] Loss: 3.9998\n",
      "Epoch [44/90], lter [471/1752] Loss: 3.3118\n",
      "Epoch [44/90], lter [481/1752] Loss: 2.2746\n",
      "Epoch [44/90], lter [491/1752] Loss: 2.9925\n",
      "Epoch [44/90], lter [501/1752] Loss: 1.9274\n",
      "Epoch [44/90], lter [511/1752] Loss: 3.3694\n",
      "Epoch [44/90], lter [521/1752] Loss: 2.4650\n",
      "Epoch [44/90], lter [531/1752] Loss: 2.0164\n",
      "Epoch [44/90], lter [541/1752] Loss: 3.1716\n",
      "Epoch [44/90], lter [551/1752] Loss: 2.4987\n",
      "Epoch [44/90], lter [561/1752] Loss: 3.2140\n",
      "Epoch [44/90], lter [571/1752] Loss: 2.6429\n",
      "Epoch [44/90], lter [581/1752] Loss: 3.5996\n",
      "Epoch [44/90], lter [591/1752] Loss: 2.9290\n",
      "Epoch [44/90], lter [601/1752] Loss: 3.7063\n",
      "Epoch [44/90], lter [611/1752] Loss: 2.2102\n",
      "Epoch [44/90], lter [621/1752] Loss: 2.5008\n",
      "Epoch [44/90], lter [631/1752] Loss: 3.2924\n",
      "Epoch [44/90], lter [641/1752] Loss: 2.8051\n",
      "Epoch [44/90], lter [651/1752] Loss: 2.6862\n",
      "Epoch [44/90], lter [661/1752] Loss: 2.5282\n",
      "Epoch [44/90], lter [671/1752] Loss: 2.9222\n",
      "Epoch [44/90], lter [681/1752] Loss: 2.5534\n",
      "Epoch [44/90], lter [691/1752] Loss: 2.8277\n",
      "Epoch [44/90], lter [701/1752] Loss: 3.0143\n",
      "Epoch [44/90], lter [711/1752] Loss: 2.8316\n",
      "Epoch [44/90], lter [721/1752] Loss: 1.7486\n",
      "Epoch [44/90], lter [731/1752] Loss: 2.3705\n",
      "Epoch [44/90], lter [741/1752] Loss: 3.3501\n",
      "Epoch [44/90], lter [751/1752] Loss: 2.9703\n",
      "Epoch [44/90], lter [761/1752] Loss: 3.1528\n",
      "Epoch [44/90], lter [771/1752] Loss: 3.7258\n",
      "Epoch [44/90], lter [781/1752] Loss: 2.8566\n",
      "Epoch [44/90], lter [791/1752] Loss: 2.8629\n",
      "Epoch [44/90], lter [801/1752] Loss: 4.2185\n",
      "Epoch [44/90], lter [811/1752] Loss: 3.3228\n",
      "Epoch [44/90], lter [821/1752] Loss: 2.8833\n",
      "Epoch [44/90], lter [831/1752] Loss: 2.8024\n",
      "Epoch [44/90], lter [841/1752] Loss: 2.7537\n",
      "Epoch [44/90], lter [851/1752] Loss: 2.6159\n",
      "Epoch [44/90], lter [861/1752] Loss: 3.4741\n",
      "Epoch [44/90], lter [871/1752] Loss: 3.2856\n",
      "Epoch [44/90], lter [881/1752] Loss: 2.7361\n",
      "Epoch [44/90], lter [891/1752] Loss: 3.5524\n",
      "Epoch [44/90], lter [901/1752] Loss: 3.5342\n",
      "Epoch [44/90], lter [911/1752] Loss: 2.3555\n",
      "Epoch [44/90], lter [921/1752] Loss: 2.2627\n",
      "Epoch [44/90], lter [931/1752] Loss: 4.2963\n",
      "Epoch [44/90], lter [941/1752] Loss: 3.3609\n",
      "Epoch [44/90], lter [951/1752] Loss: 2.9023\n",
      "Epoch [44/90], lter [961/1752] Loss: 2.8867\n",
      "Epoch [44/90], lter [971/1752] Loss: 2.8947\n",
      "Epoch [44/90], lter [981/1752] Loss: 2.1027\n",
      "Epoch [44/90], lter [991/1752] Loss: 2.3094\n",
      "Epoch [44/90], lter [1001/1752] Loss: 3.1669\n",
      "Epoch [44/90], lter [1011/1752] Loss: 2.7937\n",
      "Epoch [44/90], lter [1021/1752] Loss: 3.0042\n",
      "Epoch [44/90], lter [1031/1752] Loss: 3.0772\n",
      "Epoch [44/90], lter [1041/1752] Loss: 3.1662\n",
      "Epoch [44/90], lter [1051/1752] Loss: 3.3153\n",
      "Epoch [44/90], lter [1061/1752] Loss: 3.0359\n",
      "Epoch [44/90], lter [1071/1752] Loss: 2.0260\n",
      "Epoch [44/90], lter [1081/1752] Loss: 3.0974\n",
      "Epoch [44/90], lter [1091/1752] Loss: 3.3093\n",
      "Epoch [44/90], lter [1101/1752] Loss: 2.1531\n",
      "Epoch [44/90], lter [1111/1752] Loss: 1.9187\n",
      "Epoch [44/90], lter [1121/1752] Loss: 2.8528\n",
      "Epoch [44/90], lter [1131/1752] Loss: 2.6793\n",
      "Epoch [44/90], lter [1141/1752] Loss: 3.3964\n",
      "Epoch [44/90], lter [1151/1752] Loss: 2.2298\n",
      "Epoch [44/90], lter [1161/1752] Loss: 2.6587\n",
      "Epoch [44/90], lter [1171/1752] Loss: 3.5181\n",
      "Epoch [44/90], lter [1181/1752] Loss: 2.5577\n",
      "Epoch [44/90], lter [1191/1752] Loss: 2.2930\n",
      "Epoch [44/90], lter [1201/1752] Loss: 3.6839\n",
      "Epoch [44/90], lter [1211/1752] Loss: 3.2311\n",
      "Epoch [44/90], lter [1221/1752] Loss: 2.8989\n",
      "Epoch [44/90], lter [1231/1752] Loss: 3.3388\n",
      "Epoch [44/90], lter [1241/1752] Loss: 3.1509\n",
      "Epoch [44/90], lter [1251/1752] Loss: 2.1852\n",
      "Epoch [44/90], lter [1261/1752] Loss: 2.5754\n",
      "Epoch [44/90], lter [1271/1752] Loss: 2.9226\n",
      "Epoch [44/90], lter [1281/1752] Loss: 3.1494\n",
      "Epoch [44/90], lter [1291/1752] Loss: 2.3377\n",
      "Epoch [44/90], lter [1301/1752] Loss: 3.2202\n",
      "Epoch [44/90], lter [1311/1752] Loss: 3.5953\n",
      "Epoch [44/90], lter [1321/1752] Loss: 2.4693\n",
      "Epoch [44/90], lter [1331/1752] Loss: 3.7144\n",
      "Epoch [44/90], lter [1341/1752] Loss: 2.6350\n",
      "Epoch [44/90], lter [1351/1752] Loss: 2.7688\n",
      "Epoch [44/90], lter [1361/1752] Loss: 3.1938\n",
      "Epoch [44/90], lter [1371/1752] Loss: 2.6014\n",
      "Epoch [44/90], lter [1381/1752] Loss: 1.9336\n",
      "Epoch [44/90], lter [1391/1752] Loss: 2.8939\n",
      "Epoch [44/90], lter [1401/1752] Loss: 3.1549\n",
      "Epoch [44/90], lter [1411/1752] Loss: 3.0377\n",
      "Epoch [44/90], lter [1421/1752] Loss: 3.1780\n",
      "Epoch [44/90], lter [1431/1752] Loss: 2.5437\n",
      "Epoch [44/90], lter [1441/1752] Loss: 3.3265\n",
      "Epoch [44/90], lter [1451/1752] Loss: 1.7692\n",
      "Epoch [44/90], lter [1461/1752] Loss: 2.8443\n",
      "Epoch [44/90], lter [1471/1752] Loss: 2.7140\n",
      "Epoch [44/90], lter [1481/1752] Loss: 2.8688\n",
      "Epoch [44/90], lter [1491/1752] Loss: 4.0116\n",
      "Epoch [44/90], lter [1501/1752] Loss: 2.5380\n",
      "Epoch [44/90], lter [1511/1752] Loss: 2.7680\n",
      "Epoch [44/90], lter [1521/1752] Loss: 3.3367\n",
      "Epoch [44/90], lter [1531/1752] Loss: 3.8474\n",
      "Epoch [44/90], lter [1541/1752] Loss: 2.2242\n",
      "Epoch [44/90], lter [1551/1752] Loss: 2.6845\n",
      "Epoch [44/90], lter [1561/1752] Loss: 2.1284\n",
      "Epoch [44/90], lter [1571/1752] Loss: 3.3796\n",
      "Epoch [44/90], lter [1581/1752] Loss: 2.0152\n",
      "Epoch [44/90], lter [1591/1752] Loss: 3.6849\n",
      "Epoch [44/90], lter [1601/1752] Loss: 2.4370\n",
      "Epoch [44/90], lter [1611/1752] Loss: 3.5222\n",
      "Epoch [44/90], lter [1621/1752] Loss: 1.8745\n",
      "Epoch [44/90], lter [1631/1752] Loss: 2.4732\n",
      "Epoch [44/90], lter [1641/1752] Loss: 3.6125\n",
      "Epoch [44/90], lter [1651/1752] Loss: 3.6660\n",
      "Epoch [44/90], lter [1661/1752] Loss: 2.6652\n",
      "Epoch [44/90], lter [1671/1752] Loss: 2.0217\n",
      "Epoch [44/90], lter [1681/1752] Loss: 2.3561\n",
      "Epoch [44/90], lter [1691/1752] Loss: 2.4426\n",
      "Epoch [44/90], lter [1701/1752] Loss: 2.8678\n",
      "Epoch [44/90], lter [1711/1752] Loss: 2.9737\n",
      "Epoch [44/90], lter [1721/1752] Loss: 2.7944\n",
      "Epoch [44/90], lter [1731/1752] Loss: 2.8534\n",
      "Epoch [44/90], lter [1741/1752] Loss: 3.7547\n",
      "Epoch [44/90], lter [1751/1752] Loss: 3.1878\n",
      "Epoch:  44 | train loss : 2.7937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 44/90 [74:06:07<15:32:04, 1215.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  43 | test loss : 1.9131\n",
      "Epoch [45/90], lter [1/1752] Loss: 2.3694\n",
      "Epoch [45/90], lter [11/1752] Loss: 3.9824\n",
      "Epoch [45/90], lter [21/1752] Loss: 2.8646\n",
      "Epoch [45/90], lter [31/1752] Loss: 2.2797\n",
      "Epoch [45/90], lter [41/1752] Loss: 3.5267\n",
      "Epoch [45/90], lter [51/1752] Loss: 3.0800\n",
      "Epoch [45/90], lter [61/1752] Loss: 2.3510\n",
      "Epoch [45/90], lter [71/1752] Loss: 2.3602\n",
      "Epoch [45/90], lter [81/1752] Loss: 3.4822\n",
      "Epoch [45/90], lter [91/1752] Loss: 2.7557\n",
      "Epoch [45/90], lter [101/1752] Loss: 3.0134\n",
      "Epoch [45/90], lter [111/1752] Loss: 2.8633\n",
      "Epoch [45/90], lter [121/1752] Loss: 2.9257\n",
      "Epoch [45/90], lter [131/1752] Loss: 3.2501\n",
      "Epoch [45/90], lter [141/1752] Loss: 2.2313\n",
      "Epoch [45/90], lter [151/1752] Loss: 4.9374\n",
      "Epoch [45/90], lter [161/1752] Loss: 2.8192\n",
      "Epoch [45/90], lter [171/1752] Loss: 2.7858\n",
      "Epoch [45/90], lter [181/1752] Loss: 3.5155\n",
      "Epoch [45/90], lter [191/1752] Loss: 3.1923\n",
      "Epoch [45/90], lter [201/1752] Loss: 2.7136\n",
      "Epoch [45/90], lter [211/1752] Loss: 2.2005\n",
      "Epoch [45/90], lter [221/1752] Loss: 2.0998\n",
      "Epoch [45/90], lter [231/1752] Loss: 1.5021\n",
      "Epoch [45/90], lter [241/1752] Loss: 2.2214\n",
      "Epoch [45/90], lter [251/1752] Loss: 3.2279\n",
      "Epoch [45/90], lter [261/1752] Loss: 2.9187\n",
      "Epoch [45/90], lter [271/1752] Loss: 1.7123\n",
      "Epoch [45/90], lter [281/1752] Loss: 3.2738\n",
      "Epoch [45/90], lter [291/1752] Loss: 2.0920\n",
      "Epoch [45/90], lter [301/1752] Loss: 2.9651\n",
      "Epoch [45/90], lter [311/1752] Loss: 2.2719\n",
      "Epoch [45/90], lter [321/1752] Loss: 2.2220\n",
      "Epoch [45/90], lter [331/1752] Loss: 2.2390\n",
      "Epoch [45/90], lter [341/1752] Loss: 2.7334\n",
      "Epoch [45/90], lter [351/1752] Loss: 3.1148\n",
      "Epoch [45/90], lter [361/1752] Loss: 3.2066\n",
      "Epoch [45/90], lter [371/1752] Loss: 2.9813\n",
      "Epoch [45/90], lter [381/1752] Loss: 3.0460\n",
      "Epoch [45/90], lter [391/1752] Loss: 1.7545\n",
      "Epoch [45/90], lter [401/1752] Loss: 1.9267\n",
      "Epoch [45/90], lter [411/1752] Loss: 3.0670\n",
      "Epoch [45/90], lter [421/1752] Loss: 2.6864\n",
      "Epoch [45/90], lter [431/1752] Loss: 2.9089\n",
      "Epoch [45/90], lter [441/1752] Loss: 2.3041\n",
      "Epoch [45/90], lter [451/1752] Loss: 2.7475\n",
      "Epoch [45/90], lter [461/1752] Loss: 2.6298\n",
      "Epoch [45/90], lter [471/1752] Loss: 2.6341\n",
      "Epoch [45/90], lter [481/1752] Loss: 2.0939\n",
      "Epoch [45/90], lter [491/1752] Loss: 2.3294\n",
      "Epoch [45/90], lter [501/1752] Loss: 3.0376\n",
      "Epoch [45/90], lter [511/1752] Loss: 3.1592\n",
      "Epoch [45/90], lter [521/1752] Loss: 2.6735\n",
      "Epoch [45/90], lter [531/1752] Loss: 2.0122\n",
      "Epoch [45/90], lter [541/1752] Loss: 2.0306\n",
      "Epoch [45/90], lter [551/1752] Loss: 3.7181\n",
      "Epoch [45/90], lter [561/1752] Loss: 2.7665\n",
      "Epoch [45/90], lter [571/1752] Loss: 4.2197\n",
      "Epoch [45/90], lter [581/1752] Loss: 3.3219\n",
      "Epoch [45/90], lter [591/1752] Loss: 3.5971\n",
      "Epoch [45/90], lter [601/1752] Loss: 1.9731\n",
      "Epoch [45/90], lter [611/1752] Loss: 3.3739\n",
      "Epoch [45/90], lter [621/1752] Loss: 2.8778\n",
      "Epoch [45/90], lter [631/1752] Loss: 2.6308\n",
      "Epoch [45/90], lter [641/1752] Loss: 2.9247\n",
      "Epoch [45/90], lter [651/1752] Loss: 2.6812\n",
      "Epoch [45/90], lter [661/1752] Loss: 2.6686\n",
      "Epoch [45/90], lter [671/1752] Loss: 3.6089\n",
      "Epoch [45/90], lter [681/1752] Loss: 3.4016\n",
      "Epoch [45/90], lter [691/1752] Loss: 2.4858\n",
      "Epoch [45/90], lter [701/1752] Loss: 2.4565\n",
      "Epoch [45/90], lter [711/1752] Loss: 2.0195\n",
      "Epoch [45/90], lter [721/1752] Loss: 2.4905\n",
      "Epoch [45/90], lter [731/1752] Loss: 2.7640\n",
      "Epoch [45/90], lter [741/1752] Loss: 2.7535\n",
      "Epoch [45/90], lter [751/1752] Loss: 1.8873\n",
      "Epoch [45/90], lter [761/1752] Loss: 1.9535\n",
      "Epoch [45/90], lter [771/1752] Loss: 3.0635\n",
      "Epoch [45/90], lter [781/1752] Loss: 2.3641\n",
      "Epoch [45/90], lter [791/1752] Loss: 2.7499\n",
      "Epoch [45/90], lter [801/1752] Loss: 2.2877\n",
      "Epoch [45/90], lter [811/1752] Loss: 3.5794\n",
      "Epoch [45/90], lter [821/1752] Loss: 3.3569\n",
      "Epoch [45/90], lter [831/1752] Loss: 2.4219\n",
      "Epoch [45/90], lter [841/1752] Loss: 3.0134\n",
      "Epoch [45/90], lter [851/1752] Loss: 2.9711\n",
      "Epoch [45/90], lter [861/1752] Loss: 2.3598\n",
      "Epoch [45/90], lter [871/1752] Loss: 3.0771\n",
      "Epoch [45/90], lter [881/1752] Loss: 3.0084\n",
      "Epoch [45/90], lter [891/1752] Loss: 2.7234\n",
      "Epoch [45/90], lter [901/1752] Loss: 2.8800\n",
      "Epoch [45/90], lter [911/1752] Loss: 3.2621\n",
      "Epoch [45/90], lter [921/1752] Loss: 2.9213\n",
      "Epoch [45/90], lter [931/1752] Loss: 2.6489\n",
      "Epoch [45/90], lter [941/1752] Loss: 2.3027\n",
      "Epoch [45/90], lter [951/1752] Loss: 4.5170\n",
      "Epoch [45/90], lter [961/1752] Loss: 3.2088\n",
      "Epoch [45/90], lter [971/1752] Loss: 3.1741\n",
      "Epoch [45/90], lter [981/1752] Loss: 2.2611\n",
      "Epoch [45/90], lter [991/1752] Loss: 2.3358\n",
      "Epoch [45/90], lter [1001/1752] Loss: 2.1294\n",
      "Epoch [45/90], lter [1011/1752] Loss: 4.1917\n",
      "Epoch [45/90], lter [1021/1752] Loss: 2.2966\n",
      "Epoch [45/90], lter [1031/1752] Loss: 1.7518\n",
      "Epoch [45/90], lter [1041/1752] Loss: 2.0569\n",
      "Epoch [45/90], lter [1051/1752] Loss: 1.4547\n",
      "Epoch [45/90], lter [1061/1752] Loss: 3.1299\n",
      "Epoch [45/90], lter [1071/1752] Loss: 3.8301\n",
      "Epoch [45/90], lter [1081/1752] Loss: 3.1294\n",
      "Epoch [45/90], lter [1091/1752] Loss: 4.2718\n",
      "Epoch [45/90], lter [1101/1752] Loss: 4.2774\n",
      "Epoch [45/90], lter [1111/1752] Loss: 3.0627\n",
      "Epoch [45/90], lter [1121/1752] Loss: 2.0499\n",
      "Epoch [45/90], lter [1131/1752] Loss: 3.9930\n",
      "Epoch [45/90], lter [1141/1752] Loss: 2.2626\n",
      "Epoch [45/90], lter [1151/1752] Loss: 2.3380\n",
      "Epoch [45/90], lter [1161/1752] Loss: 1.8432\n",
      "Epoch [45/90], lter [1171/1752] Loss: 3.0085\n",
      "Epoch [45/90], lter [1181/1752] Loss: 2.2799\n",
      "Epoch [45/90], lter [1191/1752] Loss: 3.8289\n",
      "Epoch [45/90], lter [1201/1752] Loss: 2.4289\n",
      "Epoch [45/90], lter [1211/1752] Loss: 1.8714\n",
      "Epoch [45/90], lter [1221/1752] Loss: 2.5892\n",
      "Epoch [45/90], lter [1231/1752] Loss: 2.1636\n",
      "Epoch [45/90], lter [1241/1752] Loss: 2.4970\n",
      "Epoch [45/90], lter [1251/1752] Loss: 2.8324\n",
      "Epoch [45/90], lter [1261/1752] Loss: 1.7288\n",
      "Epoch [45/90], lter [1271/1752] Loss: 3.5132\n",
      "Epoch [45/90], lter [1281/1752] Loss: 2.0970\n",
      "Epoch [45/90], lter [1291/1752] Loss: 2.0479\n",
      "Epoch [45/90], lter [1301/1752] Loss: 2.9331\n",
      "Epoch [45/90], lter [1311/1752] Loss: 2.2097\n",
      "Epoch [45/90], lter [1321/1752] Loss: 2.6777\n",
      "Epoch [45/90], lter [1331/1752] Loss: 3.7737\n",
      "Epoch [45/90], lter [1341/1752] Loss: 3.4643\n",
      "Epoch [45/90], lter [1351/1752] Loss: 1.9737\n",
      "Epoch [45/90], lter [1361/1752] Loss: 1.7262\n",
      "Epoch [45/90], lter [1371/1752] Loss: 2.3630\n",
      "Epoch [45/90], lter [1381/1752] Loss: 2.4407\n",
      "Epoch [45/90], lter [1391/1752] Loss: 2.6170\n",
      "Epoch [45/90], lter [1401/1752] Loss: 2.7712\n",
      "Epoch [45/90], lter [1411/1752] Loss: 2.9751\n",
      "Epoch [45/90], lter [1421/1752] Loss: 2.6924\n",
      "Epoch [45/90], lter [1431/1752] Loss: 2.3950\n",
      "Epoch [45/90], lter [1441/1752] Loss: 3.6219\n",
      "Epoch [45/90], lter [1451/1752] Loss: 2.6677\n",
      "Epoch [45/90], lter [1461/1752] Loss: 3.4015\n",
      "Epoch [45/90], lter [1471/1752] Loss: 3.5374\n",
      "Epoch [45/90], lter [1481/1752] Loss: 2.2051\n",
      "Epoch [45/90], lter [1491/1752] Loss: 2.9166\n",
      "Epoch [45/90], lter [1501/1752] Loss: 3.0320\n",
      "Epoch [45/90], lter [1511/1752] Loss: 3.2389\n",
      "Epoch [45/90], lter [1521/1752] Loss: 3.1417\n",
      "Epoch [45/90], lter [1531/1752] Loss: 3.9895\n",
      "Epoch [45/90], lter [1541/1752] Loss: 2.8306\n",
      "Epoch [45/90], lter [1551/1752] Loss: 3.5984\n",
      "Epoch [45/90], lter [1561/1752] Loss: 2.6396\n",
      "Epoch [45/90], lter [1571/1752] Loss: 2.9626\n",
      "Epoch [45/90], lter [1581/1752] Loss: 2.3199\n",
      "Epoch [45/90], lter [1591/1752] Loss: 2.8552\n",
      "Epoch [45/90], lter [1601/1752] Loss: 2.5920\n",
      "Epoch [45/90], lter [1611/1752] Loss: 2.0032\n",
      "Epoch [45/90], lter [1621/1752] Loss: 3.7296\n",
      "Epoch [45/90], lter [1631/1752] Loss: 2.7087\n",
      "Epoch [45/90], lter [1641/1752] Loss: 2.9024\n",
      "Epoch [45/90], lter [1651/1752] Loss: 3.1939\n",
      "Epoch [45/90], lter [1661/1752] Loss: 2.5658\n",
      "Epoch [45/90], lter [1671/1752] Loss: 2.3693\n",
      "Epoch [45/90], lter [1681/1752] Loss: 2.4848\n",
      "Epoch [45/90], lter [1691/1752] Loss: 3.1221\n",
      "Epoch [45/90], lter [1701/1752] Loss: 2.8844\n",
      "Epoch [45/90], lter [1711/1752] Loss: 2.2458\n",
      "Epoch [45/90], lter [1721/1752] Loss: 2.9209\n",
      "Epoch [45/90], lter [1731/1752] Loss: 2.5045\n",
      "Epoch [45/90], lter [1741/1752] Loss: 3.2974\n",
      "Epoch [45/90], lter [1751/1752] Loss: 2.4354\n",
      "Epoch:  45 | train loss : 2.7466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 45/90 [74:26:26<15:12:29, 1216.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  44 | test loss : 1.9656\n",
      "Epoch [46/90], lter [1/1752] Loss: 1.8034\n",
      "Epoch [46/90], lter [11/1752] Loss: 2.7590\n",
      "Epoch [46/90], lter [21/1752] Loss: 2.4434\n",
      "Epoch [46/90], lter [31/1752] Loss: 2.4390\n",
      "Epoch [46/90], lter [41/1752] Loss: 3.4133\n",
      "Epoch [46/90], lter [51/1752] Loss: 3.8377\n",
      "Epoch [46/90], lter [61/1752] Loss: 2.2167\n",
      "Epoch [46/90], lter [71/1752] Loss: 1.9916\n",
      "Epoch [46/90], lter [81/1752] Loss: 2.8365\n",
      "Epoch [46/90], lter [91/1752] Loss: 4.3888\n",
      "Epoch [46/90], lter [101/1752] Loss: 3.5610\n",
      "Epoch [46/90], lter [111/1752] Loss: 3.4517\n",
      "Epoch [46/90], lter [121/1752] Loss: 3.6253\n",
      "Epoch [46/90], lter [131/1752] Loss: 2.6875\n",
      "Epoch [46/90], lter [141/1752] Loss: 2.5430\n",
      "Epoch [46/90], lter [151/1752] Loss: 2.7000\n",
      "Epoch [46/90], lter [161/1752] Loss: 2.4459\n",
      "Epoch [46/90], lter [171/1752] Loss: 2.9542\n",
      "Epoch [46/90], lter [181/1752] Loss: 1.2129\n",
      "Epoch [46/90], lter [191/1752] Loss: 3.4487\n",
      "Epoch [46/90], lter [201/1752] Loss: 2.5551\n",
      "Epoch [46/90], lter [211/1752] Loss: 1.8531\n",
      "Epoch [46/90], lter [221/1752] Loss: 3.1031\n",
      "Epoch [46/90], lter [231/1752] Loss: 2.4964\n",
      "Epoch [46/90], lter [241/1752] Loss: 2.2331\n",
      "Epoch [46/90], lter [251/1752] Loss: 1.7878\n",
      "Epoch [46/90], lter [261/1752] Loss: 3.5139\n",
      "Epoch [46/90], lter [271/1752] Loss: 2.8436\n",
      "Epoch [46/90], lter [281/1752] Loss: 2.7331\n",
      "Epoch [46/90], lter [291/1752] Loss: 2.6302\n",
      "Epoch [46/90], lter [301/1752] Loss: 2.7290\n",
      "Epoch [46/90], lter [311/1752] Loss: 2.5240\n",
      "Epoch [46/90], lter [321/1752] Loss: 3.0107\n",
      "Epoch [46/90], lter [331/1752] Loss: 2.6776\n",
      "Epoch [46/90], lter [341/1752] Loss: 2.5898\n",
      "Epoch [46/90], lter [351/1752] Loss: 1.9228\n",
      "Epoch [46/90], lter [361/1752] Loss: 2.5517\n",
      "Epoch [46/90], lter [371/1752] Loss: 3.0954\n",
      "Epoch [46/90], lter [381/1752] Loss: 3.1759\n",
      "Epoch [46/90], lter [391/1752] Loss: 2.6154\n",
      "Epoch [46/90], lter [401/1752] Loss: 2.2402\n",
      "Epoch [46/90], lter [411/1752] Loss: 1.8645\n",
      "Epoch [46/90], lter [421/1752] Loss: 2.0432\n",
      "Epoch [46/90], lter [431/1752] Loss: 2.6889\n",
      "Epoch [46/90], lter [441/1752] Loss: 1.8981\n",
      "Epoch [46/90], lter [451/1752] Loss: 2.1787\n",
      "Epoch [46/90], lter [461/1752] Loss: 3.5488\n",
      "Epoch [46/90], lter [471/1752] Loss: 3.4534\n",
      "Epoch [46/90], lter [481/1752] Loss: 2.6474\n",
      "Epoch [46/90], lter [491/1752] Loss: 2.4062\n",
      "Epoch [46/90], lter [501/1752] Loss: 2.1826\n",
      "Epoch [46/90], lter [511/1752] Loss: 3.7218\n",
      "Epoch [46/90], lter [521/1752] Loss: 2.4641\n",
      "Epoch [46/90], lter [531/1752] Loss: 2.1008\n",
      "Epoch [46/90], lter [541/1752] Loss: 3.0419\n",
      "Epoch [46/90], lter [551/1752] Loss: 2.6676\n",
      "Epoch [46/90], lter [561/1752] Loss: 3.1139\n",
      "Epoch [46/90], lter [571/1752] Loss: 3.2008\n",
      "Epoch [46/90], lter [581/1752] Loss: 3.1247\n",
      "Epoch [46/90], lter [591/1752] Loss: 2.2252\n",
      "Epoch [46/90], lter [601/1752] Loss: 1.5796\n",
      "Epoch [46/90], lter [611/1752] Loss: 3.4068\n",
      "Epoch [46/90], lter [621/1752] Loss: 2.9094\n",
      "Epoch [46/90], lter [631/1752] Loss: 3.5033\n",
      "Epoch [46/90], lter [641/1752] Loss: 2.9129\n",
      "Epoch [46/90], lter [651/1752] Loss: 2.5537\n",
      "Epoch [46/90], lter [661/1752] Loss: 2.9171\n",
      "Epoch [46/90], lter [671/1752] Loss: 1.3938\n",
      "Epoch [46/90], lter [681/1752] Loss: 1.9066\n",
      "Epoch [46/90], lter [691/1752] Loss: 2.2629\n",
      "Epoch [46/90], lter [701/1752] Loss: 2.4943\n",
      "Epoch [46/90], lter [711/1752] Loss: 2.8204\n",
      "Epoch [46/90], lter [721/1752] Loss: 2.2544\n",
      "Epoch [46/90], lter [731/1752] Loss: 3.1170\n",
      "Epoch [46/90], lter [741/1752] Loss: 2.0931\n",
      "Epoch [46/90], lter [751/1752] Loss: 3.0468\n",
      "Epoch [46/90], lter [761/1752] Loss: 2.0778\n",
      "Epoch [46/90], lter [771/1752] Loss: 3.1749\n",
      "Epoch [46/90], lter [781/1752] Loss: 2.5147\n",
      "Epoch [46/90], lter [791/1752] Loss: 2.5672\n",
      "Epoch [46/90], lter [801/1752] Loss: 2.4109\n",
      "Epoch [46/90], lter [811/1752] Loss: 2.8516\n",
      "Epoch [46/90], lter [821/1752] Loss: 3.7266\n",
      "Epoch [46/90], lter [831/1752] Loss: 2.6519\n",
      "Epoch [46/90], lter [841/1752] Loss: 3.3660\n",
      "Epoch [46/90], lter [851/1752] Loss: 2.7378\n",
      "Epoch [46/90], lter [861/1752] Loss: 2.3860\n",
      "Epoch [46/90], lter [871/1752] Loss: 2.9454\n",
      "Epoch [46/90], lter [881/1752] Loss: 3.6360\n",
      "Epoch [46/90], lter [891/1752] Loss: 2.9239\n",
      "Epoch [46/90], lter [901/1752] Loss: 2.7707\n",
      "Epoch [46/90], lter [911/1752] Loss: 1.1347\n",
      "Epoch [46/90], lter [921/1752] Loss: 2.9724\n",
      "Epoch [46/90], lter [931/1752] Loss: 1.6062\n",
      "Epoch [46/90], lter [941/1752] Loss: 3.0464\n",
      "Epoch [46/90], lter [951/1752] Loss: 2.7571\n",
      "Epoch [46/90], lter [961/1752] Loss: 2.5236\n",
      "Epoch [46/90], lter [971/1752] Loss: 2.8134\n",
      "Epoch [46/90], lter [981/1752] Loss: 3.0258\n",
      "Epoch [46/90], lter [991/1752] Loss: 2.5190\n",
      "Epoch [46/90], lter [1001/1752] Loss: 3.1716\n",
      "Epoch [46/90], lter [1011/1752] Loss: 1.8126\n",
      "Epoch [46/90], lter [1021/1752] Loss: 2.4789\n",
      "Epoch [46/90], lter [1031/1752] Loss: 2.2507\n",
      "Epoch [46/90], lter [1041/1752] Loss: 2.0694\n",
      "Epoch [46/90], lter [1051/1752] Loss: 3.9966\n",
      "Epoch [46/90], lter [1061/1752] Loss: 3.4114\n",
      "Epoch [46/90], lter [1071/1752] Loss: 2.4493\n",
      "Epoch [46/90], lter [1081/1752] Loss: 3.3640\n",
      "Epoch [46/90], lter [1091/1752] Loss: 2.2581\n",
      "Epoch [46/90], lter [1101/1752] Loss: 1.7903\n",
      "Epoch [46/90], lter [1111/1752] Loss: 2.2114\n",
      "Epoch [46/90], lter [1121/1752] Loss: 2.7991\n",
      "Epoch [46/90], lter [1131/1752] Loss: 2.2864\n",
      "Epoch [46/90], lter [1141/1752] Loss: 2.5894\n",
      "Epoch [46/90], lter [1151/1752] Loss: 3.9654\n",
      "Epoch [46/90], lter [1161/1752] Loss: 2.4300\n",
      "Epoch [46/90], lter [1171/1752] Loss: 2.4376\n",
      "Epoch [46/90], lter [1181/1752] Loss: 2.0133\n",
      "Epoch [46/90], lter [1191/1752] Loss: 2.3680\n",
      "Epoch [46/90], lter [1201/1752] Loss: 3.0903\n",
      "Epoch [46/90], lter [1211/1752] Loss: 2.5367\n",
      "Epoch [46/90], lter [1221/1752] Loss: 2.3177\n",
      "Epoch [46/90], lter [1231/1752] Loss: 2.7734\n",
      "Epoch [46/90], lter [1241/1752] Loss: 2.4910\n",
      "Epoch [46/90], lter [1251/1752] Loss: 4.0338\n",
      "Epoch [46/90], lter [1261/1752] Loss: 1.9192\n",
      "Epoch [46/90], lter [1271/1752] Loss: 2.8574\n",
      "Epoch [46/90], lter [1281/1752] Loss: 2.0452\n",
      "Epoch [46/90], lter [1291/1752] Loss: 2.1186\n",
      "Epoch [46/90], lter [1301/1752] Loss: 3.4325\n",
      "Epoch [46/90], lter [1311/1752] Loss: 3.1153\n",
      "Epoch [46/90], lter [1321/1752] Loss: 2.9867\n",
      "Epoch [46/90], lter [1331/1752] Loss: 2.1181\n",
      "Epoch [46/90], lter [1341/1752] Loss: 2.1769\n",
      "Epoch [46/90], lter [1351/1752] Loss: 2.8410\n",
      "Epoch [46/90], lter [1361/1752] Loss: 2.7219\n",
      "Epoch [46/90], lter [1371/1752] Loss: 2.0736\n",
      "Epoch [46/90], lter [1381/1752] Loss: 3.2208\n",
      "Epoch [46/90], lter [1391/1752] Loss: 2.7737\n",
      "Epoch [46/90], lter [1401/1752] Loss: 3.4411\n",
      "Epoch [46/90], lter [1411/1752] Loss: 2.7954\n",
      "Epoch [46/90], lter [1421/1752] Loss: 3.2701\n",
      "Epoch [46/90], lter [1431/1752] Loss: 2.9841\n",
      "Epoch [46/90], lter [1441/1752] Loss: 3.4632\n",
      "Epoch [46/90], lter [1451/1752] Loss: 3.0324\n",
      "Epoch [46/90], lter [1461/1752] Loss: 2.7160\n",
      "Epoch [46/90], lter [1471/1752] Loss: 2.4908\n",
      "Epoch [46/90], lter [1481/1752] Loss: 3.7793\n",
      "Epoch [46/90], lter [1491/1752] Loss: 2.6861\n",
      "Epoch [46/90], lter [1501/1752] Loss: 2.8201\n",
      "Epoch [46/90], lter [1511/1752] Loss: 2.7846\n",
      "Epoch [46/90], lter [1521/1752] Loss: 2.9409\n",
      "Epoch [46/90], lter [1531/1752] Loss: 2.6070\n",
      "Epoch [46/90], lter [1541/1752] Loss: 4.6998\n",
      "Epoch [46/90], lter [1551/1752] Loss: 2.8458\n",
      "Epoch [46/90], lter [1561/1752] Loss: 2.7456\n",
      "Epoch [46/90], lter [1571/1752] Loss: 3.2268\n",
      "Epoch [46/90], lter [1581/1752] Loss: 2.5559\n",
      "Epoch [46/90], lter [1591/1752] Loss: 2.4079\n",
      "Epoch [46/90], lter [1601/1752] Loss: 2.7571\n",
      "Epoch [46/90], lter [1611/1752] Loss: 2.5950\n",
      "Epoch [46/90], lter [1621/1752] Loss: 2.6370\n",
      "Epoch [46/90], lter [1631/1752] Loss: 1.6666\n",
      "Epoch [46/90], lter [1641/1752] Loss: 3.0254\n",
      "Epoch [46/90], lter [1651/1752] Loss: 2.4023\n",
      "Epoch [46/90], lter [1661/1752] Loss: 3.2975\n",
      "Epoch [46/90], lter [1671/1752] Loss: 3.3856\n",
      "Epoch [46/90], lter [1681/1752] Loss: 4.0811\n",
      "Epoch [46/90], lter [1691/1752] Loss: 3.2625\n",
      "Epoch [46/90], lter [1701/1752] Loss: 2.9686\n",
      "Epoch [46/90], lter [1711/1752] Loss: 3.0389\n",
      "Epoch [46/90], lter [1721/1752] Loss: 3.3753\n",
      "Epoch [46/90], lter [1731/1752] Loss: 2.0658\n",
      "Epoch [46/90], lter [1741/1752] Loss: 3.6215\n",
      "Epoch [46/90], lter [1751/1752] Loss: 2.7118\n",
      "Epoch:  46 | train loss : 2.7227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 46/90 [74:46:52<14:54:19, 1219.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  45 | test loss : 2.1375\n",
      "Epoch [47/90], lter [1/1752] Loss: 2.5110\n",
      "Epoch [47/90], lter [11/1752] Loss: 2.6171\n",
      "Epoch [47/90], lter [21/1752] Loss: 2.1152\n",
      "Epoch [47/90], lter [31/1752] Loss: 2.5730\n",
      "Epoch [47/90], lter [41/1752] Loss: 2.2246\n",
      "Epoch [47/90], lter [51/1752] Loss: 1.7122\n",
      "Epoch [47/90], lter [61/1752] Loss: 1.9670\n",
      "Epoch [47/90], lter [71/1752] Loss: 2.6885\n",
      "Epoch [47/90], lter [81/1752] Loss: 2.5904\n",
      "Epoch [47/90], lter [91/1752] Loss: 3.4144\n",
      "Epoch [47/90], lter [101/1752] Loss: 4.0808\n",
      "Epoch [47/90], lter [111/1752] Loss: 6.0013\n",
      "Epoch [47/90], lter [121/1752] Loss: 3.2715\n",
      "Epoch [47/90], lter [131/1752] Loss: 3.3612\n",
      "Epoch [47/90], lter [141/1752] Loss: 1.8926\n",
      "Epoch [47/90], lter [151/1752] Loss: 1.5991\n",
      "Epoch [47/90], lter [161/1752] Loss: 4.2544\n",
      "Epoch [47/90], lter [171/1752] Loss: 2.9595\n",
      "Epoch [47/90], lter [181/1752] Loss: 3.8906\n",
      "Epoch [47/90], lter [191/1752] Loss: 4.6406\n",
      "Epoch [47/90], lter [201/1752] Loss: 3.1447\n",
      "Epoch [47/90], lter [211/1752] Loss: 3.3092\n",
      "Epoch [47/90], lter [221/1752] Loss: 2.9452\n",
      "Epoch [47/90], lter [231/1752] Loss: 2.6463\n",
      "Epoch [47/90], lter [241/1752] Loss: 3.1189\n",
      "Epoch [47/90], lter [251/1752] Loss: 2.1118\n",
      "Epoch [47/90], lter [261/1752] Loss: 3.2437\n",
      "Epoch [47/90], lter [271/1752] Loss: 3.4368\n",
      "Epoch [47/90], lter [281/1752] Loss: 2.8540\n",
      "Epoch [47/90], lter [291/1752] Loss: 2.7859\n",
      "Epoch [47/90], lter [301/1752] Loss: 2.0016\n",
      "Epoch [47/90], lter [311/1752] Loss: 3.4302\n",
      "Epoch [47/90], lter [321/1752] Loss: 2.0014\n",
      "Epoch [47/90], lter [331/1752] Loss: 2.2513\n",
      "Epoch [47/90], lter [341/1752] Loss: 2.5876\n",
      "Epoch [47/90], lter [351/1752] Loss: 2.1960\n",
      "Epoch [47/90], lter [361/1752] Loss: 2.1226\n",
      "Epoch [47/90], lter [371/1752] Loss: 3.4598\n",
      "Epoch [47/90], lter [381/1752] Loss: 2.2304\n",
      "Epoch [47/90], lter [391/1752] Loss: 2.2396\n",
      "Epoch [47/90], lter [401/1752] Loss: 2.9842\n",
      "Epoch [47/90], lter [411/1752] Loss: 2.3615\n",
      "Epoch [47/90], lter [421/1752] Loss: 3.8979\n",
      "Epoch [47/90], lter [431/1752] Loss: 2.9324\n",
      "Epoch [47/90], lter [441/1752] Loss: 2.6717\n",
      "Epoch [47/90], lter [451/1752] Loss: 1.5409\n",
      "Epoch [47/90], lter [461/1752] Loss: 2.9164\n",
      "Epoch [47/90], lter [471/1752] Loss: 3.0530\n",
      "Epoch [47/90], lter [481/1752] Loss: 1.4236\n",
      "Epoch [47/90], lter [491/1752] Loss: 2.6180\n",
      "Epoch [47/90], lter [501/1752] Loss: 3.8690\n",
      "Epoch [47/90], lter [511/1752] Loss: 3.0794\n",
      "Epoch [47/90], lter [521/1752] Loss: 3.1104\n",
      "Epoch [47/90], lter [531/1752] Loss: 2.6506\n",
      "Epoch [47/90], lter [541/1752] Loss: 3.3345\n",
      "Epoch [47/90], lter [551/1752] Loss: 3.5833\n",
      "Epoch [47/90], lter [561/1752] Loss: 3.5923\n",
      "Epoch [47/90], lter [571/1752] Loss: 2.8357\n",
      "Epoch [47/90], lter [581/1752] Loss: 2.1944\n",
      "Epoch [47/90], lter [591/1752] Loss: 3.0099\n",
      "Epoch [47/90], lter [601/1752] Loss: 2.7883\n",
      "Epoch [47/90], lter [611/1752] Loss: 2.7786\n",
      "Epoch [47/90], lter [621/1752] Loss: 3.1220\n",
      "Epoch [47/90], lter [631/1752] Loss: 2.3441\n",
      "Epoch [47/90], lter [641/1752] Loss: 3.8002\n",
      "Epoch [47/90], lter [651/1752] Loss: 2.1104\n",
      "Epoch [47/90], lter [661/1752] Loss: 2.7281\n",
      "Epoch [47/90], lter [671/1752] Loss: 2.1428\n",
      "Epoch [47/90], lter [681/1752] Loss: 2.4348\n",
      "Epoch [47/90], lter [691/1752] Loss: 2.7053\n",
      "Epoch [47/90], lter [701/1752] Loss: 2.5130\n",
      "Epoch [47/90], lter [711/1752] Loss: 2.2834\n",
      "Epoch [47/90], lter [721/1752] Loss: 2.2662\n",
      "Epoch [47/90], lter [731/1752] Loss: 2.0944\n",
      "Epoch [47/90], lter [741/1752] Loss: 3.1059\n",
      "Epoch [47/90], lter [751/1752] Loss: 3.3152\n",
      "Epoch [47/90], lter [761/1752] Loss: 3.0003\n",
      "Epoch [47/90], lter [771/1752] Loss: 2.4213\n",
      "Epoch [47/90], lter [781/1752] Loss: 2.0477\n",
      "Epoch [47/90], lter [791/1752] Loss: 3.2381\n",
      "Epoch [47/90], lter [801/1752] Loss: 2.0550\n",
      "Epoch [47/90], lter [811/1752] Loss: 3.3616\n",
      "Epoch [47/90], lter [821/1752] Loss: 2.6057\n",
      "Epoch [47/90], lter [831/1752] Loss: 2.7628\n",
      "Epoch [47/90], lter [841/1752] Loss: 2.6882\n",
      "Epoch [47/90], lter [851/1752] Loss: 2.8071\n",
      "Epoch [47/90], lter [861/1752] Loss: 2.2417\n",
      "Epoch [47/90], lter [871/1752] Loss: 3.4071\n",
      "Epoch [47/90], lter [881/1752] Loss: 2.5070\n",
      "Epoch [47/90], lter [891/1752] Loss: 2.6203\n",
      "Epoch [47/90], lter [901/1752] Loss: 1.7969\n",
      "Epoch [47/90], lter [911/1752] Loss: 2.6649\n",
      "Epoch [47/90], lter [921/1752] Loss: 3.9011\n",
      "Epoch [47/90], lter [931/1752] Loss: 2.5486\n",
      "Epoch [47/90], lter [941/1752] Loss: 2.1253\n",
      "Epoch [47/90], lter [951/1752] Loss: 3.0468\n",
      "Epoch [47/90], lter [961/1752] Loss: 2.9729\n",
      "Epoch [47/90], lter [971/1752] Loss: 3.5495\n",
      "Epoch [47/90], lter [981/1752] Loss: 2.6707\n",
      "Epoch [47/90], lter [991/1752] Loss: 2.7876\n",
      "Epoch [47/90], lter [1001/1752] Loss: 2.9220\n",
      "Epoch [47/90], lter [1011/1752] Loss: 2.3976\n",
      "Epoch [47/90], lter [1021/1752] Loss: 2.1542\n",
      "Epoch [47/90], lter [1031/1752] Loss: 3.6417\n",
      "Epoch [47/90], lter [1041/1752] Loss: 1.7742\n",
      "Epoch [47/90], lter [1051/1752] Loss: 2.1991\n",
      "Epoch [47/90], lter [1061/1752] Loss: 3.0955\n",
      "Epoch [47/90], lter [1071/1752] Loss: 2.2767\n",
      "Epoch [47/90], lter [1081/1752] Loss: 2.3143\n",
      "Epoch [47/90], lter [1091/1752] Loss: 2.5413\n",
      "Epoch [47/90], lter [1101/1752] Loss: 3.0083\n",
      "Epoch [47/90], lter [1111/1752] Loss: 2.4376\n",
      "Epoch [47/90], lter [1121/1752] Loss: 1.7223\n",
      "Epoch [47/90], lter [1131/1752] Loss: 2.6779\n",
      "Epoch [47/90], lter [1141/1752] Loss: 2.7693\n",
      "Epoch [47/90], lter [1151/1752] Loss: 3.1670\n",
      "Epoch [47/90], lter [1161/1752] Loss: 2.7375\n",
      "Epoch [47/90], lter [1171/1752] Loss: 3.1303\n",
      "Epoch [47/90], lter [1181/1752] Loss: 3.1902\n",
      "Epoch [47/90], lter [1191/1752] Loss: 3.0600\n",
      "Epoch [47/90], lter [1201/1752] Loss: 3.1997\n",
      "Epoch [47/90], lter [1211/1752] Loss: 3.5853\n",
      "Epoch [47/90], lter [1221/1752] Loss: 2.4862\n",
      "Epoch [47/90], lter [1231/1752] Loss: 3.0754\n",
      "Epoch [47/90], lter [1241/1752] Loss: 2.3220\n",
      "Epoch [47/90], lter [1251/1752] Loss: 2.4296\n",
      "Epoch [47/90], lter [1261/1752] Loss: 3.7692\n",
      "Epoch [47/90], lter [1271/1752] Loss: 2.7795\n",
      "Epoch [47/90], lter [1281/1752] Loss: 2.6953\n",
      "Epoch [47/90], lter [1291/1752] Loss: 2.3768\n",
      "Epoch [47/90], lter [1301/1752] Loss: 2.0710\n",
      "Epoch [47/90], lter [1311/1752] Loss: 3.2482\n",
      "Epoch [47/90], lter [1321/1752] Loss: 2.8745\n",
      "Epoch [47/90], lter [1331/1752] Loss: 2.8192\n",
      "Epoch [47/90], lter [1341/1752] Loss: 3.0972\n",
      "Epoch [47/90], lter [1351/1752] Loss: 2.9813\n",
      "Epoch [47/90], lter [1361/1752] Loss: 2.6970\n",
      "Epoch [47/90], lter [1371/1752] Loss: 2.3750\n",
      "Epoch [47/90], lter [1381/1752] Loss: 2.5511\n",
      "Epoch [47/90], lter [1391/1752] Loss: 2.1360\n",
      "Epoch [47/90], lter [1401/1752] Loss: 2.4668\n",
      "Epoch [47/90], lter [1411/1752] Loss: 2.9011\n",
      "Epoch [47/90], lter [1421/1752] Loss: 2.6826\n",
      "Epoch [47/90], lter [1431/1752] Loss: 2.6122\n",
      "Epoch [47/90], lter [1441/1752] Loss: 2.2779\n",
      "Epoch [47/90], lter [1451/1752] Loss: 3.2436\n",
      "Epoch [47/90], lter [1461/1752] Loss: 2.4659\n",
      "Epoch [47/90], lter [1471/1752] Loss: 2.3571\n",
      "Epoch [47/90], lter [1481/1752] Loss: 2.2239\n",
      "Epoch [47/90], lter [1491/1752] Loss: 2.3403\n",
      "Epoch [47/90], lter [1501/1752] Loss: 1.7494\n",
      "Epoch [47/90], lter [1511/1752] Loss: 1.2394\n",
      "Epoch [47/90], lter [1521/1752] Loss: 2.8317\n",
      "Epoch [47/90], lter [1531/1752] Loss: 2.6151\n",
      "Epoch [47/90], lter [1541/1752] Loss: 3.5770\n",
      "Epoch [47/90], lter [1551/1752] Loss: 3.2096\n",
      "Epoch [47/90], lter [1561/1752] Loss: 2.1003\n",
      "Epoch [47/90], lter [1571/1752] Loss: 2.0641\n",
      "Epoch [47/90], lter [1581/1752] Loss: 1.9676\n",
      "Epoch [47/90], lter [1591/1752] Loss: 2.2133\n",
      "Epoch [47/90], lter [1601/1752] Loss: 3.0175\n",
      "Epoch [47/90], lter [1611/1752] Loss: 2.9606\n",
      "Epoch [47/90], lter [1621/1752] Loss: 2.3317\n",
      "Epoch [47/90], lter [1631/1752] Loss: 2.8711\n",
      "Epoch [47/90], lter [1641/1752] Loss: 3.6926\n",
      "Epoch [47/90], lter [1651/1752] Loss: 3.0945\n",
      "Epoch [47/90], lter [1661/1752] Loss: 1.5467\n",
      "Epoch [47/90], lter [1671/1752] Loss: 2.6158\n",
      "Epoch [47/90], lter [1681/1752] Loss: 3.1708\n",
      "Epoch [47/90], lter [1691/1752] Loss: 2.5020\n",
      "Epoch [47/90], lter [1701/1752] Loss: 3.0052\n",
      "Epoch [47/90], lter [1711/1752] Loss: 2.9114\n",
      "Epoch [47/90], lter [1721/1752] Loss: 2.8772\n",
      "Epoch [47/90], lter [1731/1752] Loss: 3.0033\n",
      "Epoch [47/90], lter [1741/1752] Loss: 2.6520\n",
      "Epoch [47/90], lter [1751/1752] Loss: 1.9084\n",
      "Epoch:  47 | train loss : 2.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 47/90 [75:07:13<14:34:22, 1220.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  46 | test loss : 1.9438\n",
      "Epoch [48/90], lter [1/1752] Loss: 2.0990\n",
      "Epoch [48/90], lter [11/1752] Loss: 2.6792\n",
      "Epoch [48/90], lter [21/1752] Loss: 2.1611\n",
      "Epoch [48/90], lter [31/1752] Loss: 3.0506\n",
      "Epoch [48/90], lter [41/1752] Loss: 1.5397\n",
      "Epoch [48/90], lter [51/1752] Loss: 3.2640\n",
      "Epoch [48/90], lter [61/1752] Loss: 3.4767\n",
      "Epoch [48/90], lter [71/1752] Loss: 3.0069\n",
      "Epoch [48/90], lter [81/1752] Loss: 4.7735\n",
      "Epoch [48/90], lter [91/1752] Loss: 2.2336\n",
      "Epoch [48/90], lter [101/1752] Loss: 2.3598\n",
      "Epoch [48/90], lter [111/1752] Loss: 3.6471\n",
      "Epoch [48/90], lter [121/1752] Loss: 3.3515\n",
      "Epoch [48/90], lter [131/1752] Loss: 2.4843\n",
      "Epoch [48/90], lter [141/1752] Loss: 3.0749\n",
      "Epoch [48/90], lter [151/1752] Loss: 2.6007\n",
      "Epoch [48/90], lter [161/1752] Loss: 2.8224\n",
      "Epoch [48/90], lter [171/1752] Loss: 2.3982\n",
      "Epoch [48/90], lter [181/1752] Loss: 2.4292\n",
      "Epoch [48/90], lter [191/1752] Loss: 3.2601\n",
      "Epoch [48/90], lter [201/1752] Loss: 1.7305\n",
      "Epoch [48/90], lter [211/1752] Loss: 3.9441\n",
      "Epoch [48/90], lter [221/1752] Loss: 5.0697\n",
      "Epoch [48/90], lter [231/1752] Loss: 2.4585\n",
      "Epoch [48/90], lter [241/1752] Loss: 3.5861\n",
      "Epoch [48/90], lter [251/1752] Loss: 3.2732\n",
      "Epoch [48/90], lter [261/1752] Loss: 2.8410\n",
      "Epoch [48/90], lter [271/1752] Loss: 1.9055\n",
      "Epoch [48/90], lter [281/1752] Loss: 2.2972\n",
      "Epoch [48/90], lter [291/1752] Loss: 3.4594\n",
      "Epoch [48/90], lter [301/1752] Loss: 2.8324\n",
      "Epoch [48/90], lter [311/1752] Loss: 2.5457\n",
      "Epoch [48/90], lter [321/1752] Loss: 2.3687\n",
      "Epoch [48/90], lter [331/1752] Loss: 3.0599\n",
      "Epoch [48/90], lter [341/1752] Loss: 2.2141\n",
      "Epoch [48/90], lter [351/1752] Loss: 1.9256\n",
      "Epoch [48/90], lter [361/1752] Loss: 2.8177\n",
      "Epoch [48/90], lter [371/1752] Loss: 2.6100\n",
      "Epoch [48/90], lter [381/1752] Loss: 2.7767\n",
      "Epoch [48/90], lter [391/1752] Loss: 1.7480\n",
      "Epoch [48/90], lter [401/1752] Loss: 3.1257\n",
      "Epoch [48/90], lter [411/1752] Loss: 2.7862\n",
      "Epoch [48/90], lter [421/1752] Loss: 1.8865\n",
      "Epoch [48/90], lter [431/1752] Loss: 2.4732\n",
      "Epoch [48/90], lter [441/1752] Loss: 3.1793\n",
      "Epoch [48/90], lter [451/1752] Loss: 3.1075\n",
      "Epoch [48/90], lter [461/1752] Loss: 3.1768\n",
      "Epoch [48/90], lter [471/1752] Loss: 2.2387\n",
      "Epoch [48/90], lter [481/1752] Loss: 2.5405\n",
      "Epoch [48/90], lter [491/1752] Loss: 3.9491\n",
      "Epoch [48/90], lter [501/1752] Loss: 3.2177\n",
      "Epoch [48/90], lter [511/1752] Loss: 2.2827\n",
      "Epoch [48/90], lter [521/1752] Loss: 1.9610\n",
      "Epoch [48/90], lter [531/1752] Loss: 2.2460\n",
      "Epoch [48/90], lter [541/1752] Loss: 3.3210\n",
      "Epoch [48/90], lter [551/1752] Loss: 2.6251\n",
      "Epoch [48/90], lter [561/1752] Loss: 2.9104\n",
      "Epoch [48/90], lter [571/1752] Loss: 2.8709\n",
      "Epoch [48/90], lter [581/1752] Loss: 2.9063\n",
      "Epoch [48/90], lter [591/1752] Loss: 1.9515\n",
      "Epoch [48/90], lter [601/1752] Loss: 2.6219\n",
      "Epoch [48/90], lter [611/1752] Loss: 2.1215\n",
      "Epoch [48/90], lter [621/1752] Loss: 2.5859\n",
      "Epoch [48/90], lter [631/1752] Loss: 2.9207\n",
      "Epoch [48/90], lter [641/1752] Loss: 2.9271\n",
      "Epoch [48/90], lter [651/1752] Loss: 2.6379\n",
      "Epoch [48/90], lter [661/1752] Loss: 2.4389\n",
      "Epoch [48/90], lter [671/1752] Loss: 2.2930\n",
      "Epoch [48/90], lter [681/1752] Loss: 3.8291\n",
      "Epoch [48/90], lter [691/1752] Loss: 2.6988\n",
      "Epoch [48/90], lter [701/1752] Loss: 1.9728\n",
      "Epoch [48/90], lter [711/1752] Loss: 2.2046\n",
      "Epoch [48/90], lter [721/1752] Loss: 2.7316\n",
      "Epoch [48/90], lter [731/1752] Loss: 3.2183\n",
      "Epoch [48/90], lter [741/1752] Loss: 2.3507\n",
      "Epoch [48/90], lter [751/1752] Loss: 2.1137\n",
      "Epoch [48/90], lter [761/1752] Loss: 2.2207\n",
      "Epoch [48/90], lter [771/1752] Loss: 2.4688\n",
      "Epoch [48/90], lter [781/1752] Loss: 3.2111\n",
      "Epoch [48/90], lter [791/1752] Loss: 2.2119\n",
      "Epoch [48/90], lter [801/1752] Loss: 2.5977\n",
      "Epoch [48/90], lter [811/1752] Loss: 2.8320\n",
      "Epoch [48/90], lter [821/1752] Loss: 2.4005\n",
      "Epoch [48/90], lter [831/1752] Loss: 2.3434\n",
      "Epoch [48/90], lter [841/1752] Loss: 4.6941\n",
      "Epoch [48/90], lter [851/1752] Loss: 3.2848\n",
      "Epoch [48/90], lter [861/1752] Loss: 3.5596\n",
      "Epoch [48/90], lter [871/1752] Loss: 2.8297\n",
      "Epoch [48/90], lter [881/1752] Loss: 2.6334\n",
      "Epoch [48/90], lter [891/1752] Loss: 3.8525\n",
      "Epoch [48/90], lter [901/1752] Loss: 2.7828\n",
      "Epoch [48/90], lter [911/1752] Loss: 3.3115\n",
      "Epoch [48/90], lter [921/1752] Loss: 3.1876\n",
      "Epoch [48/90], lter [931/1752] Loss: 1.9872\n",
      "Epoch [48/90], lter [941/1752] Loss: 2.1797\n",
      "Epoch [48/90], lter [951/1752] Loss: 2.4708\n",
      "Epoch [48/90], lter [961/1752] Loss: 2.7299\n",
      "Epoch [48/90], lter [971/1752] Loss: 2.9471\n",
      "Epoch [48/90], lter [981/1752] Loss: 3.6090\n",
      "Epoch [48/90], lter [991/1752] Loss: 2.0148\n",
      "Epoch [48/90], lter [1001/1752] Loss: 2.3169\n",
      "Epoch [48/90], lter [1011/1752] Loss: 2.7281\n",
      "Epoch [48/90], lter [1021/1752] Loss: 2.0906\n",
      "Epoch [48/90], lter [1031/1752] Loss: 2.5982\n",
      "Epoch [48/90], lter [1041/1752] Loss: 3.3129\n",
      "Epoch [48/90], lter [1051/1752] Loss: 3.3246\n",
      "Epoch [48/90], lter [1061/1752] Loss: 2.9368\n",
      "Epoch [48/90], lter [1071/1752] Loss: 3.2015\n",
      "Epoch [48/90], lter [1081/1752] Loss: 4.0516\n",
      "Epoch [48/90], lter [1091/1752] Loss: 2.6566\n",
      "Epoch [48/90], lter [1101/1752] Loss: 4.1630\n",
      "Epoch [48/90], lter [1111/1752] Loss: 3.1753\n",
      "Epoch [48/90], lter [1121/1752] Loss: 2.3997\n",
      "Epoch [48/90], lter [1131/1752] Loss: 1.8127\n",
      "Epoch [48/90], lter [1141/1752] Loss: 2.4899\n",
      "Epoch [48/90], lter [1151/1752] Loss: 4.4378\n",
      "Epoch [48/90], lter [1161/1752] Loss: 3.6657\n",
      "Epoch [48/90], lter [1171/1752] Loss: 4.1199\n",
      "Epoch [48/90], lter [1181/1752] Loss: 2.7716\n",
      "Epoch [48/90], lter [1191/1752] Loss: 3.5164\n",
      "Epoch [48/90], lter [1201/1752] Loss: 2.1714\n",
      "Epoch [48/90], lter [1211/1752] Loss: 3.4457\n",
      "Epoch [48/90], lter [1221/1752] Loss: 3.4598\n",
      "Epoch [48/90], lter [1231/1752] Loss: 2.2774\n",
      "Epoch [48/90], lter [1241/1752] Loss: 1.7799\n",
      "Epoch [48/90], lter [1251/1752] Loss: 2.6915\n",
      "Epoch [48/90], lter [1261/1752] Loss: 2.4446\n",
      "Epoch [48/90], lter [1271/1752] Loss: 2.8592\n",
      "Epoch [48/90], lter [1281/1752] Loss: 2.0883\n",
      "Epoch [48/90], lter [1291/1752] Loss: 3.1185\n",
      "Epoch [48/90], lter [1301/1752] Loss: 3.1095\n",
      "Epoch [48/90], lter [1311/1752] Loss: 3.6170\n",
      "Epoch [48/90], lter [1321/1752] Loss: 2.3111\n",
      "Epoch [48/90], lter [1331/1752] Loss: 2.2710\n",
      "Epoch [48/90], lter [1341/1752] Loss: 3.3968\n",
      "Epoch [48/90], lter [1351/1752] Loss: 3.6455\n",
      "Epoch [48/90], lter [1361/1752] Loss: 2.2763\n",
      "Epoch [48/90], lter [1371/1752] Loss: 2.8747\n",
      "Epoch [48/90], lter [1381/1752] Loss: 2.0925\n",
      "Epoch [48/90], lter [1391/1752] Loss: 1.9397\n",
      "Epoch [48/90], lter [1401/1752] Loss: 3.3554\n",
      "Epoch [48/90], lter [1411/1752] Loss: 3.5976\n",
      "Epoch [48/90], lter [1421/1752] Loss: 2.3805\n",
      "Epoch [48/90], lter [1431/1752] Loss: 3.6875\n",
      "Epoch [48/90], lter [1441/1752] Loss: 2.5515\n",
      "Epoch [48/90], lter [1451/1752] Loss: 1.6699\n",
      "Epoch [48/90], lter [1461/1752] Loss: 2.3673\n",
      "Epoch [48/90], lter [1471/1752] Loss: 2.3737\n",
      "Epoch [48/90], lter [1481/1752] Loss: 2.6803\n",
      "Epoch [48/90], lter [1491/1752] Loss: 2.7687\n",
      "Epoch [48/90], lter [1501/1752] Loss: 2.6223\n",
      "Epoch [48/90], lter [1511/1752] Loss: 2.5964\n",
      "Epoch [48/90], lter [1521/1752] Loss: 2.3108\n",
      "Epoch [48/90], lter [1531/1752] Loss: 3.2643\n",
      "Epoch [48/90], lter [1541/1752] Loss: 2.1666\n",
      "Epoch [48/90], lter [1551/1752] Loss: 3.4110\n",
      "Epoch [48/90], lter [1561/1752] Loss: 3.0816\n",
      "Epoch [48/90], lter [1571/1752] Loss: 1.8018\n",
      "Epoch [48/90], lter [1581/1752] Loss: 3.0672\n",
      "Epoch [48/90], lter [1591/1752] Loss: 3.6562\n",
      "Epoch [48/90], lter [1601/1752] Loss: 4.1159\n",
      "Epoch [48/90], lter [1611/1752] Loss: 1.8613\n",
      "Epoch [48/90], lter [1621/1752] Loss: 1.8715\n",
      "Epoch [48/90], lter [1631/1752] Loss: 2.0300\n",
      "Epoch [48/90], lter [1641/1752] Loss: 2.1486\n",
      "Epoch [48/90], lter [1651/1752] Loss: 2.8080\n",
      "Epoch [48/90], lter [1661/1752] Loss: 3.2987\n",
      "Epoch [48/90], lter [1671/1752] Loss: 3.3619\n",
      "Epoch [48/90], lter [1681/1752] Loss: 3.6609\n",
      "Epoch [48/90], lter [1691/1752] Loss: 2.4941\n",
      "Epoch [48/90], lter [1701/1752] Loss: 1.9867\n",
      "Epoch [48/90], lter [1711/1752] Loss: 2.7266\n",
      "Epoch [48/90], lter [1721/1752] Loss: 2.7574\n",
      "Epoch [48/90], lter [1731/1752] Loss: 2.4303\n",
      "Epoch [48/90], lter [1741/1752] Loss: 2.1683\n",
      "Epoch [48/90], lter [1751/1752] Loss: 2.4081\n",
      "Epoch:  48 | train loss : 2.7813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 48/90 [75:27:32<14:13:51, 1219.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  47 | test loss : 1.9690\n",
      "Epoch [49/90], lter [1/1752] Loss: 2.7852\n",
      "Epoch [49/90], lter [11/1752] Loss: 3.9164\n",
      "Epoch [49/90], lter [21/1752] Loss: 2.3728\n",
      "Epoch [49/90], lter [31/1752] Loss: 3.1202\n",
      "Epoch [49/90], lter [41/1752] Loss: 2.8944\n",
      "Epoch [49/90], lter [51/1752] Loss: 2.8153\n",
      "Epoch [49/90], lter [61/1752] Loss: 2.6583\n",
      "Epoch [49/90], lter [71/1752] Loss: 1.9385\n",
      "Epoch [49/90], lter [81/1752] Loss: 3.4998\n",
      "Epoch [49/90], lter [91/1752] Loss: 2.8346\n",
      "Epoch [49/90], lter [101/1752] Loss: 2.5559\n",
      "Epoch [49/90], lter [111/1752] Loss: 2.0724\n",
      "Epoch [49/90], lter [121/1752] Loss: 3.2043\n",
      "Epoch [49/90], lter [131/1752] Loss: 2.2762\n",
      "Epoch [49/90], lter [141/1752] Loss: 2.1554\n",
      "Epoch [49/90], lter [151/1752] Loss: 3.6354\n",
      "Epoch [49/90], lter [161/1752] Loss: 1.6808\n",
      "Epoch [49/90], lter [171/1752] Loss: 2.6931\n",
      "Epoch [49/90], lter [181/1752] Loss: 2.2655\n",
      "Epoch [49/90], lter [191/1752] Loss: 2.7993\n",
      "Epoch [49/90], lter [201/1752] Loss: 2.8404\n",
      "Epoch [49/90], lter [211/1752] Loss: 2.5969\n",
      "Epoch [49/90], lter [221/1752] Loss: 1.4351\n",
      "Epoch [49/90], lter [231/1752] Loss: 4.0839\n",
      "Epoch [49/90], lter [241/1752] Loss: 2.1107\n",
      "Epoch [49/90], lter [251/1752] Loss: 2.0268\n",
      "Epoch [49/90], lter [261/1752] Loss: 2.7388\n",
      "Epoch [49/90], lter [271/1752] Loss: 3.2837\n",
      "Epoch [49/90], lter [281/1752] Loss: 2.9329\n",
      "Epoch [49/90], lter [291/1752] Loss: 4.0488\n",
      "Epoch [49/90], lter [301/1752] Loss: 2.1252\n",
      "Epoch [49/90], lter [311/1752] Loss: 2.4975\n",
      "Epoch [49/90], lter [321/1752] Loss: 3.7396\n",
      "Epoch [49/90], lter [331/1752] Loss: 2.8595\n",
      "Epoch [49/90], lter [341/1752] Loss: 2.4161\n",
      "Epoch [49/90], lter [351/1752] Loss: 3.3613\n",
      "Epoch [49/90], lter [361/1752] Loss: 2.6004\n",
      "Epoch [49/90], lter [371/1752] Loss: 2.2113\n",
      "Epoch [49/90], lter [381/1752] Loss: 2.8350\n",
      "Epoch [49/90], lter [391/1752] Loss: 2.3698\n",
      "Epoch [49/90], lter [401/1752] Loss: 2.5532\n",
      "Epoch [49/90], lter [411/1752] Loss: 3.1330\n",
      "Epoch [49/90], lter [421/1752] Loss: 2.4182\n",
      "Epoch [49/90], lter [431/1752] Loss: 2.3733\n",
      "Epoch [49/90], lter [441/1752] Loss: 3.2794\n",
      "Epoch [49/90], lter [451/1752] Loss: 2.4414\n",
      "Epoch [49/90], lter [461/1752] Loss: 1.6538\n",
      "Epoch [49/90], lter [471/1752] Loss: 2.5712\n",
      "Epoch [49/90], lter [481/1752] Loss: 2.7357\n",
      "Epoch [49/90], lter [491/1752] Loss: 2.8613\n",
      "Epoch [49/90], lter [501/1752] Loss: 2.7599\n",
      "Epoch [49/90], lter [511/1752] Loss: 2.1208\n",
      "Epoch [49/90], lter [521/1752] Loss: 2.7222\n",
      "Epoch [49/90], lter [531/1752] Loss: 2.5379\n",
      "Epoch [49/90], lter [541/1752] Loss: 3.8305\n",
      "Epoch [49/90], lter [551/1752] Loss: 3.0129\n",
      "Epoch [49/90], lter [561/1752] Loss: 2.3836\n",
      "Epoch [49/90], lter [571/1752] Loss: 2.4460\n",
      "Epoch [49/90], lter [581/1752] Loss: 1.6980\n",
      "Epoch [49/90], lter [591/1752] Loss: 2.3248\n",
      "Epoch [49/90], lter [601/1752] Loss: 2.8073\n",
      "Epoch [49/90], lter [611/1752] Loss: 2.9497\n",
      "Epoch [49/90], lter [621/1752] Loss: 1.5138\n",
      "Epoch [49/90], lter [631/1752] Loss: 3.0517\n",
      "Epoch [49/90], lter [641/1752] Loss: 2.9851\n",
      "Epoch [49/90], lter [651/1752] Loss: 2.5190\n",
      "Epoch [49/90], lter [661/1752] Loss: 2.8838\n",
      "Epoch [49/90], lter [671/1752] Loss: 2.4789\n",
      "Epoch [49/90], lter [681/1752] Loss: 2.1443\n",
      "Epoch [49/90], lter [691/1752] Loss: 2.7950\n",
      "Epoch [49/90], lter [701/1752] Loss: 4.7267\n",
      "Epoch [49/90], lter [711/1752] Loss: 2.4822\n",
      "Epoch [49/90], lter [721/1752] Loss: 3.6021\n",
      "Epoch [49/90], lter [731/1752] Loss: 2.4864\n",
      "Epoch [49/90], lter [741/1752] Loss: 3.5173\n",
      "Epoch [49/90], lter [751/1752] Loss: 2.6802\n",
      "Epoch [49/90], lter [761/1752] Loss: 3.3271\n",
      "Epoch [49/90], lter [771/1752] Loss: 4.1752\n",
      "Epoch [49/90], lter [781/1752] Loss: 1.4224\n",
      "Epoch [49/90], lter [791/1752] Loss: 2.4022\n",
      "Epoch [49/90], lter [801/1752] Loss: 1.9436\n",
      "Epoch [49/90], lter [811/1752] Loss: 2.8625\n",
      "Epoch [49/90], lter [821/1752] Loss: 3.0175\n",
      "Epoch [49/90], lter [831/1752] Loss: 2.7501\n",
      "Epoch [49/90], lter [841/1752] Loss: 2.3632\n",
      "Epoch [49/90], lter [851/1752] Loss: 3.2237\n",
      "Epoch [49/90], lter [861/1752] Loss: 2.6607\n",
      "Epoch [49/90], lter [871/1752] Loss: 3.0915\n",
      "Epoch [49/90], lter [881/1752] Loss: 2.6475\n",
      "Epoch [49/90], lter [891/1752] Loss: 2.6890\n",
      "Epoch [49/90], lter [901/1752] Loss: 2.4945\n",
      "Epoch [49/90], lter [911/1752] Loss: 1.7526\n",
      "Epoch [49/90], lter [921/1752] Loss: 3.1196\n",
      "Epoch [49/90], lter [931/1752] Loss: 3.1530\n",
      "Epoch [49/90], lter [941/1752] Loss: 3.5433\n",
      "Epoch [49/90], lter [951/1752] Loss: 2.8090\n",
      "Epoch [49/90], lter [961/1752] Loss: 2.0004\n",
      "Epoch [49/90], lter [971/1752] Loss: 2.6480\n",
      "Epoch [49/90], lter [981/1752] Loss: 2.5743\n",
      "Epoch [49/90], lter [991/1752] Loss: 3.0436\n",
      "Epoch [49/90], lter [1001/1752] Loss: 2.8488\n",
      "Epoch [49/90], lter [1011/1752] Loss: 2.7699\n",
      "Epoch [49/90], lter [1021/1752] Loss: 2.4368\n",
      "Epoch [49/90], lter [1031/1752] Loss: 3.5028\n",
      "Epoch [49/90], lter [1041/1752] Loss: 1.7099\n",
      "Epoch [49/90], lter [1051/1752] Loss: 2.6823\n",
      "Epoch [49/90], lter [1061/1752] Loss: 2.8544\n",
      "Epoch [49/90], lter [1071/1752] Loss: 2.2342\n",
      "Epoch [49/90], lter [1081/1752] Loss: 3.3475\n",
      "Epoch [49/90], lter [1091/1752] Loss: 2.6489\n",
      "Epoch [49/90], lter [1101/1752] Loss: 2.5932\n",
      "Epoch [49/90], lter [1111/1752] Loss: 2.0990\n",
      "Epoch [49/90], lter [1121/1752] Loss: 2.0515\n",
      "Epoch [49/90], lter [1131/1752] Loss: 2.7028\n",
      "Epoch [49/90], lter [1141/1752] Loss: 2.2750\n",
      "Epoch [49/90], lter [1151/1752] Loss: 1.9330\n",
      "Epoch [49/90], lter [1161/1752] Loss: 2.7009\n",
      "Epoch [49/90], lter [1171/1752] Loss: 2.6427\n",
      "Epoch [49/90], lter [1181/1752] Loss: 3.7727\n",
      "Epoch [49/90], lter [1191/1752] Loss: 3.1993\n",
      "Epoch [49/90], lter [1201/1752] Loss: 2.2646\n",
      "Epoch [49/90], lter [1211/1752] Loss: 3.3630\n",
      "Epoch [49/90], lter [1221/1752] Loss: 2.1880\n",
      "Epoch [49/90], lter [1231/1752] Loss: 2.6956\n",
      "Epoch [49/90], lter [1241/1752] Loss: 3.4847\n",
      "Epoch [49/90], lter [1251/1752] Loss: 2.6176\n",
      "Epoch [49/90], lter [1261/1752] Loss: 2.7990\n",
      "Epoch [49/90], lter [1271/1752] Loss: 2.3476\n",
      "Epoch [49/90], lter [1281/1752] Loss: 3.1665\n",
      "Epoch [49/90], lter [1291/1752] Loss: 2.8979\n",
      "Epoch [49/90], lter [1301/1752] Loss: 1.9893\n",
      "Epoch [49/90], lter [1311/1752] Loss: 2.9929\n",
      "Epoch [49/90], lter [1321/1752] Loss: 2.8746\n",
      "Epoch [49/90], lter [1331/1752] Loss: 4.0735\n",
      "Epoch [49/90], lter [1341/1752] Loss: 3.0164\n",
      "Epoch [49/90], lter [1351/1752] Loss: 2.7289\n",
      "Epoch [49/90], lter [1361/1752] Loss: 2.7194\n",
      "Epoch [49/90], lter [1371/1752] Loss: 2.5923\n",
      "Epoch [49/90], lter [1381/1752] Loss: 2.1217\n",
      "Epoch [49/90], lter [1391/1752] Loss: 2.6184\n",
      "Epoch [49/90], lter [1401/1752] Loss: 2.5100\n",
      "Epoch [49/90], lter [1411/1752] Loss: 2.9860\n",
      "Epoch [49/90], lter [1421/1752] Loss: 2.7733\n",
      "Epoch [49/90], lter [1431/1752] Loss: 2.4353\n",
      "Epoch [49/90], lter [1441/1752] Loss: 3.0123\n",
      "Epoch [49/90], lter [1451/1752] Loss: 2.6722\n",
      "Epoch [49/90], lter [1461/1752] Loss: 3.6557\n",
      "Epoch [49/90], lter [1471/1752] Loss: 2.8760\n",
      "Epoch [49/90], lter [1481/1752] Loss: 2.6729\n",
      "Epoch [49/90], lter [1491/1752] Loss: 2.1390\n",
      "Epoch [49/90], lter [1501/1752] Loss: 2.4553\n",
      "Epoch [49/90], lter [1511/1752] Loss: 2.4897\n",
      "Epoch [49/90], lter [1521/1752] Loss: 2.3756\n",
      "Epoch [49/90], lter [1531/1752] Loss: 2.1744\n",
      "Epoch [49/90], lter [1541/1752] Loss: 2.9765\n",
      "Epoch [49/90], lter [1551/1752] Loss: 2.5396\n",
      "Epoch [49/90], lter [1561/1752] Loss: 3.1676\n",
      "Epoch [49/90], lter [1571/1752] Loss: 2.7457\n",
      "Epoch [49/90], lter [1581/1752] Loss: 2.4064\n",
      "Epoch [49/90], lter [1591/1752] Loss: 1.8177\n",
      "Epoch [49/90], lter [1601/1752] Loss: 2.2711\n",
      "Epoch [49/90], lter [1611/1752] Loss: 2.8293\n",
      "Epoch [49/90], lter [1621/1752] Loss: 3.1210\n",
      "Epoch [49/90], lter [1631/1752] Loss: 3.0178\n",
      "Epoch [49/90], lter [1641/1752] Loss: 3.0836\n",
      "Epoch [49/90], lter [1651/1752] Loss: 3.5775\n",
      "Epoch [49/90], lter [1661/1752] Loss: 2.2301\n",
      "Epoch [49/90], lter [1671/1752] Loss: 2.1703\n",
      "Epoch [49/90], lter [1681/1752] Loss: 2.2134\n",
      "Epoch [49/90], lter [1691/1752] Loss: 2.1683\n",
      "Epoch [49/90], lter [1701/1752] Loss: 3.0393\n",
      "Epoch [49/90], lter [1711/1752] Loss: 3.2636\n",
      "Epoch [49/90], lter [1721/1752] Loss: 1.5757\n",
      "Epoch [49/90], lter [1731/1752] Loss: 2.7709\n",
      "Epoch [49/90], lter [1741/1752] Loss: 3.5476\n",
      "Epoch [49/90], lter [1751/1752] Loss: 3.2132\n",
      "Epoch:  49 | train loss : 2.7614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 49/90 [75:47:59<13:54:53, 1221.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  48 | test loss : 2.3701\n",
      "Epoch [50/90], lter [1/1752] Loss: 2.8634\n",
      "Epoch [50/90], lter [11/1752] Loss: 3.6233\n",
      "Epoch [50/90], lter [21/1752] Loss: 1.8198\n",
      "Epoch [50/90], lter [31/1752] Loss: 2.5244\n",
      "Epoch [50/90], lter [41/1752] Loss: 2.6589\n",
      "Epoch [50/90], lter [51/1752] Loss: 1.8596\n",
      "Epoch [50/90], lter [61/1752] Loss: 2.0762\n",
      "Epoch [50/90], lter [71/1752] Loss: 2.6309\n",
      "Epoch [50/90], lter [81/1752] Loss: 3.0285\n",
      "Epoch [50/90], lter [91/1752] Loss: 2.9527\n",
      "Epoch [50/90], lter [101/1752] Loss: 3.2870\n",
      "Epoch [50/90], lter [111/1752] Loss: 3.8645\n",
      "Epoch [50/90], lter [121/1752] Loss: 2.2148\n",
      "Epoch [50/90], lter [131/1752] Loss: 2.7648\n",
      "Epoch [50/90], lter [141/1752] Loss: 3.0172\n",
      "Epoch [50/90], lter [151/1752] Loss: 2.0347\n",
      "Epoch [50/90], lter [161/1752] Loss: 1.4538\n",
      "Epoch [50/90], lter [171/1752] Loss: 3.1914\n",
      "Epoch [50/90], lter [181/1752] Loss: 2.6205\n",
      "Epoch [50/90], lter [191/1752] Loss: 2.5811\n",
      "Epoch [50/90], lter [201/1752] Loss: 2.0471\n",
      "Epoch [50/90], lter [211/1752] Loss: 2.9620\n",
      "Epoch [50/90], lter [221/1752] Loss: 2.0275\n",
      "Epoch [50/90], lter [231/1752] Loss: 2.0792\n",
      "Epoch [50/90], lter [241/1752] Loss: 2.6857\n",
      "Epoch [50/90], lter [251/1752] Loss: 2.5171\n",
      "Epoch [50/90], lter [261/1752] Loss: 3.4391\n",
      "Epoch [50/90], lter [271/1752] Loss: 3.2107\n",
      "Epoch [50/90], lter [281/1752] Loss: 4.1964\n",
      "Epoch [50/90], lter [291/1752] Loss: 3.5689\n",
      "Epoch [50/90], lter [301/1752] Loss: 2.1852\n",
      "Epoch [50/90], lter [311/1752] Loss: 2.3704\n",
      "Epoch [50/90], lter [321/1752] Loss: 2.7379\n",
      "Epoch [50/90], lter [331/1752] Loss: 1.7490\n",
      "Epoch [50/90], lter [341/1752] Loss: 2.1070\n",
      "Epoch [50/90], lter [351/1752] Loss: 3.2201\n",
      "Epoch [50/90], lter [361/1752] Loss: 2.2079\n",
      "Epoch [50/90], lter [371/1752] Loss: 3.1091\n",
      "Epoch [50/90], lter [381/1752] Loss: 4.0721\n",
      "Epoch [50/90], lter [391/1752] Loss: 3.3294\n",
      "Epoch [50/90], lter [401/1752] Loss: 2.6247\n",
      "Epoch [50/90], lter [411/1752] Loss: 2.4187\n",
      "Epoch [50/90], lter [421/1752] Loss: 2.2665\n",
      "Epoch [50/90], lter [431/1752] Loss: 2.4419\n",
      "Epoch [50/90], lter [441/1752] Loss: 2.6542\n",
      "Epoch [50/90], lter [451/1752] Loss: 1.9981\n",
      "Epoch [50/90], lter [461/1752] Loss: 2.2449\n",
      "Epoch [50/90], lter [471/1752] Loss: 3.6110\n",
      "Epoch [50/90], lter [481/1752] Loss: 2.6739\n",
      "Epoch [50/90], lter [491/1752] Loss: 2.2532\n",
      "Epoch [50/90], lter [501/1752] Loss: 2.7119\n",
      "Epoch [50/90], lter [511/1752] Loss: 2.5230\n",
      "Epoch [50/90], lter [521/1752] Loss: 2.5851\n",
      "Epoch [50/90], lter [531/1752] Loss: 3.3634\n",
      "Epoch [50/90], lter [541/1752] Loss: 2.9231\n",
      "Epoch [50/90], lter [551/1752] Loss: 2.9614\n",
      "Epoch [50/90], lter [561/1752] Loss: 3.2890\n",
      "Epoch [50/90], lter [571/1752] Loss: 2.6230\n",
      "Epoch [50/90], lter [581/1752] Loss: 3.6635\n",
      "Epoch [50/90], lter [591/1752] Loss: 1.9344\n",
      "Epoch [50/90], lter [601/1752] Loss: 2.5060\n",
      "Epoch [50/90], lter [611/1752] Loss: 3.0820\n",
      "Epoch [50/90], lter [621/1752] Loss: 2.0909\n",
      "Epoch [50/90], lter [631/1752] Loss: 2.3325\n",
      "Epoch [50/90], lter [641/1752] Loss: 3.2291\n",
      "Epoch [50/90], lter [651/1752] Loss: 3.2445\n",
      "Epoch [50/90], lter [661/1752] Loss: 2.1076\n",
      "Epoch [50/90], lter [671/1752] Loss: 3.2997\n",
      "Epoch [50/90], lter [681/1752] Loss: 3.7066\n",
      "Epoch [50/90], lter [691/1752] Loss: 1.9247\n",
      "Epoch [50/90], lter [701/1752] Loss: 2.7651\n",
      "Epoch [50/90], lter [711/1752] Loss: 2.6193\n",
      "Epoch [50/90], lter [721/1752] Loss: 2.1683\n",
      "Epoch [50/90], lter [731/1752] Loss: 2.5886\n",
      "Epoch [50/90], lter [741/1752] Loss: 1.7353\n",
      "Epoch [50/90], lter [751/1752] Loss: 2.9270\n",
      "Epoch [50/90], lter [761/1752] Loss: 1.5451\n",
      "Epoch [50/90], lter [771/1752] Loss: 2.3242\n",
      "Epoch [50/90], lter [781/1752] Loss: 2.7102\n",
      "Epoch [50/90], lter [791/1752] Loss: 2.1383\n",
      "Epoch [50/90], lter [801/1752] Loss: 2.4692\n",
      "Epoch [50/90], lter [811/1752] Loss: 2.8697\n",
      "Epoch [50/90], lter [821/1752] Loss: 2.3537\n",
      "Epoch [50/90], lter [831/1752] Loss: 2.2711\n",
      "Epoch [50/90], lter [841/1752] Loss: 3.0261\n",
      "Epoch [50/90], lter [851/1752] Loss: 2.7929\n",
      "Epoch [50/90], lter [861/1752] Loss: 2.5059\n",
      "Epoch [50/90], lter [871/1752] Loss: 2.3640\n",
      "Epoch [50/90], lter [881/1752] Loss: 3.2183\n",
      "Epoch [50/90], lter [891/1752] Loss: 2.7803\n",
      "Epoch [50/90], lter [901/1752] Loss: 3.2838\n",
      "Epoch [50/90], lter [911/1752] Loss: 2.9157\n",
      "Epoch [50/90], lter [921/1752] Loss: 2.4450\n",
      "Epoch [50/90], lter [931/1752] Loss: 3.1913\n",
      "Epoch [50/90], lter [941/1752] Loss: 3.4672\n",
      "Epoch [50/90], lter [951/1752] Loss: 3.8111\n",
      "Epoch [50/90], lter [961/1752] Loss: 2.8118\n",
      "Epoch [50/90], lter [971/1752] Loss: 1.9686\n",
      "Epoch [50/90], lter [981/1752] Loss: 3.0292\n",
      "Epoch [50/90], lter [991/1752] Loss: 3.3238\n",
      "Epoch [50/90], lter [1001/1752] Loss: 3.1216\n",
      "Epoch [50/90], lter [1011/1752] Loss: 3.0537\n",
      "Epoch [50/90], lter [1021/1752] Loss: 2.1349\n",
      "Epoch [50/90], lter [1031/1752] Loss: 3.5279\n",
      "Epoch [50/90], lter [1041/1752] Loss: 2.6344\n",
      "Epoch [50/90], lter [1051/1752] Loss: 3.9085\n",
      "Epoch [50/90], lter [1061/1752] Loss: 2.7142\n",
      "Epoch [50/90], lter [1071/1752] Loss: 3.7112\n",
      "Epoch [50/90], lter [1081/1752] Loss: 4.0468\n",
      "Epoch [50/90], lter [1091/1752] Loss: 3.8791\n",
      "Epoch [50/90], lter [1101/1752] Loss: 1.8556\n",
      "Epoch [50/90], lter [1111/1752] Loss: 2.0369\n",
      "Epoch [50/90], lter [1121/1752] Loss: 3.2534\n",
      "Epoch [50/90], lter [1131/1752] Loss: 2.1780\n",
      "Epoch [50/90], lter [1141/1752] Loss: 3.1266\n",
      "Epoch [50/90], lter [1151/1752] Loss: 3.3725\n",
      "Epoch [50/90], lter [1161/1752] Loss: 1.5483\n",
      "Epoch [50/90], lter [1171/1752] Loss: 3.1077\n",
      "Epoch [50/90], lter [1181/1752] Loss: 3.3051\n",
      "Epoch [50/90], lter [1191/1752] Loss: 2.5360\n",
      "Epoch [50/90], lter [1201/1752] Loss: 2.7523\n",
      "Epoch [50/90], lter [1211/1752] Loss: 2.0713\n",
      "Epoch [50/90], lter [1221/1752] Loss: 3.1747\n",
      "Epoch [50/90], lter [1231/1752] Loss: 2.1319\n",
      "Epoch [50/90], lter [1241/1752] Loss: 2.9041\n",
      "Epoch [50/90], lter [1251/1752] Loss: 2.3329\n",
      "Epoch [50/90], lter [1261/1752] Loss: 2.4715\n",
      "Epoch [50/90], lter [1271/1752] Loss: 2.5291\n",
      "Epoch [50/90], lter [1281/1752] Loss: 2.6894\n",
      "Epoch [50/90], lter [1291/1752] Loss: 2.5608\n",
      "Epoch [50/90], lter [1301/1752] Loss: 2.5841\n",
      "Epoch [50/90], lter [1311/1752] Loss: 2.6229\n",
      "Epoch [50/90], lter [1321/1752] Loss: 2.8208\n",
      "Epoch [50/90], lter [1331/1752] Loss: 2.8188\n",
      "Epoch [50/90], lter [1341/1752] Loss: 2.0256\n",
      "Epoch [50/90], lter [1351/1752] Loss: 2.7644\n",
      "Epoch [50/90], lter [1361/1752] Loss: 2.9370\n",
      "Epoch [50/90], lter [1371/1752] Loss: 3.7782\n",
      "Epoch [50/90], lter [1381/1752] Loss: 4.1559\n",
      "Epoch [50/90], lter [1391/1752] Loss: 2.8083\n",
      "Epoch [50/90], lter [1401/1752] Loss: 3.4735\n",
      "Epoch [50/90], lter [1411/1752] Loss: 2.3093\n",
      "Epoch [50/90], lter [1421/1752] Loss: 2.8560\n",
      "Epoch [50/90], lter [1431/1752] Loss: 3.9352\n",
      "Epoch [50/90], lter [1441/1752] Loss: 3.0760\n",
      "Epoch [50/90], lter [1451/1752] Loss: 2.9977\n",
      "Epoch [50/90], lter [1461/1752] Loss: 3.0521\n",
      "Epoch [50/90], lter [1471/1752] Loss: 3.3730\n",
      "Epoch [50/90], lter [1481/1752] Loss: 3.1189\n",
      "Epoch [50/90], lter [1491/1752] Loss: 3.0385\n",
      "Epoch [50/90], lter [1501/1752] Loss: 2.3375\n",
      "Epoch [50/90], lter [1511/1752] Loss: 2.4577\n",
      "Epoch [50/90], lter [1521/1752] Loss: 1.4227\n",
      "Epoch [50/90], lter [1531/1752] Loss: 2.1940\n",
      "Epoch [50/90], lter [1541/1752] Loss: 4.3685\n",
      "Epoch [50/90], lter [1551/1752] Loss: 2.8759\n",
      "Epoch [50/90], lter [1561/1752] Loss: 2.3410\n",
      "Epoch [50/90], lter [1571/1752] Loss: 2.2242\n",
      "Epoch [50/90], lter [1581/1752] Loss: 2.6003\n",
      "Epoch [50/90], lter [1591/1752] Loss: 3.0576\n",
      "Epoch [50/90], lter [1601/1752] Loss: 2.3020\n",
      "Epoch [50/90], lter [1611/1752] Loss: 3.5496\n",
      "Epoch [50/90], lter [1621/1752] Loss: 3.3942\n",
      "Epoch [50/90], lter [1631/1752] Loss: 3.5995\n",
      "Epoch [50/90], lter [1641/1752] Loss: 1.9474\n",
      "Epoch [50/90], lter [1651/1752] Loss: 2.1288\n",
      "Epoch [50/90], lter [1661/1752] Loss: 2.4704\n",
      "Epoch [50/90], lter [1671/1752] Loss: 3.1282\n",
      "Epoch [50/90], lter [1681/1752] Loss: 2.6995\n",
      "Epoch [50/90], lter [1691/1752] Loss: 3.1366\n",
      "Epoch [50/90], lter [1701/1752] Loss: 2.2639\n",
      "Epoch [50/90], lter [1711/1752] Loss: 3.7518\n",
      "Epoch [50/90], lter [1721/1752] Loss: 2.2684\n",
      "Epoch [50/90], lter [1731/1752] Loss: 3.2368\n",
      "Epoch [50/90], lter [1741/1752] Loss: 2.5660\n",
      "Epoch [50/90], lter [1751/1752] Loss: 2.5506\n",
      "Epoch:  50 | train loss : 2.7853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 50/90 [76:08:12<13:32:46, 1219.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  49 | test loss : 2.2058\n",
      "Epoch [51/90], lter [1/1752] Loss: 3.2327\n",
      "Epoch [51/90], lter [11/1752] Loss: 2.6511\n",
      "Epoch [51/90], lter [21/1752] Loss: 2.6652\n",
      "Epoch [51/90], lter [31/1752] Loss: 2.0051\n",
      "Epoch [51/90], lter [41/1752] Loss: 3.0353\n",
      "Epoch [51/90], lter [51/1752] Loss: 3.1409\n",
      "Epoch [51/90], lter [61/1752] Loss: 3.2226\n",
      "Epoch [51/90], lter [71/1752] Loss: 2.4364\n",
      "Epoch [51/90], lter [81/1752] Loss: 3.2796\n",
      "Epoch [51/90], lter [91/1752] Loss: 2.0748\n",
      "Epoch [51/90], lter [101/1752] Loss: 2.3076\n",
      "Epoch [51/90], lter [111/1752] Loss: 2.3449\n",
      "Epoch [51/90], lter [121/1752] Loss: 2.3773\n",
      "Epoch [51/90], lter [131/1752] Loss: 2.1861\n",
      "Epoch [51/90], lter [141/1752] Loss: 2.5814\n",
      "Epoch [51/90], lter [151/1752] Loss: 4.5878\n",
      "Epoch [51/90], lter [161/1752] Loss: 2.4428\n",
      "Epoch [51/90], lter [171/1752] Loss: 1.9128\n",
      "Epoch [51/90], lter [181/1752] Loss: 2.8526\n",
      "Epoch [51/90], lter [191/1752] Loss: 3.0105\n",
      "Epoch [51/90], lter [201/1752] Loss: 4.7494\n",
      "Epoch [51/90], lter [211/1752] Loss: 3.0888\n",
      "Epoch [51/90], lter [221/1752] Loss: 2.2817\n",
      "Epoch [51/90], lter [231/1752] Loss: 1.9128\n",
      "Epoch [51/90], lter [241/1752] Loss: 3.7382\n",
      "Epoch [51/90], lter [251/1752] Loss: 2.7903\n",
      "Epoch [51/90], lter [261/1752] Loss: 2.3853\n",
      "Epoch [51/90], lter [271/1752] Loss: 2.9312\n",
      "Epoch [51/90], lter [281/1752] Loss: 2.3864\n",
      "Epoch [51/90], lter [291/1752] Loss: 2.1324\n",
      "Epoch [51/90], lter [301/1752] Loss: 2.6253\n",
      "Epoch [51/90], lter [311/1752] Loss: 2.2038\n",
      "Epoch [51/90], lter [321/1752] Loss: 2.8373\n",
      "Epoch [51/90], lter [331/1752] Loss: 1.9550\n",
      "Epoch [51/90], lter [341/1752] Loss: 3.3986\n",
      "Epoch [51/90], lter [351/1752] Loss: 2.6928\n",
      "Epoch [51/90], lter [361/1752] Loss: 2.4959\n",
      "Epoch [51/90], lter [371/1752] Loss: 1.9678\n",
      "Epoch [51/90], lter [381/1752] Loss: 1.9926\n",
      "Epoch [51/90], lter [391/1752] Loss: 2.9402\n",
      "Epoch [51/90], lter [401/1752] Loss: 3.2146\n",
      "Epoch [51/90], lter [411/1752] Loss: 3.4107\n",
      "Epoch [51/90], lter [421/1752] Loss: 3.8200\n",
      "Epoch [51/90], lter [431/1752] Loss: 3.0977\n",
      "Epoch [51/90], lter [441/1752] Loss: 3.1421\n",
      "Epoch [51/90], lter [451/1752] Loss: 2.5136\n",
      "Epoch [51/90], lter [461/1752] Loss: 3.1275\n",
      "Epoch [51/90], lter [471/1752] Loss: 3.3549\n",
      "Epoch [51/90], lter [481/1752] Loss: 2.4312\n",
      "Epoch [51/90], lter [491/1752] Loss: 3.6295\n",
      "Epoch [51/90], lter [501/1752] Loss: 2.6610\n",
      "Epoch [51/90], lter [511/1752] Loss: 3.2445\n",
      "Epoch [51/90], lter [521/1752] Loss: 2.8033\n",
      "Epoch [51/90], lter [531/1752] Loss: 2.2729\n",
      "Epoch [51/90], lter [541/1752] Loss: 2.6290\n",
      "Epoch [51/90], lter [551/1752] Loss: 2.1613\n",
      "Epoch [51/90], lter [561/1752] Loss: 3.2351\n",
      "Epoch [51/90], lter [571/1752] Loss: 2.2594\n",
      "Epoch [51/90], lter [581/1752] Loss: 2.4885\n",
      "Epoch [51/90], lter [591/1752] Loss: 2.2979\n",
      "Epoch [51/90], lter [601/1752] Loss: 2.8223\n",
      "Epoch [51/90], lter [611/1752] Loss: 2.6060\n",
      "Epoch [51/90], lter [621/1752] Loss: 2.6692\n",
      "Epoch [51/90], lter [631/1752] Loss: 3.2793\n",
      "Epoch [51/90], lter [641/1752] Loss: 2.0365\n",
      "Epoch [51/90], lter [651/1752] Loss: 2.5900\n",
      "Epoch [51/90], lter [661/1752] Loss: 3.0811\n",
      "Epoch [51/90], lter [671/1752] Loss: 2.7937\n",
      "Epoch [51/90], lter [681/1752] Loss: 2.4507\n",
      "Epoch [51/90], lter [691/1752] Loss: 2.3316\n",
      "Epoch [51/90], lter [701/1752] Loss: 2.9995\n",
      "Epoch [51/90], lter [711/1752] Loss: 2.7500\n",
      "Epoch [51/90], lter [721/1752] Loss: 3.9453\n",
      "Epoch [51/90], lter [731/1752] Loss: 4.1689\n",
      "Epoch [51/90], lter [741/1752] Loss: 3.1810\n",
      "Epoch [51/90], lter [751/1752] Loss: 3.1559\n",
      "Epoch [51/90], lter [761/1752] Loss: 2.5486\n",
      "Epoch [51/90], lter [771/1752] Loss: 2.5010\n",
      "Epoch [51/90], lter [781/1752] Loss: 2.1367\n",
      "Epoch [51/90], lter [791/1752] Loss: 2.3968\n",
      "Epoch [51/90], lter [801/1752] Loss: 2.8937\n",
      "Epoch [51/90], lter [811/1752] Loss: 2.5266\n",
      "Epoch [51/90], lter [821/1752] Loss: 2.5171\n",
      "Epoch [51/90], lter [831/1752] Loss: 1.8096\n",
      "Epoch [51/90], lter [841/1752] Loss: 2.9093\n",
      "Epoch [51/90], lter [851/1752] Loss: 3.2230\n",
      "Epoch [51/90], lter [861/1752] Loss: 3.4088\n",
      "Epoch [51/90], lter [871/1752] Loss: 2.7509\n",
      "Epoch [51/90], lter [881/1752] Loss: 1.9938\n",
      "Epoch [51/90], lter [891/1752] Loss: 3.5799\n",
      "Epoch [51/90], lter [901/1752] Loss: 2.9892\n",
      "Epoch [51/90], lter [911/1752] Loss: 2.2214\n",
      "Epoch [51/90], lter [921/1752] Loss: 2.9683\n",
      "Epoch [51/90], lter [931/1752] Loss: 2.8934\n",
      "Epoch [51/90], lter [941/1752] Loss: 2.3951\n",
      "Epoch [51/90], lter [951/1752] Loss: 1.9774\n",
      "Epoch [51/90], lter [961/1752] Loss: 2.7600\n",
      "Epoch [51/90], lter [971/1752] Loss: 3.0621\n",
      "Epoch [51/90], lter [981/1752] Loss: 2.3064\n",
      "Epoch [51/90], lter [991/1752] Loss: 3.4785\n",
      "Epoch [51/90], lter [1001/1752] Loss: 2.5068\n",
      "Epoch [51/90], lter [1011/1752] Loss: 2.9047\n",
      "Epoch [51/90], lter [1021/1752] Loss: 3.0278\n",
      "Epoch [51/90], lter [1031/1752] Loss: 3.1023\n",
      "Epoch [51/90], lter [1041/1752] Loss: 2.5561\n",
      "Epoch [51/90], lter [1051/1752] Loss: 2.7121\n",
      "Epoch [51/90], lter [1061/1752] Loss: 3.4814\n",
      "Epoch [51/90], lter [1071/1752] Loss: 3.0424\n",
      "Epoch [51/90], lter [1081/1752] Loss: 2.0795\n",
      "Epoch [51/90], lter [1091/1752] Loss: 2.0225\n",
      "Epoch [51/90], lter [1101/1752] Loss: 1.9751\n",
      "Epoch [51/90], lter [1111/1752] Loss: 3.3838\n",
      "Epoch [51/90], lter [1121/1752] Loss: 3.1416\n",
      "Epoch [51/90], lter [1131/1752] Loss: 4.1261\n",
      "Epoch [51/90], lter [1141/1752] Loss: 3.0016\n",
      "Epoch [51/90], lter [1151/1752] Loss: 2.5908\n",
      "Epoch [51/90], lter [1161/1752] Loss: 2.6158\n",
      "Epoch [51/90], lter [1171/1752] Loss: 3.4226\n",
      "Epoch [51/90], lter [1181/1752] Loss: 2.6974\n",
      "Epoch [51/90], lter [1191/1752] Loss: 2.4596\n",
      "Epoch [51/90], lter [1201/1752] Loss: 2.6461\n",
      "Epoch [51/90], lter [1211/1752] Loss: 2.5049\n",
      "Epoch [51/90], lter [1221/1752] Loss: 2.9316\n",
      "Epoch [51/90], lter [1231/1752] Loss: 3.0226\n",
      "Epoch [51/90], lter [1241/1752] Loss: 2.5067\n",
      "Epoch [51/90], lter [1251/1752] Loss: 2.9285\n",
      "Epoch [51/90], lter [1261/1752] Loss: 2.2453\n",
      "Epoch [51/90], lter [1271/1752] Loss: 3.1207\n",
      "Epoch [51/90], lter [1281/1752] Loss: 4.9445\n",
      "Epoch [51/90], lter [1291/1752] Loss: 2.9826\n",
      "Epoch [51/90], lter [1301/1752] Loss: 2.3007\n",
      "Epoch [51/90], lter [1311/1752] Loss: 4.1835\n",
      "Epoch [51/90], lter [1321/1752] Loss: 2.5343\n",
      "Epoch [51/90], lter [1331/1752] Loss: 1.9841\n",
      "Epoch [51/90], lter [1341/1752] Loss: 3.1832\n",
      "Epoch [51/90], lter [1351/1752] Loss: 2.2651\n",
      "Epoch [51/90], lter [1361/1752] Loss: 3.2618\n",
      "Epoch [51/90], lter [1371/1752] Loss: 2.6910\n",
      "Epoch [51/90], lter [1381/1752] Loss: 2.8826\n",
      "Epoch [51/90], lter [1391/1752] Loss: 4.6786\n",
      "Epoch [51/90], lter [1401/1752] Loss: 1.7474\n",
      "Epoch [51/90], lter [1411/1752] Loss: 1.9665\n",
      "Epoch [51/90], lter [1421/1752] Loss: 2.8309\n",
      "Epoch [51/90], lter [1431/1752] Loss: 3.0249\n",
      "Epoch [51/90], lter [1441/1752] Loss: 2.7846\n",
      "Epoch [51/90], lter [1451/1752] Loss: 3.5087\n",
      "Epoch [51/90], lter [1461/1752] Loss: 1.9829\n",
      "Epoch [51/90], lter [1471/1752] Loss: 1.7943\n",
      "Epoch [51/90], lter [1481/1752] Loss: 2.2884\n",
      "Epoch [51/90], lter [1491/1752] Loss: 2.7382\n",
      "Epoch [51/90], lter [1501/1752] Loss: 2.0842\n",
      "Epoch [51/90], lter [1511/1752] Loss: 2.7137\n",
      "Epoch [51/90], lter [1521/1752] Loss: 2.3746\n",
      "Epoch [51/90], lter [1531/1752] Loss: 3.3390\n",
      "Epoch [51/90], lter [1541/1752] Loss: 2.9227\n",
      "Epoch [51/90], lter [1551/1752] Loss: 2.4417\n",
      "Epoch [51/90], lter [1561/1752] Loss: 2.4268\n",
      "Epoch [51/90], lter [1571/1752] Loss: 2.8291\n",
      "Epoch [51/90], lter [1581/1752] Loss: 2.5040\n",
      "Epoch [51/90], lter [1591/1752] Loss: 1.9718\n",
      "Epoch [51/90], lter [1601/1752] Loss: 2.1170\n",
      "Epoch [51/90], lter [1611/1752] Loss: 3.2697\n",
      "Epoch [51/90], lter [1621/1752] Loss: 2.0045\n",
      "Epoch [51/90], lter [1631/1752] Loss: 2.8533\n",
      "Epoch [51/90], lter [1641/1752] Loss: 2.4719\n",
      "Epoch [51/90], lter [1651/1752] Loss: 2.2476\n",
      "Epoch [51/90], lter [1661/1752] Loss: 2.6631\n",
      "Epoch [51/90], lter [1671/1752] Loss: 2.8982\n",
      "Epoch [51/90], lter [1681/1752] Loss: 3.3777\n",
      "Epoch [51/90], lter [1691/1752] Loss: 3.3377\n",
      "Epoch [51/90], lter [1701/1752] Loss: 2.3979\n",
      "Epoch [51/90], lter [1711/1752] Loss: 2.9802\n",
      "Epoch [51/90], lter [1721/1752] Loss: 2.4954\n",
      "Epoch [51/90], lter [1731/1752] Loss: 2.1384\n",
      "Epoch [51/90], lter [1741/1752] Loss: 3.5825\n",
      "Epoch [51/90], lter [1751/1752] Loss: 2.9793\n",
      "Epoch:  51 | train loss : 2.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 51/90 [76:28:33<13:12:48, 1219.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  50 | test loss : 2.3945\n",
      "Epoch [52/90], lter [1/1752] Loss: 3.6735\n",
      "Epoch [52/90], lter [11/1752] Loss: 2.9600\n",
      "Epoch [52/90], lter [21/1752] Loss: 4.7853\n",
      "Epoch [52/90], lter [31/1752] Loss: 3.3464\n",
      "Epoch [52/90], lter [41/1752] Loss: 2.8630\n",
      "Epoch [52/90], lter [51/1752] Loss: 2.1317\n",
      "Epoch [52/90], lter [61/1752] Loss: 2.9575\n",
      "Epoch [52/90], lter [71/1752] Loss: 2.9053\n",
      "Epoch [52/90], lter [81/1752] Loss: 2.7124\n",
      "Epoch [52/90], lter [91/1752] Loss: 2.3434\n",
      "Epoch [52/90], lter [101/1752] Loss: 2.4376\n",
      "Epoch [52/90], lter [111/1752] Loss: 2.9100\n",
      "Epoch [52/90], lter [121/1752] Loss: 3.2317\n",
      "Epoch [52/90], lter [131/1752] Loss: 2.2978\n",
      "Epoch [52/90], lter [141/1752] Loss: 2.5385\n",
      "Epoch [52/90], lter [151/1752] Loss: 2.4681\n",
      "Epoch [52/90], lter [161/1752] Loss: 3.4774\n",
      "Epoch [52/90], lter [171/1752] Loss: 2.0605\n",
      "Epoch [52/90], lter [181/1752] Loss: 2.3806\n",
      "Epoch [52/90], lter [191/1752] Loss: 1.7359\n",
      "Epoch [52/90], lter [201/1752] Loss: 2.5032\n",
      "Epoch [52/90], lter [211/1752] Loss: 2.7002\n",
      "Epoch [52/90], lter [221/1752] Loss: 2.5517\n",
      "Epoch [52/90], lter [231/1752] Loss: 2.8734\n",
      "Epoch [52/90], lter [241/1752] Loss: 2.8876\n",
      "Epoch [52/90], lter [251/1752] Loss: 3.1828\n",
      "Epoch [52/90], lter [261/1752] Loss: 3.1949\n",
      "Epoch [52/90], lter [271/1752] Loss: 2.8771\n",
      "Epoch [52/90], lter [281/1752] Loss: 2.6838\n",
      "Epoch [52/90], lter [291/1752] Loss: 3.2473\n",
      "Epoch [52/90], lter [301/1752] Loss: 2.3375\n",
      "Epoch [52/90], lter [311/1752] Loss: 2.2711\n",
      "Epoch [52/90], lter [321/1752] Loss: 2.6047\n",
      "Epoch [52/90], lter [331/1752] Loss: 2.8279\n",
      "Epoch [52/90], lter [341/1752] Loss: 2.4546\n",
      "Epoch [52/90], lter [351/1752] Loss: 2.1834\n",
      "Epoch [52/90], lter [361/1752] Loss: 2.9168\n",
      "Epoch [52/90], lter [371/1752] Loss: 2.9212\n",
      "Epoch [52/90], lter [381/1752] Loss: 1.8208\n",
      "Epoch [52/90], lter [391/1752] Loss: 3.0496\n",
      "Epoch [52/90], lter [401/1752] Loss: 1.9166\n",
      "Epoch [52/90], lter [411/1752] Loss: 2.9101\n",
      "Epoch [52/90], lter [421/1752] Loss: 2.4410\n",
      "Epoch [52/90], lter [431/1752] Loss: 2.4095\n",
      "Epoch [52/90], lter [441/1752] Loss: 3.5990\n",
      "Epoch [52/90], lter [451/1752] Loss: 3.8273\n",
      "Epoch [52/90], lter [461/1752] Loss: 2.7355\n",
      "Epoch [52/90], lter [471/1752] Loss: 3.2054\n",
      "Epoch [52/90], lter [481/1752] Loss: 2.9841\n",
      "Epoch [52/90], lter [491/1752] Loss: 1.5576\n",
      "Epoch [52/90], lter [501/1752] Loss: 1.7446\n",
      "Epoch [52/90], lter [511/1752] Loss: 1.5479\n",
      "Epoch [52/90], lter [521/1752] Loss: 1.6639\n",
      "Epoch [52/90], lter [531/1752] Loss: 3.3359\n",
      "Epoch [52/90], lter [541/1752] Loss: 2.3823\n",
      "Epoch [52/90], lter [551/1752] Loss: 2.0447\n",
      "Epoch [52/90], lter [561/1752] Loss: 2.3933\n",
      "Epoch [52/90], lter [571/1752] Loss: 2.4234\n",
      "Epoch [52/90], lter [581/1752] Loss: 3.3439\n",
      "Epoch [52/90], lter [591/1752] Loss: 3.6493\n",
      "Epoch [52/90], lter [601/1752] Loss: 2.5659\n",
      "Epoch [52/90], lter [611/1752] Loss: 3.0725\n",
      "Epoch [52/90], lter [621/1752] Loss: 2.6560\n",
      "Epoch [52/90], lter [631/1752] Loss: 2.4740\n",
      "Epoch [52/90], lter [641/1752] Loss: 2.8352\n",
      "Epoch [52/90], lter [651/1752] Loss: 3.3634\n",
      "Epoch [52/90], lter [661/1752] Loss: 2.5716\n",
      "Epoch [52/90], lter [671/1752] Loss: 3.0434\n",
      "Epoch [52/90], lter [681/1752] Loss: 2.7750\n",
      "Epoch [52/90], lter [691/1752] Loss: 3.0715\n",
      "Epoch [52/90], lter [701/1752] Loss: 2.0430\n",
      "Epoch [52/90], lter [711/1752] Loss: 3.4976\n",
      "Epoch [52/90], lter [721/1752] Loss: 2.6896\n",
      "Epoch [52/90], lter [731/1752] Loss: 2.7721\n",
      "Epoch [52/90], lter [741/1752] Loss: 2.4338\n",
      "Epoch [52/90], lter [751/1752] Loss: 3.3925\n",
      "Epoch [52/90], lter [761/1752] Loss: 3.5841\n",
      "Epoch [52/90], lter [771/1752] Loss: 3.1166\n",
      "Epoch [52/90], lter [781/1752] Loss: 2.6111\n",
      "Epoch [52/90], lter [791/1752] Loss: 2.8364\n",
      "Epoch [52/90], lter [801/1752] Loss: 2.5180\n",
      "Epoch [52/90], lter [811/1752] Loss: 2.7185\n",
      "Epoch [52/90], lter [821/1752] Loss: 2.0349\n",
      "Epoch [52/90], lter [831/1752] Loss: 2.9330\n",
      "Epoch [52/90], lter [841/1752] Loss: 2.7137\n",
      "Epoch [52/90], lter [851/1752] Loss: 3.6256\n",
      "Epoch [52/90], lter [861/1752] Loss: 2.1072\n",
      "Epoch [52/90], lter [871/1752] Loss: 2.6132\n",
      "Epoch [52/90], lter [881/1752] Loss: 2.2101\n",
      "Epoch [52/90], lter [891/1752] Loss: 2.2173\n",
      "Epoch [52/90], lter [901/1752] Loss: 3.0728\n",
      "Epoch [52/90], lter [911/1752] Loss: 2.4165\n",
      "Epoch [52/90], lter [921/1752] Loss: 2.3664\n",
      "Epoch [52/90], lter [931/1752] Loss: 2.7502\n",
      "Epoch [52/90], lter [941/1752] Loss: 1.5954\n",
      "Epoch [52/90], lter [951/1752] Loss: 3.9694\n",
      "Epoch [52/90], lter [961/1752] Loss: 2.7451\n",
      "Epoch [52/90], lter [971/1752] Loss: 3.4775\n",
      "Epoch [52/90], lter [981/1752] Loss: 3.9763\n",
      "Epoch [52/90], lter [991/1752] Loss: 2.1234\n",
      "Epoch [52/90], lter [1001/1752] Loss: 3.2880\n",
      "Epoch [52/90], lter [1011/1752] Loss: 2.4108\n",
      "Epoch [52/90], lter [1021/1752] Loss: 1.6691\n",
      "Epoch [52/90], lter [1031/1752] Loss: 1.8958\n",
      "Epoch [52/90], lter [1041/1752] Loss: 2.8836\n",
      "Epoch [52/90], lter [1051/1752] Loss: 3.2297\n",
      "Epoch [52/90], lter [1061/1752] Loss: 3.6129\n",
      "Epoch [52/90], lter [1071/1752] Loss: 3.4723\n",
      "Epoch [52/90], lter [1081/1752] Loss: 2.8564\n",
      "Epoch [52/90], lter [1091/1752] Loss: 1.7675\n",
      "Epoch [52/90], lter [1101/1752] Loss: 2.6419\n",
      "Epoch [52/90], lter [1111/1752] Loss: 3.1224\n",
      "Epoch [52/90], lter [1121/1752] Loss: 1.9748\n",
      "Epoch [52/90], lter [1131/1752] Loss: 1.8060\n",
      "Epoch [52/90], lter [1141/1752] Loss: 3.3296\n",
      "Epoch [52/90], lter [1151/1752] Loss: 2.2203\n",
      "Epoch [52/90], lter [1161/1752] Loss: 2.9954\n",
      "Epoch [52/90], lter [1171/1752] Loss: 3.0688\n",
      "Epoch [52/90], lter [1181/1752] Loss: 3.2352\n",
      "Epoch [52/90], lter [1191/1752] Loss: 3.1667\n",
      "Epoch [52/90], lter [1201/1752] Loss: 2.8443\n",
      "Epoch [52/90], lter [1211/1752] Loss: 3.4572\n",
      "Epoch [52/90], lter [1221/1752] Loss: 2.3444\n",
      "Epoch [52/90], lter [1231/1752] Loss: 2.0650\n",
      "Epoch [52/90], lter [1241/1752] Loss: 2.1597\n",
      "Epoch [52/90], lter [1251/1752] Loss: 2.8723\n",
      "Epoch [52/90], lter [1261/1752] Loss: 2.6347\n",
      "Epoch [52/90], lter [1271/1752] Loss: 2.8640\n",
      "Epoch [52/90], lter [1281/1752] Loss: 2.9406\n",
      "Epoch [52/90], lter [1291/1752] Loss: 3.3471\n",
      "Epoch [52/90], lter [1301/1752] Loss: 1.4827\n",
      "Epoch [52/90], lter [1311/1752] Loss: 2.7711\n",
      "Epoch [52/90], lter [1321/1752] Loss: 3.4156\n",
      "Epoch [52/90], lter [1331/1752] Loss: 2.7843\n",
      "Epoch [52/90], lter [1341/1752] Loss: 3.0303\n",
      "Epoch [52/90], lter [1351/1752] Loss: 1.7411\n",
      "Epoch [52/90], lter [1361/1752] Loss: 3.0675\n",
      "Epoch [52/90], lter [1371/1752] Loss: 2.4005\n",
      "Epoch [52/90], lter [1381/1752] Loss: 2.8626\n",
      "Epoch [52/90], lter [1391/1752] Loss: 1.9060\n",
      "Epoch [52/90], lter [1401/1752] Loss: 2.2630\n",
      "Epoch [52/90], lter [1411/1752] Loss: 2.6031\n",
      "Epoch [52/90], lter [1421/1752] Loss: 2.8252\n",
      "Epoch [52/90], lter [1431/1752] Loss: 1.9530\n",
      "Epoch [52/90], lter [1441/1752] Loss: 2.7144\n",
      "Epoch [52/90], lter [1451/1752] Loss: 3.2307\n",
      "Epoch [52/90], lter [1461/1752] Loss: 2.8970\n",
      "Epoch [52/90], lter [1471/1752] Loss: 2.2070\n",
      "Epoch [52/90], lter [1481/1752] Loss: 3.3542\n",
      "Epoch [52/90], lter [1491/1752] Loss: 3.1971\n",
      "Epoch [52/90], lter [1501/1752] Loss: 4.0482\n",
      "Epoch [52/90], lter [1511/1752] Loss: 2.5878\n",
      "Epoch [52/90], lter [1521/1752] Loss: 2.5921\n",
      "Epoch [52/90], lter [1531/1752] Loss: 3.1085\n",
      "Epoch [52/90], lter [1541/1752] Loss: 2.0698\n",
      "Epoch [52/90], lter [1551/1752] Loss: 4.1715\n",
      "Epoch [52/90], lter [1561/1752] Loss: 2.3966\n",
      "Epoch [52/90], lter [1571/1752] Loss: 2.0554\n",
      "Epoch [52/90], lter [1581/1752] Loss: 3.2531\n",
      "Epoch [52/90], lter [1591/1752] Loss: 1.6493\n",
      "Epoch [52/90], lter [1601/1752] Loss: 3.3074\n",
      "Epoch [52/90], lter [1611/1752] Loss: 3.0087\n",
      "Epoch [52/90], lter [1621/1752] Loss: 2.7023\n",
      "Epoch [52/90], lter [1631/1752] Loss: 3.2303\n",
      "Epoch [52/90], lter [1641/1752] Loss: 3.5503\n",
      "Epoch [52/90], lter [1651/1752] Loss: 2.9860\n",
      "Epoch [52/90], lter [1661/1752] Loss: 3.1495\n",
      "Epoch [52/90], lter [1671/1752] Loss: 3.0224\n",
      "Epoch [52/90], lter [1681/1752] Loss: 2.9778\n",
      "Epoch [52/90], lter [1691/1752] Loss: 2.0276\n",
      "Epoch [52/90], lter [1701/1752] Loss: 3.8041\n",
      "Epoch [52/90], lter [1711/1752] Loss: 2.5770\n",
      "Epoch [52/90], lter [1721/1752] Loss: 2.8251\n",
      "Epoch [52/90], lter [1731/1752] Loss: 1.7535\n",
      "Epoch [52/90], lter [1741/1752] Loss: 2.3057\n",
      "Epoch [52/90], lter [1751/1752] Loss: 1.7255\n",
      "Epoch:  52 | train loss : 2.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 52/90 [76:48:59<12:53:43, 1221.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  51 | test loss : 2.2226\n",
      "Epoch [53/90], lter [1/1752] Loss: 3.2367\n",
      "Epoch [53/90], lter [11/1752] Loss: 1.5090\n",
      "Epoch [53/90], lter [21/1752] Loss: 3.1356\n",
      "Epoch [53/90], lter [31/1752] Loss: 2.8476\n",
      "Epoch [53/90], lter [41/1752] Loss: 2.6055\n",
      "Epoch [53/90], lter [51/1752] Loss: 2.8154\n",
      "Epoch [53/90], lter [61/1752] Loss: 2.3506\n",
      "Epoch [53/90], lter [71/1752] Loss: 2.0323\n",
      "Epoch [53/90], lter [81/1752] Loss: 2.9087\n",
      "Epoch [53/90], lter [91/1752] Loss: 2.1240\n",
      "Epoch [53/90], lter [101/1752] Loss: 2.9451\n",
      "Epoch [53/90], lter [111/1752] Loss: 4.0862\n",
      "Epoch [53/90], lter [121/1752] Loss: 2.3222\n",
      "Epoch [53/90], lter [131/1752] Loss: 3.1101\n",
      "Epoch [53/90], lter [141/1752] Loss: 2.7735\n",
      "Epoch [53/90], lter [151/1752] Loss: 2.6155\n",
      "Epoch [53/90], lter [161/1752] Loss: 3.2111\n",
      "Epoch [53/90], lter [171/1752] Loss: 2.8879\n",
      "Epoch [53/90], lter [181/1752] Loss: 3.0886\n",
      "Epoch [53/90], lter [191/1752] Loss: 2.7127\n",
      "Epoch [53/90], lter [201/1752] Loss: 1.9560\n",
      "Epoch [53/90], lter [211/1752] Loss: 1.9607\n",
      "Epoch [53/90], lter [221/1752] Loss: 2.9436\n",
      "Epoch [53/90], lter [231/1752] Loss: 3.5877\n",
      "Epoch [53/90], lter [241/1752] Loss: 2.1640\n",
      "Epoch [53/90], lter [251/1752] Loss: 3.2781\n",
      "Epoch [53/90], lter [261/1752] Loss: 3.2244\n",
      "Epoch [53/90], lter [271/1752] Loss: 2.9757\n",
      "Epoch [53/90], lter [281/1752] Loss: 1.6923\n",
      "Epoch [53/90], lter [291/1752] Loss: 1.6616\n",
      "Epoch [53/90], lter [301/1752] Loss: 1.8185\n",
      "Epoch [53/90], lter [311/1752] Loss: 2.1023\n",
      "Epoch [53/90], lter [321/1752] Loss: 3.9769\n",
      "Epoch [53/90], lter [331/1752] Loss: 2.4045\n",
      "Epoch [53/90], lter [341/1752] Loss: 2.4348\n",
      "Epoch [53/90], lter [351/1752] Loss: 1.6407\n",
      "Epoch [53/90], lter [361/1752] Loss: 1.8194\n",
      "Epoch [53/90], lter [371/1752] Loss: 2.4369\n",
      "Epoch [53/90], lter [381/1752] Loss: 3.8587\n",
      "Epoch [53/90], lter [391/1752] Loss: 2.7361\n",
      "Epoch [53/90], lter [401/1752] Loss: 1.4155\n",
      "Epoch [53/90], lter [411/1752] Loss: 3.1059\n",
      "Epoch [53/90], lter [421/1752] Loss: 3.6099\n",
      "Epoch [53/90], lter [431/1752] Loss: 2.9536\n",
      "Epoch [53/90], lter [441/1752] Loss: 3.2439\n",
      "Epoch [53/90], lter [451/1752] Loss: 2.5072\n",
      "Epoch [53/90], lter [461/1752] Loss: 2.5401\n",
      "Epoch [53/90], lter [471/1752] Loss: 3.0929\n",
      "Epoch [53/90], lter [481/1752] Loss: 2.9108\n",
      "Epoch [53/90], lter [491/1752] Loss: 2.1669\n",
      "Epoch [53/90], lter [501/1752] Loss: 2.2571\n",
      "Epoch [53/90], lter [511/1752] Loss: 3.1743\n",
      "Epoch [53/90], lter [521/1752] Loss: 1.9640\n",
      "Epoch [53/90], lter [531/1752] Loss: 2.8359\n",
      "Epoch [53/90], lter [541/1752] Loss: 2.7187\n",
      "Epoch [53/90], lter [551/1752] Loss: 2.7954\n",
      "Epoch [53/90], lter [561/1752] Loss: 2.0617\n",
      "Epoch [53/90], lter [571/1752] Loss: 1.9856\n",
      "Epoch [53/90], lter [581/1752] Loss: 3.1893\n",
      "Epoch [53/90], lter [591/1752] Loss: 2.4525\n",
      "Epoch [53/90], lter [601/1752] Loss: 4.1507\n",
      "Epoch [53/90], lter [611/1752] Loss: 3.3474\n",
      "Epoch [53/90], lter [621/1752] Loss: 2.8636\n",
      "Epoch [53/90], lter [631/1752] Loss: 1.8009\n",
      "Epoch [53/90], lter [641/1752] Loss: 2.4839\n",
      "Epoch [53/90], lter [651/1752] Loss: 2.5608\n",
      "Epoch [53/90], lter [661/1752] Loss: 2.3022\n",
      "Epoch [53/90], lter [671/1752] Loss: 2.2725\n",
      "Epoch [53/90], lter [681/1752] Loss: 1.9122\n",
      "Epoch [53/90], lter [691/1752] Loss: 2.4204\n",
      "Epoch [53/90], lter [701/1752] Loss: 2.5858\n",
      "Epoch [53/90], lter [711/1752] Loss: 2.0150\n",
      "Epoch [53/90], lter [721/1752] Loss: 2.9614\n",
      "Epoch [53/90], lter [731/1752] Loss: 2.3514\n",
      "Epoch [53/90], lter [741/1752] Loss: 3.6880\n",
      "Epoch [53/90], lter [751/1752] Loss: 2.7437\n",
      "Epoch [53/90], lter [761/1752] Loss: 2.5779\n",
      "Epoch [53/90], lter [771/1752] Loss: 4.4998\n",
      "Epoch [53/90], lter [781/1752] Loss: 3.5214\n",
      "Epoch [53/90], lter [791/1752] Loss: 4.5942\n",
      "Epoch [53/90], lter [801/1752] Loss: 2.5453\n",
      "Epoch [53/90], lter [811/1752] Loss: 2.7320\n",
      "Epoch [53/90], lter [821/1752] Loss: 3.2238\n",
      "Epoch [53/90], lter [831/1752] Loss: 2.1961\n",
      "Epoch [53/90], lter [841/1752] Loss: 2.1652\n",
      "Epoch [53/90], lter [851/1752] Loss: 2.9373\n",
      "Epoch [53/90], lter [861/1752] Loss: 2.9678\n",
      "Epoch [53/90], lter [871/1752] Loss: 2.2497\n",
      "Epoch [53/90], lter [881/1752] Loss: 3.1785\n",
      "Epoch [53/90], lter [891/1752] Loss: 2.2720\n",
      "Epoch [53/90], lter [901/1752] Loss: 1.8451\n",
      "Epoch [53/90], lter [911/1752] Loss: 2.4973\n",
      "Epoch [53/90], lter [921/1752] Loss: 1.7659\n",
      "Epoch [53/90], lter [931/1752] Loss: 2.2762\n",
      "Epoch [53/90], lter [941/1752] Loss: 3.9874\n",
      "Epoch [53/90], lter [951/1752] Loss: 2.1379\n",
      "Epoch [53/90], lter [961/1752] Loss: 2.9145\n",
      "Epoch [53/90], lter [971/1752] Loss: 3.1225\n",
      "Epoch [53/90], lter [981/1752] Loss: 3.0681\n",
      "Epoch [53/90], lter [991/1752] Loss: 3.0412\n",
      "Epoch [53/90], lter [1001/1752] Loss: 2.3929\n",
      "Epoch [53/90], lter [1011/1752] Loss: 3.1328\n",
      "Epoch [53/90], lter [1021/1752] Loss: 1.7315\n",
      "Epoch [53/90], lter [1031/1752] Loss: 1.7879\n",
      "Epoch [53/90], lter [1041/1752] Loss: 3.0729\n",
      "Epoch [53/90], lter [1051/1752] Loss: 3.2379\n",
      "Epoch [53/90], lter [1061/1752] Loss: 2.9012\n",
      "Epoch [53/90], lter [1071/1752] Loss: 1.2818\n",
      "Epoch [53/90], lter [1081/1752] Loss: 2.8848\n",
      "Epoch [53/90], lter [1091/1752] Loss: 1.9028\n",
      "Epoch [53/90], lter [1101/1752] Loss: 2.8920\n",
      "Epoch [53/90], lter [1111/1752] Loss: 3.4109\n",
      "Epoch [53/90], lter [1121/1752] Loss: 1.9087\n",
      "Epoch [53/90], lter [1131/1752] Loss: 2.0789\n",
      "Epoch [53/90], lter [1141/1752] Loss: 2.1686\n",
      "Epoch [53/90], lter [1151/1752] Loss: 1.7707\n",
      "Epoch [53/90], lter [1161/1752] Loss: 1.5027\n",
      "Epoch [53/90], lter [1171/1752] Loss: 3.4240\n",
      "Epoch [53/90], lter [1181/1752] Loss: 3.2656\n",
      "Epoch [53/90], lter [1191/1752] Loss: 2.7874\n",
      "Epoch [53/90], lter [1201/1752] Loss: 2.2458\n",
      "Epoch [53/90], lter [1211/1752] Loss: 3.5582\n",
      "Epoch [53/90], lter [1221/1752] Loss: 2.2444\n",
      "Epoch [53/90], lter [1231/1752] Loss: 2.8708\n",
      "Epoch [53/90], lter [1241/1752] Loss: 3.1078\n",
      "Epoch [53/90], lter [1251/1752] Loss: 2.7536\n",
      "Epoch [53/90], lter [1261/1752] Loss: 1.6650\n",
      "Epoch [53/90], lter [1271/1752] Loss: 2.7809\n",
      "Epoch [53/90], lter [1281/1752] Loss: 2.2083\n",
      "Epoch [53/90], lter [1291/1752] Loss: 1.8590\n",
      "Epoch [53/90], lter [1301/1752] Loss: 2.7925\n",
      "Epoch [53/90], lter [1311/1752] Loss: 1.6784\n",
      "Epoch [53/90], lter [1321/1752] Loss: 2.4923\n",
      "Epoch [53/90], lter [1331/1752] Loss: 3.1109\n",
      "Epoch [53/90], lter [1341/1752] Loss: 1.5934\n",
      "Epoch [53/90], lter [1351/1752] Loss: 2.9620\n",
      "Epoch [53/90], lter [1361/1752] Loss: 3.8288\n",
      "Epoch [53/90], lter [1371/1752] Loss: 2.5562\n",
      "Epoch [53/90], lter [1381/1752] Loss: 1.2562\n",
      "Epoch [53/90], lter [1391/1752] Loss: 2.8058\n",
      "Epoch [53/90], lter [1401/1752] Loss: 2.8922\n",
      "Epoch [53/90], lter [1411/1752] Loss: 4.1068\n",
      "Epoch [53/90], lter [1421/1752] Loss: 3.9028\n",
      "Epoch [53/90], lter [1431/1752] Loss: 3.5864\n",
      "Epoch [53/90], lter [1441/1752] Loss: 3.7400\n",
      "Epoch [53/90], lter [1451/1752] Loss: 2.9692\n",
      "Epoch [53/90], lter [1461/1752] Loss: 1.7972\n",
      "Epoch [53/90], lter [1471/1752] Loss: 2.1783\n",
      "Epoch [53/90], lter [1481/1752] Loss: 2.0886\n",
      "Epoch [53/90], lter [1491/1752] Loss: 4.1939\n",
      "Epoch [53/90], lter [1501/1752] Loss: 2.4385\n",
      "Epoch [53/90], lter [1511/1752] Loss: 2.4517\n",
      "Epoch [53/90], lter [1521/1752] Loss: 4.2379\n",
      "Epoch [53/90], lter [1531/1752] Loss: 1.7627\n",
      "Epoch [53/90], lter [1541/1752] Loss: 2.6455\n",
      "Epoch [53/90], lter [1551/1752] Loss: 3.3734\n",
      "Epoch [53/90], lter [1561/1752] Loss: 1.8323\n",
      "Epoch [53/90], lter [1571/1752] Loss: 2.3638\n",
      "Epoch [53/90], lter [1581/1752] Loss: 1.4679\n",
      "Epoch [53/90], lter [1591/1752] Loss: 3.4415\n",
      "Epoch [53/90], lter [1601/1752] Loss: 3.2460\n",
      "Epoch [53/90], lter [1611/1752] Loss: 3.5783\n",
      "Epoch [53/90], lter [1621/1752] Loss: 1.9766\n",
      "Epoch [53/90], lter [1631/1752] Loss: 2.2752\n",
      "Epoch [53/90], lter [1641/1752] Loss: 2.5838\n",
      "Epoch [53/90], lter [1651/1752] Loss: 1.9959\n",
      "Epoch [53/90], lter [1661/1752] Loss: 2.8502\n",
      "Epoch [53/90], lter [1671/1752] Loss: 2.1359\n",
      "Epoch [53/90], lter [1681/1752] Loss: 4.6427\n",
      "Epoch [53/90], lter [1691/1752] Loss: 2.1540\n",
      "Epoch [53/90], lter [1701/1752] Loss: 1.9124\n",
      "Epoch [53/90], lter [1711/1752] Loss: 2.9862\n",
      "Epoch [53/90], lter [1721/1752] Loss: 2.6267\n",
      "Epoch [53/90], lter [1731/1752] Loss: 2.9827\n",
      "Epoch [53/90], lter [1741/1752] Loss: 2.5154\n",
      "Epoch [53/90], lter [1751/1752] Loss: 2.3431\n",
      "Epoch:  53 | train loss : 2.7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 53/90 [77:09:23<12:33:46, 1222.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  52 | test loss : 1.9288\n",
      "Epoch [54/90], lter [1/1752] Loss: 2.8763\n",
      "Epoch [54/90], lter [11/1752] Loss: 2.4455\n",
      "Epoch [54/90], lter [21/1752] Loss: 2.4184\n",
      "Epoch [54/90], lter [31/1752] Loss: 3.5775\n",
      "Epoch [54/90], lter [41/1752] Loss: 2.4201\n",
      "Epoch [54/90], lter [51/1752] Loss: 2.8741\n",
      "Epoch [54/90], lter [61/1752] Loss: 1.5837\n",
      "Epoch [54/90], lter [71/1752] Loss: 2.9324\n",
      "Epoch [54/90], lter [81/1752] Loss: 4.0449\n",
      "Epoch [54/90], lter [91/1752] Loss: 3.0889\n",
      "Epoch [54/90], lter [101/1752] Loss: 3.1805\n",
      "Epoch [54/90], lter [111/1752] Loss: 2.7857\n",
      "Epoch [54/90], lter [121/1752] Loss: 1.9437\n",
      "Epoch [54/90], lter [131/1752] Loss: 3.3521\n",
      "Epoch [54/90], lter [141/1752] Loss: 2.8036\n",
      "Epoch [54/90], lter [151/1752] Loss: 2.7445\n",
      "Epoch [54/90], lter [161/1752] Loss: 2.2657\n",
      "Epoch [54/90], lter [171/1752] Loss: 4.1992\n",
      "Epoch [54/90], lter [181/1752] Loss: 3.3922\n",
      "Epoch [54/90], lter [191/1752] Loss: 2.1446\n",
      "Epoch [54/90], lter [201/1752] Loss: 2.1792\n",
      "Epoch [54/90], lter [211/1752] Loss: 2.9823\n",
      "Epoch [54/90], lter [221/1752] Loss: 3.8501\n",
      "Epoch [54/90], lter [231/1752] Loss: 3.6949\n",
      "Epoch [54/90], lter [241/1752] Loss: 3.1034\n",
      "Epoch [54/90], lter [251/1752] Loss: 3.1477\n",
      "Epoch [54/90], lter [261/1752] Loss: 3.1613\n",
      "Epoch [54/90], lter [271/1752] Loss: 3.4908\n",
      "Epoch [54/90], lter [281/1752] Loss: 3.5236\n",
      "Epoch [54/90], lter [291/1752] Loss: 3.3350\n",
      "Epoch [54/90], lter [301/1752] Loss: 1.9601\n",
      "Epoch [54/90], lter [311/1752] Loss: 2.2643\n",
      "Epoch [54/90], lter [321/1752] Loss: 2.3396\n",
      "Epoch [54/90], lter [331/1752] Loss: 2.3179\n",
      "Epoch [54/90], lter [341/1752] Loss: 2.0039\n",
      "Epoch [54/90], lter [351/1752] Loss: 2.3031\n",
      "Epoch [54/90], lter [361/1752] Loss: 2.9208\n",
      "Epoch [54/90], lter [371/1752] Loss: 3.0616\n",
      "Epoch [54/90], lter [381/1752] Loss: 3.1890\n",
      "Epoch [54/90], lter [391/1752] Loss: 2.8598\n",
      "Epoch [54/90], lter [401/1752] Loss: 2.8900\n",
      "Epoch [54/90], lter [411/1752] Loss: 3.2356\n",
      "Epoch [54/90], lter [421/1752] Loss: 2.9682\n",
      "Epoch [54/90], lter [431/1752] Loss: 2.6464\n",
      "Epoch [54/90], lter [441/1752] Loss: 2.3371\n",
      "Epoch [54/90], lter [451/1752] Loss: 3.2531\n",
      "Epoch [54/90], lter [461/1752] Loss: 2.1135\n",
      "Epoch [54/90], lter [471/1752] Loss: 2.4170\n",
      "Epoch [54/90], lter [481/1752] Loss: 2.1387\n",
      "Epoch [54/90], lter [491/1752] Loss: 2.8336\n",
      "Epoch [54/90], lter [501/1752] Loss: 2.8218\n",
      "Epoch [54/90], lter [511/1752] Loss: 1.4092\n",
      "Epoch [54/90], lter [521/1752] Loss: 2.8898\n",
      "Epoch [54/90], lter [531/1752] Loss: 3.3837\n",
      "Epoch [54/90], lter [541/1752] Loss: 2.6799\n",
      "Epoch [54/90], lter [551/1752] Loss: 3.0559\n",
      "Epoch [54/90], lter [561/1752] Loss: 3.8034\n",
      "Epoch [54/90], lter [571/1752] Loss: 3.7003\n",
      "Epoch [54/90], lter [581/1752] Loss: 3.7232\n",
      "Epoch [54/90], lter [591/1752] Loss: 2.9330\n",
      "Epoch [54/90], lter [601/1752] Loss: 2.6514\n",
      "Epoch [54/90], lter [611/1752] Loss: 2.3737\n",
      "Epoch [54/90], lter [621/1752] Loss: 4.7502\n",
      "Epoch [54/90], lter [631/1752] Loss: 3.1987\n",
      "Epoch [54/90], lter [641/1752] Loss: 2.3929\n",
      "Epoch [54/90], lter [651/1752] Loss: 1.6679\n",
      "Epoch [54/90], lter [661/1752] Loss: 2.5262\n",
      "Epoch [54/90], lter [671/1752] Loss: 2.0941\n",
      "Epoch [54/90], lter [681/1752] Loss: 1.5303\n",
      "Epoch [54/90], lter [691/1752] Loss: 2.3590\n",
      "Epoch [54/90], lter [701/1752] Loss: 2.5844\n",
      "Epoch [54/90], lter [711/1752] Loss: 3.1641\n",
      "Epoch [54/90], lter [721/1752] Loss: 2.1313\n",
      "Epoch [54/90], lter [731/1752] Loss: 3.4543\n",
      "Epoch [54/90], lter [741/1752] Loss: 2.0304\n",
      "Epoch [54/90], lter [751/1752] Loss: 2.4314\n",
      "Epoch [54/90], lter [761/1752] Loss: 2.5692\n",
      "Epoch [54/90], lter [771/1752] Loss: 3.2066\n",
      "Epoch [54/90], lter [781/1752] Loss: 2.6402\n",
      "Epoch [54/90], lter [791/1752] Loss: 2.5811\n",
      "Epoch [54/90], lter [801/1752] Loss: 2.9839\n",
      "Epoch [54/90], lter [811/1752] Loss: 3.4740\n",
      "Epoch [54/90], lter [821/1752] Loss: 2.9603\n",
      "Epoch [54/90], lter [831/1752] Loss: 3.1381\n",
      "Epoch [54/90], lter [841/1752] Loss: 1.8779\n",
      "Epoch [54/90], lter [851/1752] Loss: 3.6996\n",
      "Epoch [54/90], lter [861/1752] Loss: 3.3263\n",
      "Epoch [54/90], lter [871/1752] Loss: 2.7619\n",
      "Epoch [54/90], lter [881/1752] Loss: 1.5230\n",
      "Epoch [54/90], lter [891/1752] Loss: 2.6788\n",
      "Epoch [54/90], lter [901/1752] Loss: 2.5356\n",
      "Epoch [54/90], lter [911/1752] Loss: 2.4843\n",
      "Epoch [54/90], lter [921/1752] Loss: 2.5817\n",
      "Epoch [54/90], lter [931/1752] Loss: 2.2423\n",
      "Epoch [54/90], lter [941/1752] Loss: 4.8459\n",
      "Epoch [54/90], lter [951/1752] Loss: 2.2488\n",
      "Epoch [54/90], lter [961/1752] Loss: 2.5255\n",
      "Epoch [54/90], lter [971/1752] Loss: 3.3178\n",
      "Epoch [54/90], lter [981/1752] Loss: 3.0518\n",
      "Epoch [54/90], lter [991/1752] Loss: 2.9388\n",
      "Epoch [54/90], lter [1001/1752] Loss: 2.3768\n",
      "Epoch [54/90], lter [1011/1752] Loss: 1.2762\n",
      "Epoch [54/90], lter [1021/1752] Loss: 2.0219\n",
      "Epoch [54/90], lter [1031/1752] Loss: 2.8132\n",
      "Epoch [54/90], lter [1041/1752] Loss: 3.3195\n",
      "Epoch [54/90], lter [1051/1752] Loss: 2.2007\n",
      "Epoch [54/90], lter [1061/1752] Loss: 3.5744\n",
      "Epoch [54/90], lter [1071/1752] Loss: 2.0480\n",
      "Epoch [54/90], lter [1081/1752] Loss: 2.7472\n",
      "Epoch [54/90], lter [1091/1752] Loss: 2.6765\n",
      "Epoch [54/90], lter [1101/1752] Loss: 3.2809\n",
      "Epoch [54/90], lter [1111/1752] Loss: 2.5477\n",
      "Epoch [54/90], lter [1121/1752] Loss: 2.3757\n",
      "Epoch [54/90], lter [1131/1752] Loss: 2.7823\n",
      "Epoch [54/90], lter [1141/1752] Loss: 2.1905\n",
      "Epoch [54/90], lter [1151/1752] Loss: 3.4749\n",
      "Epoch [54/90], lter [1161/1752] Loss: 3.4331\n",
      "Epoch [54/90], lter [1171/1752] Loss: 3.4238\n",
      "Epoch [54/90], lter [1181/1752] Loss: 2.6573\n",
      "Epoch [54/90], lter [1191/1752] Loss: 3.3011\n",
      "Epoch [54/90], lter [1201/1752] Loss: 2.5690\n",
      "Epoch [54/90], lter [1211/1752] Loss: 2.8054\n",
      "Epoch [54/90], lter [1221/1752] Loss: 2.0176\n",
      "Epoch [54/90], lter [1231/1752] Loss: 2.9207\n",
      "Epoch [54/90], lter [1241/1752] Loss: 4.4323\n",
      "Epoch [54/90], lter [1251/1752] Loss: 2.7659\n",
      "Epoch [54/90], lter [1261/1752] Loss: 2.6148\n",
      "Epoch [54/90], lter [1271/1752] Loss: 1.7912\n",
      "Epoch [54/90], lter [1281/1752] Loss: 2.6148\n",
      "Epoch [54/90], lter [1291/1752] Loss: 2.3231\n",
      "Epoch [54/90], lter [1301/1752] Loss: 3.1409\n",
      "Epoch [54/90], lter [1311/1752] Loss: 2.1325\n",
      "Epoch [54/90], lter [1321/1752] Loss: 2.5793\n",
      "Epoch [54/90], lter [1331/1752] Loss: 2.2560\n",
      "Epoch [54/90], lter [1341/1752] Loss: 2.6083\n",
      "Epoch [54/90], lter [1351/1752] Loss: 2.5829\n",
      "Epoch [54/90], lter [1361/1752] Loss: 2.7670\n",
      "Epoch [54/90], lter [1371/1752] Loss: 1.8977\n",
      "Epoch [54/90], lter [1381/1752] Loss: 4.2985\n",
      "Epoch [54/90], lter [1391/1752] Loss: 3.3786\n",
      "Epoch [54/90], lter [1401/1752] Loss: 2.3003\n",
      "Epoch [54/90], lter [1411/1752] Loss: 3.4405\n",
      "Epoch [54/90], lter [1421/1752] Loss: 3.4263\n",
      "Epoch [54/90], lter [1431/1752] Loss: 2.2790\n",
      "Epoch [54/90], lter [1441/1752] Loss: 3.5551\n",
      "Epoch [54/90], lter [1451/1752] Loss: 2.6745\n",
      "Epoch [54/90], lter [1461/1752] Loss: 3.2747\n",
      "Epoch [54/90], lter [1471/1752] Loss: 3.6623\n",
      "Epoch [54/90], lter [1481/1752] Loss: 2.5044\n",
      "Epoch [54/90], lter [1491/1752] Loss: 2.9585\n",
      "Epoch [54/90], lter [1501/1752] Loss: 3.9918\n",
      "Epoch [54/90], lter [1511/1752] Loss: 3.6592\n",
      "Epoch [54/90], lter [1521/1752] Loss: 1.4970\n",
      "Epoch [54/90], lter [1531/1752] Loss: 3.7564\n",
      "Epoch [54/90], lter [1541/1752] Loss: 3.1001\n",
      "Epoch [54/90], lter [1551/1752] Loss: 4.8422\n",
      "Epoch [54/90], lter [1561/1752] Loss: 2.5636\n",
      "Epoch [54/90], lter [1571/1752] Loss: 2.7046\n",
      "Epoch [54/90], lter [1581/1752] Loss: 2.3168\n",
      "Epoch [54/90], lter [1591/1752] Loss: 1.8721\n",
      "Epoch [54/90], lter [1601/1752] Loss: 3.5914\n",
      "Epoch [54/90], lter [1611/1752] Loss: 2.3996\n",
      "Epoch [54/90], lter [1621/1752] Loss: 2.3182\n",
      "Epoch [54/90], lter [1631/1752] Loss: 2.2088\n",
      "Epoch [54/90], lter [1641/1752] Loss: 3.0253\n",
      "Epoch [54/90], lter [1651/1752] Loss: 2.9414\n",
      "Epoch [54/90], lter [1661/1752] Loss: 2.2742\n",
      "Epoch [54/90], lter [1671/1752] Loss: 3.0109\n",
      "Epoch [54/90], lter [1681/1752] Loss: 2.3288\n",
      "Epoch [54/90], lter [1691/1752] Loss: 2.8375\n",
      "Epoch [54/90], lter [1701/1752] Loss: 3.4365\n",
      "Epoch [54/90], lter [1711/1752] Loss: 2.9465\n",
      "Epoch [54/90], lter [1721/1752] Loss: 3.6966\n",
      "Epoch [54/90], lter [1731/1752] Loss: 3.3124\n",
      "Epoch [54/90], lter [1741/1752] Loss: 3.1745\n",
      "Epoch [54/90], lter [1751/1752] Loss: 3.3331\n",
      "Epoch:  54 | train loss : 2.8025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 54/90 [77:29:45<12:13:24, 1222.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  53 | test loss : 2.2016\n",
      "Epoch [55/90], lter [1/1752] Loss: 1.6602\n",
      "Epoch [55/90], lter [11/1752] Loss: 2.1523\n",
      "Epoch [55/90], lter [21/1752] Loss: 1.7008\n",
      "Epoch [55/90], lter [31/1752] Loss: 3.2713\n",
      "Epoch [55/90], lter [41/1752] Loss: 3.1268\n",
      "Epoch [55/90], lter [51/1752] Loss: 2.7366\n",
      "Epoch [55/90], lter [61/1752] Loss: 2.2475\n",
      "Epoch [55/90], lter [71/1752] Loss: 3.0912\n",
      "Epoch [55/90], lter [81/1752] Loss: 3.1722\n",
      "Epoch [55/90], lter [91/1752] Loss: 3.2676\n",
      "Epoch [55/90], lter [101/1752] Loss: 3.3916\n",
      "Epoch [55/90], lter [111/1752] Loss: 2.5814\n",
      "Epoch [55/90], lter [121/1752] Loss: 1.8750\n",
      "Epoch [55/90], lter [131/1752] Loss: 2.4426\n",
      "Epoch [55/90], lter [141/1752] Loss: 2.3729\n",
      "Epoch [55/90], lter [151/1752] Loss: 3.4046\n",
      "Epoch [55/90], lter [161/1752] Loss: 2.8543\n",
      "Epoch [55/90], lter [171/1752] Loss: 2.4200\n",
      "Epoch [55/90], lter [181/1752] Loss: 3.5979\n",
      "Epoch [55/90], lter [191/1752] Loss: 2.4075\n",
      "Epoch [55/90], lter [201/1752] Loss: 3.7455\n",
      "Epoch [55/90], lter [211/1752] Loss: 2.9054\n",
      "Epoch [55/90], lter [221/1752] Loss: 3.0686\n",
      "Epoch [55/90], lter [231/1752] Loss: 2.7231\n",
      "Epoch [55/90], lter [241/1752] Loss: 2.5001\n",
      "Epoch [55/90], lter [251/1752] Loss: 2.7979\n",
      "Epoch [55/90], lter [261/1752] Loss: 1.5492\n",
      "Epoch [55/90], lter [271/1752] Loss: 2.8249\n",
      "Epoch [55/90], lter [281/1752] Loss: 3.2668\n",
      "Epoch [55/90], lter [291/1752] Loss: 1.9422\n",
      "Epoch [55/90], lter [301/1752] Loss: 2.7800\n",
      "Epoch [55/90], lter [311/1752] Loss: 2.9826\n",
      "Epoch [55/90], lter [321/1752] Loss: 2.6581\n",
      "Epoch [55/90], lter [331/1752] Loss: 2.6484\n",
      "Epoch [55/90], lter [341/1752] Loss: 3.0076\n",
      "Epoch [55/90], lter [351/1752] Loss: 2.8414\n",
      "Epoch [55/90], lter [361/1752] Loss: 2.2830\n",
      "Epoch [55/90], lter [371/1752] Loss: 1.9520\n",
      "Epoch [55/90], lter [381/1752] Loss: 3.0749\n",
      "Epoch [55/90], lter [391/1752] Loss: 2.6768\n",
      "Epoch [55/90], lter [401/1752] Loss: 1.8073\n",
      "Epoch [55/90], lter [411/1752] Loss: 2.8866\n",
      "Epoch [55/90], lter [421/1752] Loss: 2.6617\n",
      "Epoch [55/90], lter [431/1752] Loss: 3.9637\n",
      "Epoch [55/90], lter [441/1752] Loss: 3.9007\n",
      "Epoch [55/90], lter [451/1752] Loss: 3.5131\n",
      "Epoch [55/90], lter [461/1752] Loss: 1.2662\n",
      "Epoch [55/90], lter [471/1752] Loss: 1.8193\n",
      "Epoch [55/90], lter [481/1752] Loss: 4.0578\n",
      "Epoch [55/90], lter [491/1752] Loss: 2.0462\n",
      "Epoch [55/90], lter [501/1752] Loss: 2.4826\n",
      "Epoch [55/90], lter [511/1752] Loss: 1.8873\n",
      "Epoch [55/90], lter [521/1752] Loss: 2.5894\n",
      "Epoch [55/90], lter [531/1752] Loss: 3.3606\n",
      "Epoch [55/90], lter [541/1752] Loss: 2.4465\n",
      "Epoch [55/90], lter [551/1752] Loss: 3.0963\n",
      "Epoch [55/90], lter [561/1752] Loss: 2.5772\n",
      "Epoch [55/90], lter [571/1752] Loss: 2.8073\n",
      "Epoch [55/90], lter [581/1752] Loss: 2.3379\n",
      "Epoch [55/90], lter [591/1752] Loss: 2.6991\n",
      "Epoch [55/90], lter [601/1752] Loss: 3.2289\n",
      "Epoch [55/90], lter [611/1752] Loss: 2.1950\n",
      "Epoch [55/90], lter [621/1752] Loss: 3.2625\n",
      "Epoch [55/90], lter [631/1752] Loss: 3.4031\n",
      "Epoch [55/90], lter [641/1752] Loss: 2.1189\n",
      "Epoch [55/90], lter [651/1752] Loss: 3.1549\n",
      "Epoch [55/90], lter [661/1752] Loss: 3.0176\n",
      "Epoch [55/90], lter [671/1752] Loss: 2.3258\n",
      "Epoch [55/90], lter [681/1752] Loss: 3.6513\n",
      "Epoch [55/90], lter [691/1752] Loss: 2.9323\n",
      "Epoch [55/90], lter [701/1752] Loss: 2.2072\n",
      "Epoch [55/90], lter [711/1752] Loss: 2.5832\n",
      "Epoch [55/90], lter [721/1752] Loss: 2.2259\n",
      "Epoch [55/90], lter [731/1752] Loss: 2.3106\n",
      "Epoch [55/90], lter [741/1752] Loss: 3.5747\n",
      "Epoch [55/90], lter [751/1752] Loss: 2.4780\n",
      "Epoch [55/90], lter [761/1752] Loss: 3.6297\n",
      "Epoch [55/90], lter [771/1752] Loss: 3.1398\n",
      "Epoch [55/90], lter [781/1752] Loss: 2.6162\n",
      "Epoch [55/90], lter [791/1752] Loss: 3.6448\n",
      "Epoch [55/90], lter [801/1752] Loss: 2.6027\n",
      "Epoch [55/90], lter [811/1752] Loss: 2.9308\n",
      "Epoch [55/90], lter [821/1752] Loss: 3.1069\n",
      "Epoch [55/90], lter [831/1752] Loss: 3.8594\n",
      "Epoch [55/90], lter [841/1752] Loss: 1.8529\n",
      "Epoch [55/90], lter [851/1752] Loss: 3.8451\n",
      "Epoch [55/90], lter [861/1752] Loss: 2.5386\n",
      "Epoch [55/90], lter [871/1752] Loss: 3.4860\n",
      "Epoch [55/90], lter [881/1752] Loss: 3.6003\n",
      "Epoch [55/90], lter [891/1752] Loss: 1.5118\n",
      "Epoch [55/90], lter [901/1752] Loss: 3.1663\n",
      "Epoch [55/90], lter [911/1752] Loss: 1.8264\n",
      "Epoch [55/90], lter [921/1752] Loss: 2.6274\n",
      "Epoch [55/90], lter [931/1752] Loss: 3.3510\n",
      "Epoch [55/90], lter [941/1752] Loss: 2.5123\n",
      "Epoch [55/90], lter [951/1752] Loss: 2.7634\n",
      "Epoch [55/90], lter [961/1752] Loss: 2.6951\n",
      "Epoch [55/90], lter [971/1752] Loss: 1.9619\n",
      "Epoch [55/90], lter [981/1752] Loss: 3.1444\n",
      "Epoch [55/90], lter [991/1752] Loss: 2.9593\n",
      "Epoch [55/90], lter [1001/1752] Loss: 3.2913\n",
      "Epoch [55/90], lter [1011/1752] Loss: 2.0503\n",
      "Epoch [55/90], lter [1021/1752] Loss: 2.9495\n",
      "Epoch [55/90], lter [1031/1752] Loss: 2.2585\n",
      "Epoch [55/90], lter [1041/1752] Loss: 2.4726\n",
      "Epoch [55/90], lter [1051/1752] Loss: 2.1529\n",
      "Epoch [55/90], lter [1061/1752] Loss: 3.9183\n",
      "Epoch [55/90], lter [1071/1752] Loss: 2.9565\n",
      "Epoch [55/90], lter [1081/1752] Loss: 2.8307\n",
      "Epoch [55/90], lter [1091/1752] Loss: 3.8056\n",
      "Epoch [55/90], lter [1101/1752] Loss: 3.1322\n",
      "Epoch [55/90], lter [1111/1752] Loss: 2.3813\n",
      "Epoch [55/90], lter [1121/1752] Loss: 2.9642\n",
      "Epoch [55/90], lter [1131/1752] Loss: 2.7628\n",
      "Epoch [55/90], lter [1141/1752] Loss: 2.3417\n",
      "Epoch [55/90], lter [1151/1752] Loss: 2.9054\n",
      "Epoch [55/90], lter [1161/1752] Loss: 2.2048\n",
      "Epoch [55/90], lter [1171/1752] Loss: 3.6583\n",
      "Epoch [55/90], lter [1181/1752] Loss: 3.4420\n",
      "Epoch [55/90], lter [1191/1752] Loss: 1.7758\n",
      "Epoch [55/90], lter [1201/1752] Loss: 2.7660\n",
      "Epoch [55/90], lter [1211/1752] Loss: 2.9385\n",
      "Epoch [55/90], lter [1221/1752] Loss: 2.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/90], lter [361/1752] Loss: 1.3931\n",
      "Epoch [82/90], lter [371/1752] Loss: 1.9494\n",
      "Epoch [82/90], lter [381/1752] Loss: 1.8064\n",
      "Epoch [82/90], lter [391/1752] Loss: 1.5116\n",
      "Epoch [82/90], lter [401/1752] Loss: 1.8631\n",
      "Epoch [82/90], lter [411/1752] Loss: 1.4383\n",
      "Epoch [82/90], lter [421/1752] Loss: 1.7976\n",
      "Epoch [82/90], lter [431/1752] Loss: 1.6961\n",
      "Epoch [82/90], lter [441/1752] Loss: 2.0432\n",
      "Epoch [82/90], lter [451/1752] Loss: 1.6041\n",
      "Epoch [82/90], lter [461/1752] Loss: 1.7248\n",
      "Epoch [82/90], lter [471/1752] Loss: 1.4768\n",
      "Epoch [82/90], lter [481/1752] Loss: 1.8028\n",
      "Epoch [82/90], lter [491/1752] Loss: 1.8521\n",
      "Epoch [82/90], lter [501/1752] Loss: 1.5909\n",
      "Epoch [82/90], lter [511/1752] Loss: 1.3418\n",
      "Epoch [82/90], lter [521/1752] Loss: 1.6152\n",
      "Epoch [82/90], lter [531/1752] Loss: 1.7131\n",
      "Epoch [82/90], lter [541/1752] Loss: 1.6945\n",
      "Epoch [82/90], lter [551/1752] Loss: 1.6871\n",
      "Epoch [82/90], lter [561/1752] Loss: 1.6675\n",
      "Epoch [82/90], lter [571/1752] Loss: 1.7723\n",
      "Epoch [82/90], lter [581/1752] Loss: 1.6079\n",
      "Epoch [82/90], lter [591/1752] Loss: 1.9402\n",
      "Epoch [82/90], lter [601/1752] Loss: 1.5904\n",
      "Epoch [82/90], lter [611/1752] Loss: 1.8747\n",
      "Epoch [82/90], lter [621/1752] Loss: 1.4816\n",
      "Epoch [82/90], lter [631/1752] Loss: 1.6661\n",
      "Epoch [82/90], lter [641/1752] Loss: 1.6483\n",
      "Epoch [82/90], lter [651/1752] Loss: 1.7684\n",
      "Epoch [82/90], lter [661/1752] Loss: 1.8016\n",
      "Epoch [82/90], lter [671/1752] Loss: 1.3618\n",
      "Epoch [82/90], lter [681/1752] Loss: 1.3960\n",
      "Epoch [82/90], lter [691/1752] Loss: 1.5180\n",
      "Epoch [82/90], lter [701/1752] Loss: 1.7985\n",
      "Epoch [82/90], lter [711/1752] Loss: 1.4895\n",
      "Epoch [82/90], lter [721/1752] Loss: 1.6506\n",
      "Epoch [82/90], lter [731/1752] Loss: 1.3527\n",
      "Epoch [82/90], lter [741/1752] Loss: 1.3548\n",
      "Epoch [82/90], lter [751/1752] Loss: 1.6432\n",
      "Epoch [82/90], lter [761/1752] Loss: 1.5661\n",
      "Epoch [82/90], lter [771/1752] Loss: 2.0188\n",
      "Epoch [82/90], lter [781/1752] Loss: 1.6354\n",
      "Epoch [82/90], lter [791/1752] Loss: 1.7558\n",
      "Epoch [82/90], lter [801/1752] Loss: 1.8388\n",
      "Epoch [82/90], lter [811/1752] Loss: 1.5765\n",
      "Epoch [82/90], lter [821/1752] Loss: 1.5589\n",
      "Epoch [82/90], lter [831/1752] Loss: 1.7764\n",
      "Epoch [82/90], lter [841/1752] Loss: 2.0779\n",
      "Epoch [82/90], lter [851/1752] Loss: 1.6633\n",
      "Epoch [82/90], lter [861/1752] Loss: 1.1705\n",
      "Epoch [82/90], lter [871/1752] Loss: 2.1766\n",
      "Epoch [82/90], lter [881/1752] Loss: 1.6571\n",
      "Epoch [82/90], lter [891/1752] Loss: 1.3791\n",
      "Epoch [82/90], lter [901/1752] Loss: 1.8794\n",
      "Epoch [82/90], lter [911/1752] Loss: 1.8202\n",
      "Epoch [82/90], lter [921/1752] Loss: 1.8803\n",
      "Epoch [82/90], lter [931/1752] Loss: 1.7019\n",
      "Epoch [82/90], lter [941/1752] Loss: 1.5801\n",
      "Epoch [82/90], lter [951/1752] Loss: 1.3272\n",
      "Epoch [82/90], lter [961/1752] Loss: 1.7260\n",
      "Epoch [82/90], lter [971/1752] Loss: 1.5970\n",
      "Epoch [82/90], lter [981/1752] Loss: 1.4294\n",
      "Epoch [82/90], lter [991/1752] Loss: 1.6422\n",
      "Epoch [82/90], lter [1001/1752] Loss: 1.3131\n",
      "Epoch [82/90], lter [1011/1752] Loss: 1.5947\n",
      "Epoch [82/90], lter [1021/1752] Loss: 1.4723\n",
      "Epoch [82/90], lter [1031/1752] Loss: 1.1863\n",
      "Epoch [82/90], lter [1041/1752] Loss: 1.7763\n",
      "Epoch [82/90], lter [1051/1752] Loss: 1.5850\n",
      "Epoch [82/90], lter [1061/1752] Loss: 1.5328\n",
      "Epoch [82/90], lter [1071/1752] Loss: 1.2845\n",
      "Epoch [82/90], lter [1081/1752] Loss: 1.6022\n",
      "Epoch [82/90], lter [1091/1752] Loss: 1.6211\n",
      "Epoch [82/90], lter [1101/1752] Loss: 2.0693\n",
      "Epoch [82/90], lter [1111/1752] Loss: 1.8323\n",
      "Epoch [82/90], lter [1121/1752] Loss: 1.8882\n",
      "Epoch [82/90], lter [1131/1752] Loss: 1.9494\n",
      "Epoch [82/90], lter [1141/1752] Loss: 1.5795\n",
      "Epoch [82/90], lter [1151/1752] Loss: 1.7526\n",
      "Epoch [82/90], lter [1161/1752] Loss: 1.8900\n",
      "Epoch [82/90], lter [1171/1752] Loss: 1.7269\n",
      "Epoch [82/90], lter [1181/1752] Loss: 1.4926\n",
      "Epoch [82/90], lter [1191/1752] Loss: 1.8021\n",
      "Epoch [82/90], lter [1201/1752] Loss: 1.5030\n",
      "Epoch [82/90], lter [1211/1752] Loss: 1.8126\n",
      "Epoch [82/90], lter [1221/1752] Loss: 1.7333\n",
      "Epoch [82/90], lter [1231/1752] Loss: 1.7330\n",
      "Epoch [82/90], lter [1241/1752] Loss: 1.4905\n",
      "Epoch [82/90], lter [1251/1752] Loss: 1.4007\n",
      "Epoch [82/90], lter [1261/1752] Loss: 1.4646\n",
      "Epoch [82/90], lter [1271/1752] Loss: 1.4982\n",
      "Epoch [82/90], lter [1281/1752] Loss: 1.4628\n",
      "Epoch [82/90], lter [1291/1752] Loss: 1.1498\n",
      "Epoch [82/90], lter [1301/1752] Loss: 1.8175\n",
      "Epoch [82/90], lter [1311/1752] Loss: 1.4735\n",
      "Epoch [82/90], lter [1321/1752] Loss: 1.8411\n",
      "Epoch [82/90], lter [1331/1752] Loss: 1.9846\n",
      "Epoch [82/90], lter [1341/1752] Loss: 1.6673\n",
      "Epoch [82/90], lter [1351/1752] Loss: 1.4087\n",
      "Epoch [82/90], lter [1361/1752] Loss: 1.6283\n",
      "Epoch [82/90], lter [1371/1752] Loss: 1.5270\n",
      "Epoch [82/90], lter [1381/1752] Loss: 1.4375\n",
      "Epoch [82/90], lter [1391/1752] Loss: 1.8382\n",
      "Epoch [82/90], lter [1401/1752] Loss: 1.6237\n",
      "Epoch [82/90], lter [1411/1752] Loss: 2.0152\n",
      "Epoch [82/90], lter [1421/1752] Loss: 1.6366\n",
      "Epoch [82/90], lter [1431/1752] Loss: 1.5784\n",
      "Epoch [82/90], lter [1441/1752] Loss: 1.5840\n",
      "Epoch [82/90], lter [1451/1752] Loss: 1.8393\n",
      "Epoch [82/90], lter [1461/1752] Loss: 1.8604\n",
      "Epoch [82/90], lter [1471/1752] Loss: 1.1816\n",
      "Epoch [82/90], lter [1481/1752] Loss: 1.4953\n",
      "Epoch [82/90], lter [1491/1752] Loss: 1.7413\n",
      "Epoch [82/90], lter [1501/1752] Loss: 1.5508\n",
      "Epoch [82/90], lter [1511/1752] Loss: 1.5600\n",
      "Epoch [82/90], lter [1521/1752] Loss: 1.6450\n",
      "Epoch [82/90], lter [1531/1752] Loss: 1.8529\n",
      "Epoch [82/90], lter [1541/1752] Loss: 1.4563\n",
      "Epoch [82/90], lter [1551/1752] Loss: 1.2921\n",
      "Epoch [82/90], lter [1561/1752] Loss: 1.6655\n",
      "Epoch [82/90], lter [1571/1752] Loss: 1.7812\n",
      "Epoch [82/90], lter [1581/1752] Loss: 1.4024\n",
      "Epoch [82/90], lter [1591/1752] Loss: 1.4792\n",
      "Epoch [82/90], lter [1601/1752] Loss: 1.7925\n",
      "Epoch [82/90], lter [1611/1752] Loss: 1.6466\n",
      "Epoch [82/90], lter [1621/1752] Loss: 1.5141\n",
      "Epoch [82/90], lter [1631/1752] Loss: 1.6934\n",
      "Epoch [82/90], lter [1641/1752] Loss: 1.5130\n",
      "Epoch [82/90], lter [1651/1752] Loss: 1.5928\n",
      "Epoch [82/90], lter [1661/1752] Loss: 1.6091\n",
      "Epoch [82/90], lter [1671/1752] Loss: 1.4591\n",
      "Epoch [82/90], lter [1681/1752] Loss: 1.6628\n",
      "Epoch [82/90], lter [1691/1752] Loss: 1.9156\n",
      "Epoch [82/90], lter [1701/1752] Loss: 1.5252\n",
      "Epoch [82/90], lter [1711/1752] Loss: 1.1817\n",
      "Epoch [82/90], lter [1721/1752] Loss: 2.3235\n",
      "Epoch [82/90], lter [1731/1752] Loss: 1.8366\n",
      "Epoch [82/90], lter [1741/1752] Loss: 1.8021\n",
      "Epoch [82/90], lter [1751/1752] Loss: 1.5287\n",
      "Epoch:  82 | train loss : 1.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 82/90 [87:15:04<3:01:16, 1359.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  81 | test loss : 1.5645\n",
      "Epoch [83/90], lter [1/1752] Loss: 1.5462\n",
      "Epoch [83/90], lter [11/1752] Loss: 1.4039\n",
      "Epoch [83/90], lter [21/1752] Loss: 1.5558\n",
      "Epoch [83/90], lter [31/1752] Loss: 1.7181\n",
      "Epoch [83/90], lter [41/1752] Loss: 1.3748\n",
      "Epoch [83/90], lter [51/1752] Loss: 1.9833\n",
      "Epoch [83/90], lter [61/1752] Loss: 1.9844\n",
      "Epoch [83/90], lter [71/1752] Loss: 1.4439\n",
      "Epoch [83/90], lter [81/1752] Loss: 1.8510\n",
      "Epoch [83/90], lter [91/1752] Loss: 1.7707\n",
      "Epoch [83/90], lter [101/1752] Loss: 1.4952\n",
      "Epoch [83/90], lter [111/1752] Loss: 1.2808\n",
      "Epoch [83/90], lter [121/1752] Loss: 1.6041\n",
      "Epoch [83/90], lter [131/1752] Loss: 1.6028\n",
      "Epoch [83/90], lter [141/1752] Loss: 1.5859\n",
      "Epoch [83/90], lter [151/1752] Loss: 1.4697\n",
      "Epoch [83/90], lter [161/1752] Loss: 1.6777\n",
      "Epoch [83/90], lter [171/1752] Loss: 1.9735\n",
      "Epoch [83/90], lter [181/1752] Loss: 1.8678\n",
      "Epoch [83/90], lter [191/1752] Loss: 1.3848\n",
      "Epoch [83/90], lter [201/1752] Loss: 1.2260\n",
      "Epoch [83/90], lter [211/1752] Loss: 1.4607\n",
      "Epoch [83/90], lter [221/1752] Loss: 2.0298\n",
      "Epoch [83/90], lter [231/1752] Loss: 1.7658\n",
      "Epoch [83/90], lter [241/1752] Loss: 1.5160\n",
      "Epoch [83/90], lter [251/1752] Loss: 1.8719\n",
      "Epoch [83/90], lter [261/1752] Loss: 1.6310\n",
      "Epoch [83/90], lter [271/1752] Loss: 1.5910\n",
      "Epoch [83/90], lter [281/1752] Loss: 1.4824\n",
      "Epoch [83/90], lter [291/1752] Loss: 1.4825\n",
      "Epoch [83/90], lter [301/1752] Loss: 1.5498\n",
      "Epoch [83/90], lter [311/1752] Loss: 1.6126\n",
      "Epoch [83/90], lter [321/1752] Loss: 1.6457\n",
      "Epoch [83/90], lter [331/1752] Loss: 1.4765\n",
      "Epoch [83/90], lter [341/1752] Loss: 1.8275\n",
      "Epoch [83/90], lter [351/1752] Loss: 1.4242\n",
      "Epoch [83/90], lter [361/1752] Loss: 2.2561\n",
      "Epoch [83/90], lter [371/1752] Loss: 2.0710\n",
      "Epoch [83/90], lter [381/1752] Loss: 1.7375\n",
      "Epoch [83/90], lter [391/1752] Loss: 1.3657\n",
      "Epoch [83/90], lter [401/1752] Loss: 1.5039\n",
      "Epoch [83/90], lter [411/1752] Loss: 1.6901\n",
      "Epoch [83/90], lter [421/1752] Loss: 1.6403\n",
      "Epoch [83/90], lter [431/1752] Loss: 1.8542\n",
      "Epoch [83/90], lter [441/1752] Loss: 1.7435\n",
      "Epoch [83/90], lter [451/1752] Loss: 2.0215\n",
      "Epoch [83/90], lter [461/1752] Loss: 1.5160\n",
      "Epoch [83/90], lter [471/1752] Loss: 1.5339\n",
      "Epoch [83/90], lter [481/1752] Loss: 1.6135\n",
      "Epoch [83/90], lter [491/1752] Loss: 1.4838\n",
      "Epoch [83/90], lter [501/1752] Loss: 1.9732\n",
      "Epoch [83/90], lter [511/1752] Loss: 2.0600\n",
      "Epoch [83/90], lter [521/1752] Loss: 1.8746\n",
      "Epoch [83/90], lter [531/1752] Loss: 1.5340\n",
      "Epoch [83/90], lter [541/1752] Loss: 1.6880\n",
      "Epoch [83/90], lter [551/1752] Loss: 1.5532\n",
      "Epoch [83/90], lter [561/1752] Loss: 1.8913\n",
      "Epoch [83/90], lter [571/1752] Loss: 1.6199\n",
      "Epoch [83/90], lter [581/1752] Loss: 1.5613\n",
      "Epoch [83/90], lter [591/1752] Loss: 1.6928\n",
      "Epoch [83/90], lter [601/1752] Loss: 1.4279\n",
      "Epoch [83/90], lter [611/1752] Loss: 1.6854\n",
      "Epoch [83/90], lter [621/1752] Loss: 1.3546\n",
      "Epoch [83/90], lter [631/1752] Loss: 1.7335\n",
      "Epoch [83/90], lter [641/1752] Loss: 1.4381\n",
      "Epoch [83/90], lter [651/1752] Loss: 1.6892\n",
      "Epoch [83/90], lter [661/1752] Loss: 1.5010\n",
      "Epoch [83/90], lter [671/1752] Loss: 1.4674\n",
      "Epoch [83/90], lter [681/1752] Loss: 1.4900\n",
      "Epoch [83/90], lter [691/1752] Loss: 1.9056\n",
      "Epoch [83/90], lter [701/1752] Loss: 2.1506\n",
      "Epoch [83/90], lter [711/1752] Loss: 1.2536\n",
      "Epoch [83/90], lter [721/1752] Loss: 1.7170\n",
      "Epoch [83/90], lter [731/1752] Loss: 1.3511\n",
      "Epoch [83/90], lter [741/1752] Loss: 1.4705\n",
      "Epoch [83/90], lter [751/1752] Loss: 1.7211\n",
      "Epoch [83/90], lter [761/1752] Loss: 1.6159\n",
      "Epoch [83/90], lter [771/1752] Loss: 1.6494\n",
      "Epoch [83/90], lter [781/1752] Loss: 1.4676\n",
      "Epoch [83/90], lter [791/1752] Loss: 1.8998\n",
      "Epoch [83/90], lter [801/1752] Loss: 2.0795\n",
      "Epoch [83/90], lter [811/1752] Loss: 1.7851\n",
      "Epoch [83/90], lter [821/1752] Loss: 1.4194\n",
      "Epoch [83/90], lter [831/1752] Loss: 1.8265\n",
      "Epoch [83/90], lter [841/1752] Loss: 1.8542\n",
      "Epoch [83/90], lter [851/1752] Loss: 1.7006\n",
      "Epoch [83/90], lter [861/1752] Loss: 2.2238\n",
      "Epoch [83/90], lter [871/1752] Loss: 1.6382\n",
      "Epoch [83/90], lter [881/1752] Loss: 1.6137\n",
      "Epoch [83/90], lter [891/1752] Loss: 1.4653\n",
      "Epoch [83/90], lter [901/1752] Loss: 1.5349\n",
      "Epoch [83/90], lter [911/1752] Loss: 1.6763\n",
      "Epoch [83/90], lter [921/1752] Loss: 1.6814\n",
      "Epoch [83/90], lter [931/1752] Loss: 1.7976\n",
      "Epoch [83/90], lter [941/1752] Loss: 1.8804\n",
      "Epoch [83/90], lter [951/1752] Loss: 1.6117\n",
      "Epoch [83/90], lter [961/1752] Loss: 1.9085\n",
      "Epoch [83/90], lter [971/1752] Loss: 1.7794\n",
      "Epoch [83/90], lter [981/1752] Loss: 1.7131\n",
      "Epoch [83/90], lter [991/1752] Loss: 1.5456\n",
      "Epoch [83/90], lter [1001/1752] Loss: 1.6279\n",
      "Epoch [83/90], lter [1011/1752] Loss: 1.4560\n",
      "Epoch [83/90], lter [1021/1752] Loss: 1.6140\n",
      "Epoch [83/90], lter [1031/1752] Loss: 1.5655\n",
      "Epoch [83/90], lter [1041/1752] Loss: 1.8975\n",
      "Epoch [83/90], lter [1051/1752] Loss: 1.7704\n",
      "Epoch [83/90], lter [1061/1752] Loss: 1.6793\n",
      "Epoch [83/90], lter [1071/1752] Loss: 1.3752\n",
      "Epoch [83/90], lter [1081/1752] Loss: 1.5393\n",
      "Epoch [83/90], lter [1091/1752] Loss: 1.5473\n",
      "Epoch [83/90], lter [1101/1752] Loss: 1.5176\n",
      "Epoch [83/90], lter [1111/1752] Loss: 1.6658\n",
      "Epoch [83/90], lter [1121/1752] Loss: 1.6106\n",
      "Epoch [83/90], lter [1131/1752] Loss: 2.0860\n",
      "Epoch [83/90], lter [1141/1752] Loss: 2.0412\n",
      "Epoch [83/90], lter [1151/1752] Loss: 1.5655\n",
      "Epoch [83/90], lter [1161/1752] Loss: 1.7493\n",
      "Epoch [83/90], lter [1171/1752] Loss: 1.4492\n",
      "Epoch [83/90], lter [1181/1752] Loss: 1.8000\n",
      "Epoch [83/90], lter [1191/1752] Loss: 1.6928\n",
      "Epoch [83/90], lter [1201/1752] Loss: 1.3366\n",
      "Epoch [83/90], lter [1211/1752] Loss: 1.6487\n",
      "Epoch [83/90], lter [1221/1752] Loss: 1.5321\n",
      "Epoch [83/90], lter [1231/1752] Loss: 1.8719\n",
      "Epoch [83/90], lter [1241/1752] Loss: 1.3752\n",
      "Epoch [83/90], lter [1251/1752] Loss: 1.5165\n",
      "Epoch [83/90], lter [1261/1752] Loss: 1.7472\n",
      "Epoch [83/90], lter [1271/1752] Loss: 1.5893\n",
      "Epoch [83/90], lter [1281/1752] Loss: 1.9739\n",
      "Epoch [83/90], lter [1291/1752] Loss: 1.5484\n",
      "Epoch [83/90], lter [1301/1752] Loss: 1.5970\n",
      "Epoch [83/90], lter [1311/1752] Loss: 2.0384\n",
      "Epoch [83/90], lter [1321/1752] Loss: 1.5724\n",
      "Epoch [83/90], lter [1331/1752] Loss: 1.7324\n",
      "Epoch [83/90], lter [1341/1752] Loss: 1.3509\n",
      "Epoch [83/90], lter [1351/1752] Loss: 1.7555\n",
      "Epoch [83/90], lter [1361/1752] Loss: 1.5303\n",
      "Epoch [83/90], lter [1371/1752] Loss: 1.9889\n",
      "Epoch [83/90], lter [1381/1752] Loss: 1.9570\n",
      "Epoch [83/90], lter [1391/1752] Loss: 1.4282\n",
      "Epoch [83/90], lter [1401/1752] Loss: 1.1956\n",
      "Epoch [83/90], lter [1411/1752] Loss: 1.5294\n",
      "Epoch [83/90], lter [1421/1752] Loss: 1.5945\n",
      "Epoch [83/90], lter [1431/1752] Loss: 1.7282\n",
      "Epoch [83/90], lter [1441/1752] Loss: 1.3091\n",
      "Epoch [83/90], lter [1451/1752] Loss: 1.9461\n",
      "Epoch [83/90], lter [1461/1752] Loss: 1.6245\n",
      "Epoch [83/90], lter [1471/1752] Loss: 1.6047\n",
      "Epoch [83/90], lter [1481/1752] Loss: 2.2655\n",
      "Epoch [83/90], lter [1491/1752] Loss: 1.5821\n",
      "Epoch [83/90], lter [1501/1752] Loss: 1.4225\n",
      "Epoch [83/90], lter [1511/1752] Loss: 1.5776\n",
      "Epoch [83/90], lter [1521/1752] Loss: 1.8225\n",
      "Epoch [83/90], lter [1531/1752] Loss: 1.5496\n",
      "Epoch [83/90], lter [1541/1752] Loss: 1.8232\n",
      "Epoch [83/90], lter [1551/1752] Loss: 1.7748\n",
      "Epoch [83/90], lter [1561/1752] Loss: 1.5727\n",
      "Epoch [83/90], lter [1571/1752] Loss: 1.6511\n",
      "Epoch [83/90], lter [1581/1752] Loss: 1.8783\n",
      "Epoch [83/90], lter [1591/1752] Loss: 1.5430\n",
      "Epoch [83/90], lter [1601/1752] Loss: 1.6185\n",
      "Epoch [83/90], lter [1611/1752] Loss: 1.4682\n",
      "Epoch [83/90], lter [1621/1752] Loss: 1.7225\n",
      "Epoch [83/90], lter [1631/1752] Loss: 1.6531\n",
      "Epoch [83/90], lter [1641/1752] Loss: 1.5678\n",
      "Epoch [83/90], lter [1651/1752] Loss: 1.3843\n",
      "Epoch [83/90], lter [1661/1752] Loss: 1.4816\n",
      "Epoch [83/90], lter [1671/1752] Loss: 1.6218\n",
      "Epoch [83/90], lter [1681/1752] Loss: 2.0511\n",
      "Epoch [83/90], lter [1691/1752] Loss: 1.5388\n",
      "Epoch [83/90], lter [1701/1752] Loss: 1.6100\n",
      "Epoch [83/90], lter [1711/1752] Loss: 1.7143\n",
      "Epoch [83/90], lter [1721/1752] Loss: 1.9441\n",
      "Epoch [83/90], lter [1731/1752] Loss: 1.6339\n",
      "Epoch [83/90], lter [1741/1752] Loss: 1.4658\n",
      "Epoch [83/90], lter [1751/1752] Loss: 1.8465\n",
      "Epoch:  83 | train loss : 1.6653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 83/90 [87:39:24<2:42:06, 1389.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  82 | test loss : 1.5862\n",
      "Epoch [84/90], lter [1/1752] Loss: 1.3729\n",
      "Epoch [84/90], lter [11/1752] Loss: 1.7176\n",
      "Epoch [84/90], lter [21/1752] Loss: 1.7152\n",
      "Epoch [84/90], lter [31/1752] Loss: 1.6018\n",
      "Epoch [84/90], lter [41/1752] Loss: 1.6573\n",
      "Epoch [84/90], lter [51/1752] Loss: 1.4665\n",
      "Epoch [84/90], lter [61/1752] Loss: 1.6287\n",
      "Epoch [84/90], lter [71/1752] Loss: 1.9579\n",
      "Epoch [84/90], lter [81/1752] Loss: 1.3227\n",
      "Epoch [84/90], lter [91/1752] Loss: 1.9680\n",
      "Epoch [84/90], lter [101/1752] Loss: 1.8674\n",
      "Epoch [84/90], lter [111/1752] Loss: 1.9383\n",
      "Epoch [84/90], lter [121/1752] Loss: 1.7858\n",
      "Epoch [84/90], lter [131/1752] Loss: 1.5164\n",
      "Epoch [84/90], lter [141/1752] Loss: 1.3726\n",
      "Epoch [84/90], lter [151/1752] Loss: 1.8061\n",
      "Epoch [84/90], lter [161/1752] Loss: 1.3947\n",
      "Epoch [84/90], lter [171/1752] Loss: 1.8270\n",
      "Epoch [84/90], lter [181/1752] Loss: 1.8446\n",
      "Epoch [84/90], lter [191/1752] Loss: 1.5826\n",
      "Epoch [84/90], lter [201/1752] Loss: 1.5761\n",
      "Epoch [84/90], lter [211/1752] Loss: 1.7056\n",
      "Epoch [84/90], lter [221/1752] Loss: 1.6159\n",
      "Epoch [84/90], lter [231/1752] Loss: 1.6307\n",
      "Epoch [84/90], lter [241/1752] Loss: 1.4117\n",
      "Epoch [84/90], lter [251/1752] Loss: 1.6912\n",
      "Epoch [84/90], lter [261/1752] Loss: 1.4930\n",
      "Epoch [84/90], lter [271/1752] Loss: 1.9008\n",
      "Epoch [84/90], lter [281/1752] Loss: 1.6077\n",
      "Epoch [84/90], lter [291/1752] Loss: 1.3871\n",
      "Epoch [84/90], lter [301/1752] Loss: 1.3988\n",
      "Epoch [84/90], lter [311/1752] Loss: 1.6286\n",
      "Epoch [84/90], lter [321/1752] Loss: 1.2512\n",
      "Epoch [84/90], lter [331/1752] Loss: 1.8317\n",
      "Epoch [84/90], lter [341/1752] Loss: 1.5054\n",
      "Epoch [84/90], lter [351/1752] Loss: 1.5934\n",
      "Epoch [84/90], lter [361/1752] Loss: 1.5179\n",
      "Epoch [84/90], lter [371/1752] Loss: 1.6597\n",
      "Epoch [84/90], lter [381/1752] Loss: 1.1536\n",
      "Epoch [84/90], lter [391/1752] Loss: 1.4992\n",
      "Epoch [84/90], lter [401/1752] Loss: 1.8360\n",
      "Epoch [84/90], lter [411/1752] Loss: 1.6522\n",
      "Epoch [84/90], lter [421/1752] Loss: 1.9133\n",
      "Epoch [84/90], lter [431/1752] Loss: 1.6291\n",
      "Epoch [84/90], lter [441/1752] Loss: 1.6179\n",
      "Epoch [84/90], lter [451/1752] Loss: 1.5997\n",
      "Epoch [84/90], lter [461/1752] Loss: 1.5683\n",
      "Epoch [84/90], lter [471/1752] Loss: 1.7450\n",
      "Epoch [84/90], lter [481/1752] Loss: 1.4892\n",
      "Epoch [84/90], lter [491/1752] Loss: 1.6293\n",
      "Epoch [84/90], lter [501/1752] Loss: 2.1372\n",
      "Epoch [84/90], lter [511/1752] Loss: 1.5210\n",
      "Epoch [84/90], lter [521/1752] Loss: 1.7191\n",
      "Epoch [84/90], lter [531/1752] Loss: 1.6396\n",
      "Epoch [84/90], lter [541/1752] Loss: 1.7043\n",
      "Epoch [84/90], lter [551/1752] Loss: 2.2633\n",
      "Epoch [84/90], lter [561/1752] Loss: 1.7850\n",
      "Epoch [84/90], lter [571/1752] Loss: 1.7283\n",
      "Epoch [84/90], lter [581/1752] Loss: 1.7050\n",
      "Epoch [84/90], lter [591/1752] Loss: 1.4033\n",
      "Epoch [84/90], lter [601/1752] Loss: 1.8129\n",
      "Epoch [84/90], lter [611/1752] Loss: 1.5114\n",
      "Epoch [84/90], lter [621/1752] Loss: 1.5285\n",
      "Epoch [84/90], lter [631/1752] Loss: 1.3184\n",
      "Epoch [84/90], lter [641/1752] Loss: 1.5331\n",
      "Epoch [84/90], lter [651/1752] Loss: 1.8341\n",
      "Epoch [84/90], lter [661/1752] Loss: 1.6917\n",
      "Epoch [84/90], lter [671/1752] Loss: 2.0848\n",
      "Epoch [84/90], lter [681/1752] Loss: 1.7313\n",
      "Epoch [84/90], lter [691/1752] Loss: 1.7584\n",
      "Epoch [84/90], lter [701/1752] Loss: 1.3772\n",
      "Epoch [84/90], lter [711/1752] Loss: 1.4891\n",
      "Epoch [84/90], lter [721/1752] Loss: 1.7035\n",
      "Epoch [84/90], lter [731/1752] Loss: 1.6099\n",
      "Epoch [84/90], lter [741/1752] Loss: 1.9562\n",
      "Epoch [84/90], lter [751/1752] Loss: 1.6169\n",
      "Epoch [84/90], lter [761/1752] Loss: 1.5203\n",
      "Epoch [84/90], lter [771/1752] Loss: 1.8664\n",
      "Epoch [84/90], lter [781/1752] Loss: 1.2735\n",
      "Epoch [84/90], lter [791/1752] Loss: 1.8169\n",
      "Epoch [84/90], lter [801/1752] Loss: 1.5981\n",
      "Epoch [84/90], lter [811/1752] Loss: 1.6123\n",
      "Epoch [84/90], lter [821/1752] Loss: 1.6991\n",
      "Epoch [84/90], lter [831/1752] Loss: 1.4619\n",
      "Epoch [84/90], lter [841/1752] Loss: 1.3264\n",
      "Epoch [84/90], lter [851/1752] Loss: 1.7203\n",
      "Epoch [84/90], lter [861/1752] Loss: 1.5850\n",
      "Epoch [84/90], lter [871/1752] Loss: 1.7926\n",
      "Epoch [84/90], lter [881/1752] Loss: 1.6335\n",
      "Epoch [84/90], lter [891/1752] Loss: 1.7352\n",
      "Epoch [84/90], lter [901/1752] Loss: 1.6670\n",
      "Epoch [84/90], lter [911/1752] Loss: 1.3664\n",
      "Epoch [84/90], lter [921/1752] Loss: 1.5927\n",
      "Epoch [84/90], lter [931/1752] Loss: 1.4980\n",
      "Epoch [84/90], lter [941/1752] Loss: 1.4338\n",
      "Epoch [84/90], lter [951/1752] Loss: 1.5670\n",
      "Epoch [84/90], lter [961/1752] Loss: 1.8763\n",
      "Epoch [84/90], lter [971/1752] Loss: 1.6378\n",
      "Epoch [84/90], lter [981/1752] Loss: 1.5324\n",
      "Epoch [84/90], lter [991/1752] Loss: 1.3188\n",
      "Epoch [84/90], lter [1001/1752] Loss: 1.4779\n",
      "Epoch [84/90], lter [1011/1752] Loss: 1.9361\n",
      "Epoch [84/90], lter [1021/1752] Loss: 1.6847\n",
      "Epoch [84/90], lter [1031/1752] Loss: 1.4678\n",
      "Epoch [84/90], lter [1041/1752] Loss: 1.4933\n",
      "Epoch [84/90], lter [1051/1752] Loss: 1.9046\n",
      "Epoch [84/90], lter [1061/1752] Loss: 1.3740\n",
      "Epoch [84/90], lter [1071/1752] Loss: 2.3952\n",
      "Epoch [84/90], lter [1081/1752] Loss: 2.0327\n",
      "Epoch [84/90], lter [1091/1752] Loss: 1.3913\n",
      "Epoch [84/90], lter [1101/1752] Loss: 1.5748\n",
      "Epoch [84/90], lter [1111/1752] Loss: 1.5394\n",
      "Epoch [84/90], lter [1121/1752] Loss: 1.7368\n",
      "Epoch [84/90], lter [1131/1752] Loss: 1.4445\n",
      "Epoch [84/90], lter [1141/1752] Loss: 1.5280\n",
      "Epoch [84/90], lter [1151/1752] Loss: 1.4818\n",
      "Epoch [84/90], lter [1161/1752] Loss: 1.6441\n",
      "Epoch [84/90], lter [1171/1752] Loss: 1.5653\n",
      "Epoch [84/90], lter [1181/1752] Loss: 1.5660\n",
      "Epoch [84/90], lter [1191/1752] Loss: 1.6199\n",
      "Epoch [84/90], lter [1201/1752] Loss: 1.5938\n",
      "Epoch [84/90], lter [1211/1752] Loss: 1.9801\n",
      "Epoch [84/90], lter [1221/1752] Loss: 1.5732\n",
      "Epoch [84/90], lter [1231/1752] Loss: 1.6928\n",
      "Epoch [84/90], lter [1241/1752] Loss: 1.8821\n",
      "Epoch [84/90], lter [1251/1752] Loss: 2.0520\n",
      "Epoch [84/90], lter [1261/1752] Loss: 1.8820\n",
      "Epoch [84/90], lter [1271/1752] Loss: 1.8749\n",
      "Epoch [84/90], lter [1281/1752] Loss: 1.5390\n",
      "Epoch [84/90], lter [1291/1752] Loss: 1.7198\n",
      "Epoch [84/90], lter [1301/1752] Loss: 1.4294\n",
      "Epoch [84/90], lter [1311/1752] Loss: 1.7633\n",
      "Epoch [84/90], lter [1321/1752] Loss: 1.5795\n",
      "Epoch [84/90], lter [1331/1752] Loss: 2.1364\n",
      "Epoch [84/90], lter [1341/1752] Loss: 1.6085\n",
      "Epoch [84/90], lter [1351/1752] Loss: 1.6396\n",
      "Epoch [84/90], lter [1361/1752] Loss: 1.5847\n",
      "Epoch [84/90], lter [1371/1752] Loss: 1.5861\n",
      "Epoch [84/90], lter [1381/1752] Loss: 1.0569\n",
      "Epoch [84/90], lter [1391/1752] Loss: 1.2240\n",
      "Epoch [84/90], lter [1401/1752] Loss: 1.7334\n",
      "Epoch [84/90], lter [1411/1752] Loss: 1.8028\n",
      "Epoch [84/90], lter [1421/1752] Loss: 1.3986\n",
      "Epoch [84/90], lter [1431/1752] Loss: 1.4851\n",
      "Epoch [84/90], lter [1441/1752] Loss: 1.5921\n",
      "Epoch [84/90], lter [1451/1752] Loss: 1.6507\n",
      "Epoch [84/90], lter [1461/1752] Loss: 1.4808\n",
      "Epoch [84/90], lter [1471/1752] Loss: 1.5099\n",
      "Epoch [84/90], lter [1481/1752] Loss: 1.3636\n",
      "Epoch [84/90], lter [1491/1752] Loss: 1.5411\n",
      "Epoch [84/90], lter [1501/1752] Loss: 1.6428\n",
      "Epoch [84/90], lter [1511/1752] Loss: 1.7045\n",
      "Epoch [84/90], lter [1521/1752] Loss: 1.6606\n",
      "Epoch [84/90], lter [1531/1752] Loss: 1.6265\n",
      "Epoch [84/90], lter [1541/1752] Loss: 1.6450\n",
      "Epoch [84/90], lter [1551/1752] Loss: 1.8778\n",
      "Epoch [84/90], lter [1561/1752] Loss: 1.6487\n",
      "Epoch [84/90], lter [1571/1752] Loss: 2.0223\n",
      "Epoch [84/90], lter [1581/1752] Loss: 1.5689\n",
      "Epoch [84/90], lter [1591/1752] Loss: 1.5237\n",
      "Epoch [84/90], lter [1601/1752] Loss: 1.3638\n",
      "Epoch [84/90], lter [1611/1752] Loss: 1.3762\n",
      "Epoch [84/90], lter [1621/1752] Loss: 1.6779\n",
      "Epoch [84/90], lter [1631/1752] Loss: 1.9963\n",
      "Epoch [84/90], lter [1641/1752] Loss: 1.8343\n",
      "Epoch [84/90], lter [1651/1752] Loss: 1.8760\n",
      "Epoch [84/90], lter [1661/1752] Loss: 1.8064\n",
      "Epoch [84/90], lter [1671/1752] Loss: 1.5867\n",
      "Epoch [84/90], lter [1681/1752] Loss: 1.2461\n",
      "Epoch [84/90], lter [1691/1752] Loss: 1.6718\n",
      "Epoch [84/90], lter [1701/1752] Loss: 1.2270\n",
      "Epoch [84/90], lter [1711/1752] Loss: 1.4353\n",
      "Epoch [84/90], lter [1721/1752] Loss: 1.8956\n",
      "Epoch [84/90], lter [1731/1752] Loss: 1.9200\n",
      "Epoch [84/90], lter [1741/1752] Loss: 1.5223\n",
      "Epoch [84/90], lter [1751/1752] Loss: 1.4228\n",
      "Epoch:  84 | train loss : 1.6620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 84/90 [88:02:17<2:18:27, 1384.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  83 | test loss : 1.5649\n",
      "Epoch [85/90], lter [1/1752] Loss: 1.6726\n",
      "Epoch [85/90], lter [11/1752] Loss: 1.7624\n",
      "Epoch [85/90], lter [21/1752] Loss: 2.0756\n",
      "Epoch [85/90], lter [31/1752] Loss: 1.7406\n",
      "Epoch [85/90], lter [41/1752] Loss: 1.5307\n",
      "Epoch [85/90], lter [51/1752] Loss: 1.6288\n",
      "Epoch [85/90], lter [61/1752] Loss: 1.6571\n",
      "Epoch [85/90], lter [71/1752] Loss: 1.4613\n",
      "Epoch [85/90], lter [81/1752] Loss: 1.5205\n",
      "Epoch [85/90], lter [91/1752] Loss: 1.6855\n",
      "Epoch [85/90], lter [101/1752] Loss: 1.4829\n",
      "Epoch [85/90], lter [111/1752] Loss: 1.4463\n",
      "Epoch [85/90], lter [121/1752] Loss: 1.7554\n",
      "Epoch [85/90], lter [131/1752] Loss: 1.3977\n",
      "Epoch [85/90], lter [141/1752] Loss: 1.6944\n",
      "Epoch [85/90], lter [151/1752] Loss: 1.4581\n",
      "Epoch [85/90], lter [161/1752] Loss: 1.4876\n",
      "Epoch [85/90], lter [171/1752] Loss: 1.5082\n",
      "Epoch [85/90], lter [181/1752] Loss: 1.8897\n",
      "Epoch [85/90], lter [191/1752] Loss: 1.3746\n",
      "Epoch [85/90], lter [201/1752] Loss: 1.6561\n",
      "Epoch [85/90], lter [211/1752] Loss: 1.6913\n",
      "Epoch [85/90], lter [221/1752] Loss: 2.2785\n",
      "Epoch [85/90], lter [231/1752] Loss: 1.9450\n",
      "Epoch [85/90], lter [241/1752] Loss: 1.2401\n",
      "Epoch [85/90], lter [251/1752] Loss: 1.4235\n",
      "Epoch [85/90], lter [261/1752] Loss: 1.7320\n",
      "Epoch [85/90], lter [271/1752] Loss: 1.3421\n",
      "Epoch [85/90], lter [281/1752] Loss: 1.2973\n",
      "Epoch [85/90], lter [291/1752] Loss: 1.6301\n",
      "Epoch [85/90], lter [301/1752] Loss: 1.8244\n",
      "Epoch [85/90], lter [311/1752] Loss: 1.4735\n",
      "Epoch [85/90], lter [321/1752] Loss: 1.5057\n",
      "Epoch [85/90], lter [331/1752] Loss: 1.7375\n",
      "Epoch [85/90], lter [341/1752] Loss: 1.6387\n",
      "Epoch [85/90], lter [351/1752] Loss: 1.5640\n",
      "Epoch [85/90], lter [361/1752] Loss: 1.5154\n",
      "Epoch [85/90], lter [371/1752] Loss: 1.6476\n",
      "Epoch [85/90], lter [381/1752] Loss: 1.4230\n",
      "Epoch [85/90], lter [391/1752] Loss: 1.7578\n",
      "Epoch [85/90], lter [401/1752] Loss: 1.4451\n",
      "Epoch [85/90], lter [411/1752] Loss: 1.8052\n",
      "Epoch [85/90], lter [421/1752] Loss: 1.5682\n",
      "Epoch [85/90], lter [431/1752] Loss: 1.3418\n",
      "Epoch [85/90], lter [441/1752] Loss: 1.5769\n",
      "Epoch [85/90], lter [451/1752] Loss: 1.6020\n",
      "Epoch [85/90], lter [461/1752] Loss: 1.5740\n",
      "Epoch [85/90], lter [471/1752] Loss: 1.7017\n",
      "Epoch [85/90], lter [481/1752] Loss: 1.6781\n",
      "Epoch [85/90], lter [491/1752] Loss: 1.7303\n",
      "Epoch [85/90], lter [501/1752] Loss: 1.9670\n",
      "Epoch [85/90], lter [511/1752] Loss: 1.5364\n",
      "Epoch [85/90], lter [521/1752] Loss: 2.0030\n",
      "Epoch [85/90], lter [531/1752] Loss: 1.4923\n",
      "Epoch [85/90], lter [541/1752] Loss: 1.5999\n",
      "Epoch [85/90], lter [551/1752] Loss: 1.7267\n",
      "Epoch [85/90], lter [561/1752] Loss: 1.4638\n",
      "Epoch [85/90], lter [571/1752] Loss: 2.3915\n",
      "Epoch [85/90], lter [581/1752] Loss: 1.4737\n",
      "Epoch [85/90], lter [591/1752] Loss: 1.6346\n",
      "Epoch [85/90], lter [601/1752] Loss: 1.6858\n",
      "Epoch [85/90], lter [611/1752] Loss: 1.7196\n",
      "Epoch [85/90], lter [621/1752] Loss: 1.5471\n",
      "Epoch [85/90], lter [631/1752] Loss: 1.1876\n",
      "Epoch [85/90], lter [641/1752] Loss: 1.1934\n",
      "Epoch [85/90], lter [651/1752] Loss: 1.8997\n",
      "Epoch [85/90], lter [661/1752] Loss: 1.5035\n",
      "Epoch [85/90], lter [671/1752] Loss: 1.6854\n",
      "Epoch [85/90], lter [681/1752] Loss: 1.7443\n",
      "Epoch [85/90], lter [691/1752] Loss: 1.7793\n",
      "Epoch [85/90], lter [701/1752] Loss: 1.8281\n",
      "Epoch [85/90], lter [711/1752] Loss: 1.7384\n",
      "Epoch [85/90], lter [721/1752] Loss: 1.6763\n",
      "Epoch [85/90], lter [731/1752] Loss: 1.6115\n",
      "Epoch [85/90], lter [741/1752] Loss: 1.7362\n",
      "Epoch [85/90], lter [751/1752] Loss: 1.6945\n",
      "Epoch [85/90], lter [761/1752] Loss: 1.6544\n",
      "Epoch [85/90], lter [771/1752] Loss: 1.9864\n",
      "Epoch [85/90], lter [781/1752] Loss: 1.8105\n",
      "Epoch [85/90], lter [791/1752] Loss: 1.9672\n",
      "Epoch [85/90], lter [801/1752] Loss: 1.7208\n",
      "Epoch [85/90], lter [811/1752] Loss: 1.2561\n",
      "Epoch [85/90], lter [821/1752] Loss: 1.9823\n",
      "Epoch [85/90], lter [831/1752] Loss: 1.7585\n",
      "Epoch [85/90], lter [841/1752] Loss: 1.6311\n",
      "Epoch [85/90], lter [851/1752] Loss: 1.3567\n",
      "Epoch [85/90], lter [861/1752] Loss: 1.3471\n",
      "Epoch [85/90], lter [871/1752] Loss: 1.3612\n",
      "Epoch [85/90], lter [881/1752] Loss: 1.8405\n",
      "Epoch [85/90], lter [891/1752] Loss: 1.6846\n",
      "Epoch [85/90], lter [901/1752] Loss: 1.3861\n",
      "Epoch [85/90], lter [911/1752] Loss: 1.8232\n",
      "Epoch [85/90], lter [921/1752] Loss: 1.9437\n",
      "Epoch [85/90], lter [931/1752] Loss: 1.9310\n",
      "Epoch [85/90], lter [941/1752] Loss: 1.7623\n",
      "Epoch [85/90], lter [951/1752] Loss: 1.8144\n",
      "Epoch [85/90], lter [961/1752] Loss: 1.6568\n",
      "Epoch [85/90], lter [971/1752] Loss: 1.7129\n",
      "Epoch [85/90], lter [981/1752] Loss: 1.6148\n",
      "Epoch [85/90], lter [991/1752] Loss: 1.5415\n",
      "Epoch [85/90], lter [1001/1752] Loss: 1.5855\n",
      "Epoch [85/90], lter [1011/1752] Loss: 1.5401\n",
      "Epoch [85/90], lter [1021/1752] Loss: 1.9145\n",
      "Epoch [85/90], lter [1031/1752] Loss: 1.6582\n",
      "Epoch [85/90], lter [1041/1752] Loss: 1.7795\n",
      "Epoch [85/90], lter [1051/1752] Loss: 1.6514\n",
      "Epoch [85/90], lter [1061/1752] Loss: 1.9966\n",
      "Epoch [85/90], lter [1071/1752] Loss: 1.8428\n",
      "Epoch [85/90], lter [1081/1752] Loss: 1.9963\n",
      "Epoch [85/90], lter [1091/1752] Loss: 1.5815\n",
      "Epoch [85/90], lter [1101/1752] Loss: 1.6931\n",
      "Epoch [85/90], lter [1111/1752] Loss: 1.2321\n",
      "Epoch [85/90], lter [1121/1752] Loss: 1.3064\n",
      "Epoch [85/90], lter [1131/1752] Loss: 1.6049\n",
      "Epoch [85/90], lter [1141/1752] Loss: 1.6035\n",
      "Epoch [85/90], lter [1151/1752] Loss: 1.5115\n",
      "Epoch [85/90], lter [1161/1752] Loss: 1.5162\n",
      "Epoch [85/90], lter [1171/1752] Loss: 1.7150\n",
      "Epoch [85/90], lter [1181/1752] Loss: 1.5615\n",
      "Epoch [85/90], lter [1191/1752] Loss: 1.4422\n",
      "Epoch [85/90], lter [1201/1752] Loss: 1.7307\n",
      "Epoch [85/90], lter [1211/1752] Loss: 1.5211\n",
      "Epoch [85/90], lter [1221/1752] Loss: 1.8414\n",
      "Epoch [85/90], lter [1231/1752] Loss: 1.7680\n",
      "Epoch [85/90], lter [1241/1752] Loss: 1.7064\n",
      "Epoch [85/90], lter [1251/1752] Loss: 2.0062\n",
      "Epoch [85/90], lter [1261/1752] Loss: 1.8008\n",
      "Epoch [85/90], lter [1271/1752] Loss: 1.5860\n",
      "Epoch [85/90], lter [1281/1752] Loss: 1.2300\n",
      "Epoch [85/90], lter [1291/1752] Loss: 1.2845\n",
      "Epoch [85/90], lter [1301/1752] Loss: 2.0462\n",
      "Epoch [85/90], lter [1311/1752] Loss: 1.1908\n",
      "Epoch [85/90], lter [1321/1752] Loss: 1.8837\n",
      "Epoch [85/90], lter [1331/1752] Loss: 1.6452\n",
      "Epoch [85/90], lter [1341/1752] Loss: 1.7586\n",
      "Epoch [85/90], lter [1351/1752] Loss: 1.7754\n",
      "Epoch [85/90], lter [1361/1752] Loss: 1.8573\n",
      "Epoch [85/90], lter [1371/1752] Loss: 1.6370\n",
      "Epoch [85/90], lter [1381/1752] Loss: 1.9067\n",
      "Epoch [85/90], lter [1391/1752] Loss: 1.5394\n",
      "Epoch [85/90], lter [1401/1752] Loss: 1.8293\n",
      "Epoch [85/90], lter [1411/1752] Loss: 1.9486\n",
      "Epoch [85/90], lter [1421/1752] Loss: 1.6426\n",
      "Epoch [85/90], lter [1431/1752] Loss: 1.6249\n",
      "Epoch [85/90], lter [1441/1752] Loss: 1.4779\n",
      "Epoch [85/90], lter [1451/1752] Loss: 1.4790\n",
      "Epoch [85/90], lter [1461/1752] Loss: 1.3343\n",
      "Epoch [85/90], lter [1471/1752] Loss: 1.6778\n",
      "Epoch [85/90], lter [1481/1752] Loss: 1.5986\n",
      "Epoch [85/90], lter [1491/1752] Loss: 1.4982\n",
      "Epoch [85/90], lter [1501/1752] Loss: 2.1814\n",
      "Epoch [85/90], lter [1511/1752] Loss: 1.7187\n",
      "Epoch [85/90], lter [1521/1752] Loss: 1.9566\n",
      "Epoch [85/90], lter [1531/1752] Loss: 1.4650\n",
      "Epoch [85/90], lter [1541/1752] Loss: 1.4352\n",
      "Epoch [85/90], lter [1551/1752] Loss: 1.5618\n",
      "Epoch [85/90], lter [1561/1752] Loss: 1.8339\n",
      "Epoch [85/90], lter [1571/1752] Loss: 1.6078\n",
      "Epoch [85/90], lter [1581/1752] Loss: 1.8198\n",
      "Epoch [85/90], lter [1591/1752] Loss: 1.3386\n",
      "Epoch [85/90], lter [1601/1752] Loss: 1.2799\n",
      "Epoch [85/90], lter [1611/1752] Loss: 1.7846\n",
      "Epoch [85/90], lter [1621/1752] Loss: 1.4948\n",
      "Epoch [85/90], lter [1631/1752] Loss: 1.5824\n",
      "Epoch [85/90], lter [1641/1752] Loss: 1.7246\n",
      "Epoch [85/90], lter [1651/1752] Loss: 1.7906\n",
      "Epoch [85/90], lter [1661/1752] Loss: 1.9084\n",
      "Epoch [85/90], lter [1671/1752] Loss: 1.8008\n",
      "Epoch [85/90], lter [1681/1752] Loss: 1.7760\n",
      "Epoch [85/90], lter [1691/1752] Loss: 1.7242\n",
      "Epoch [85/90], lter [1701/1752] Loss: 1.6769\n",
      "Epoch [85/90], lter [1711/1752] Loss: 1.8478\n",
      "Epoch [85/90], lter [1721/1752] Loss: 1.9105\n",
      "Epoch [85/90], lter [1731/1752] Loss: 1.6688\n",
      "Epoch [85/90], lter [1741/1752] Loss: 1.4559\n",
      "Epoch [85/90], lter [1751/1752] Loss: 1.6329\n",
      "Epoch:  85 | train loss : 1.6590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 85/90 [88:25:26<1:55:30, 1386.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  84 | test loss : 1.5738\n",
      "Epoch [86/90], lter [1/1752] Loss: 1.6758\n",
      "Epoch [86/90], lter [11/1752] Loss: 1.7772\n",
      "Epoch [86/90], lter [21/1752] Loss: 1.5676\n",
      "Epoch [86/90], lter [31/1752] Loss: 1.6376\n",
      "Epoch [86/90], lter [41/1752] Loss: 1.4149\n",
      "Epoch [86/90], lter [51/1752] Loss: 1.6358\n",
      "Epoch [86/90], lter [61/1752] Loss: 1.5629\n",
      "Epoch [86/90], lter [71/1752] Loss: 1.6865\n",
      "Epoch [86/90], lter [81/1752] Loss: 1.2041\n",
      "Epoch [86/90], lter [91/1752] Loss: 1.6458\n",
      "Epoch [86/90], lter [101/1752] Loss: 1.9140\n",
      "Epoch [86/90], lter [111/1752] Loss: 1.5363\n",
      "Epoch [86/90], lter [121/1752] Loss: 1.8842\n",
      "Epoch [86/90], lter [131/1752] Loss: 1.8189\n",
      "Epoch [86/90], lter [141/1752] Loss: 1.2604\n",
      "Epoch [86/90], lter [151/1752] Loss: 1.8489\n",
      "Epoch [86/90], lter [161/1752] Loss: 1.6314\n",
      "Epoch [86/90], lter [171/1752] Loss: 1.6324\n",
      "Epoch [86/90], lter [181/1752] Loss: 1.8469\n",
      "Epoch [86/90], lter [191/1752] Loss: 1.5091\n",
      "Epoch [86/90], lter [201/1752] Loss: 1.4766\n",
      "Epoch [86/90], lter [211/1752] Loss: 1.4981\n",
      "Epoch [86/90], lter [221/1752] Loss: 1.6817\n",
      "Epoch [86/90], lter [231/1752] Loss: 1.5879\n",
      "Epoch [86/90], lter [241/1752] Loss: 1.8572\n",
      "Epoch [86/90], lter [251/1752] Loss: 1.2934\n",
      "Epoch [86/90], lter [261/1752] Loss: 1.4202\n",
      "Epoch [86/90], lter [271/1752] Loss: 1.8220\n",
      "Epoch [86/90], lter [281/1752] Loss: 1.4071\n",
      "Epoch [86/90], lter [291/1752] Loss: 1.6309\n",
      "Epoch [86/90], lter [301/1752] Loss: 1.8292\n",
      "Epoch [86/90], lter [311/1752] Loss: 1.8359\n",
      "Epoch [86/90], lter [321/1752] Loss: 1.8832\n",
      "Epoch [86/90], lter [331/1752] Loss: 2.0908\n",
      "Epoch [86/90], lter [341/1752] Loss: 1.8336\n",
      "Epoch [86/90], lter [351/1752] Loss: 1.7634\n",
      "Epoch [86/90], lter [361/1752] Loss: 1.3597\n",
      "Epoch [86/90], lter [371/1752] Loss: 1.5859\n",
      "Epoch [86/90], lter [381/1752] Loss: 1.5762\n",
      "Epoch [86/90], lter [391/1752] Loss: 1.7782\n",
      "Epoch [86/90], lter [401/1752] Loss: 1.9489\n",
      "Epoch [86/90], lter [411/1752] Loss: 1.6288\n",
      "Epoch [86/90], lter [421/1752] Loss: 1.7433\n",
      "Epoch [86/90], lter [431/1752] Loss: 1.9871\n",
      "Epoch [86/90], lter [441/1752] Loss: 1.6416\n",
      "Epoch [86/90], lter [451/1752] Loss: 2.0759\n",
      "Epoch [86/90], lter [461/1752] Loss: 2.1760\n",
      "Epoch [86/90], lter [471/1752] Loss: 1.5340\n",
      "Epoch [86/90], lter [481/1752] Loss: 1.4516\n",
      "Epoch [86/90], lter [491/1752] Loss: 1.7921\n",
      "Epoch [86/90], lter [501/1752] Loss: 2.0940\n",
      "Epoch [86/90], lter [511/1752] Loss: 1.7138\n",
      "Epoch [86/90], lter [521/1752] Loss: 1.6027\n",
      "Epoch [86/90], lter [531/1752] Loss: 2.0725\n",
      "Epoch [86/90], lter [541/1752] Loss: 2.1992\n",
      "Epoch [86/90], lter [551/1752] Loss: 1.3120\n",
      "Epoch [86/90], lter [561/1752] Loss: 1.2328\n",
      "Epoch [86/90], lter [571/1752] Loss: 1.6763\n",
      "Epoch [86/90], lter [581/1752] Loss: 2.0712\n",
      "Epoch [86/90], lter [591/1752] Loss: 1.5133\n",
      "Epoch [86/90], lter [601/1752] Loss: 1.6029\n",
      "Epoch [86/90], lter [611/1752] Loss: 1.6010\n",
      "Epoch [86/90], lter [621/1752] Loss: 1.6702\n",
      "Epoch [86/90], lter [631/1752] Loss: 1.9772\n",
      "Epoch [86/90], lter [641/1752] Loss: 1.4170\n",
      "Epoch [86/90], lter [651/1752] Loss: 1.7045\n",
      "Epoch [86/90], lter [661/1752] Loss: 1.5075\n",
      "Epoch [86/90], lter [671/1752] Loss: 1.7589\n",
      "Epoch [86/90], lter [681/1752] Loss: 1.7740\n",
      "Epoch [86/90], lter [691/1752] Loss: 1.9292\n",
      "Epoch [86/90], lter [701/1752] Loss: 1.8241\n",
      "Epoch [86/90], lter [711/1752] Loss: 1.5916\n",
      "Epoch [86/90], lter [721/1752] Loss: 1.6620\n",
      "Epoch [86/90], lter [731/1752] Loss: 1.9058\n",
      "Epoch [86/90], lter [741/1752] Loss: 1.5818\n",
      "Epoch [86/90], lter [751/1752] Loss: 1.7208\n",
      "Epoch [86/90], lter [761/1752] Loss: 1.9839\n",
      "Epoch [86/90], lter [771/1752] Loss: 1.6588\n",
      "Epoch [86/90], lter [781/1752] Loss: 1.6347\n",
      "Epoch [86/90], lter [791/1752] Loss: 1.8152\n",
      "Epoch [86/90], lter [801/1752] Loss: 1.7549\n",
      "Epoch [86/90], lter [811/1752] Loss: 1.3865\n",
      "Epoch [86/90], lter [821/1752] Loss: 1.5603\n",
      "Epoch [86/90], lter [831/1752] Loss: 1.8399\n",
      "Epoch [86/90], lter [841/1752] Loss: 1.8416\n",
      "Epoch [86/90], lter [851/1752] Loss: 1.4933\n",
      "Epoch [86/90], lter [861/1752] Loss: 1.2385\n",
      "Epoch [86/90], lter [871/1752] Loss: 1.4838\n",
      "Epoch [86/90], lter [881/1752] Loss: 1.6102\n",
      "Epoch [86/90], lter [891/1752] Loss: 1.5517\n",
      "Epoch [86/90], lter [901/1752] Loss: 1.8142\n",
      "Epoch [86/90], lter [911/1752] Loss: 1.6761\n",
      "Epoch [86/90], lter [921/1752] Loss: 1.6366\n",
      "Epoch [86/90], lter [931/1752] Loss: 1.6544\n",
      "Epoch [86/90], lter [941/1752] Loss: 1.5922\n",
      "Epoch [86/90], lter [951/1752] Loss: 1.6152\n",
      "Epoch [86/90], lter [961/1752] Loss: 1.7553\n",
      "Epoch [86/90], lter [971/1752] Loss: 1.6715\n",
      "Epoch [86/90], lter [981/1752] Loss: 1.8327\n",
      "Epoch [86/90], lter [991/1752] Loss: 1.8678\n",
      "Epoch [86/90], lter [1001/1752] Loss: 1.9402\n",
      "Epoch [86/90], lter [1011/1752] Loss: 1.4720\n",
      "Epoch [86/90], lter [1021/1752] Loss: 1.4868\n",
      "Epoch [86/90], lter [1031/1752] Loss: 1.7623\n",
      "Epoch [86/90], lter [1041/1752] Loss: 1.7880\n",
      "Epoch [86/90], lter [1051/1752] Loss: 1.5275\n",
      "Epoch [86/90], lter [1061/1752] Loss: 1.4385\n",
      "Epoch [86/90], lter [1071/1752] Loss: 1.6636\n",
      "Epoch [86/90], lter [1081/1752] Loss: 2.1635\n",
      "Epoch [86/90], lter [1091/1752] Loss: 1.6971\n",
      "Epoch [86/90], lter [1101/1752] Loss: 1.6009\n",
      "Epoch [86/90], lter [1111/1752] Loss: 1.3230\n",
      "Epoch [86/90], lter [1121/1752] Loss: 1.4759\n",
      "Epoch [86/90], lter [1131/1752] Loss: 2.2965\n",
      "Epoch [86/90], lter [1141/1752] Loss: 1.4465\n",
      "Epoch [86/90], lter [1151/1752] Loss: 1.7687\n",
      "Epoch [86/90], lter [1161/1752] Loss: 1.7670\n",
      "Epoch [86/90], lter [1171/1752] Loss: 1.6718\n",
      "Epoch [86/90], lter [1181/1752] Loss: 1.4696\n",
      "Epoch [86/90], lter [1191/1752] Loss: 1.6907\n",
      "Epoch [86/90], lter [1201/1752] Loss: 1.6851\n",
      "Epoch [86/90], lter [1211/1752] Loss: 1.3635\n",
      "Epoch [86/90], lter [1221/1752] Loss: 1.3468\n",
      "Epoch [86/90], lter [1231/1752] Loss: 1.6906\n",
      "Epoch [86/90], lter [1241/1752] Loss: 2.0326\n",
      "Epoch [86/90], lter [1251/1752] Loss: 1.9798\n",
      "Epoch [86/90], lter [1261/1752] Loss: 1.7785\n",
      "Epoch [86/90], lter [1271/1752] Loss: 1.3733\n",
      "Epoch [86/90], lter [1281/1752] Loss: 1.7442\n",
      "Epoch [86/90], lter [1291/1752] Loss: 1.4278\n",
      "Epoch [86/90], lter [1301/1752] Loss: 1.4375\n",
      "Epoch [86/90], lter [1311/1752] Loss: 1.5769\n",
      "Epoch [86/90], lter [1321/1752] Loss: 1.9450\n",
      "Epoch [86/90], lter [1331/1752] Loss: 1.7704\n",
      "Epoch [86/90], lter [1341/1752] Loss: 1.8314\n",
      "Epoch [86/90], lter [1351/1752] Loss: 1.8410\n",
      "Epoch [86/90], lter [1361/1752] Loss: 1.5118\n",
      "Epoch [86/90], lter [1371/1752] Loss: 1.5634\n",
      "Epoch [86/90], lter [1381/1752] Loss: 1.7240\n",
      "Epoch [86/90], lter [1391/1752] Loss: 1.6583\n",
      "Epoch [86/90], lter [1401/1752] Loss: 1.7286\n",
      "Epoch [86/90], lter [1411/1752] Loss: 1.7572\n",
      "Epoch [86/90], lter [1421/1752] Loss: 1.6557\n",
      "Epoch [86/90], lter [1431/1752] Loss: 1.9055\n",
      "Epoch [86/90], lter [1441/1752] Loss: 1.7080\n",
      "Epoch [86/90], lter [1451/1752] Loss: 1.5957\n",
      "Epoch [86/90], lter [1461/1752] Loss: 1.3899\n",
      "Epoch [86/90], lter [1471/1752] Loss: 1.8525\n",
      "Epoch [86/90], lter [1481/1752] Loss: 1.3761\n",
      "Epoch [86/90], lter [1491/1752] Loss: 1.6646\n",
      "Epoch [86/90], lter [1621/1752] Loss: 1.7875\n",
      "Epoch [86/90], lter [1631/1752] Loss: 1.7643\n",
      "Epoch [86/90], lter [1641/1752] Loss: 1.8310\n",
      "Epoch [86/90], lter [1651/1752] Loss: 1.3530\n",
      "Epoch [86/90], lter [1661/1752] Loss: 1.6103\n",
      "Epoch [86/90], lter [1671/1752] Loss: 1.4722\n",
      "Epoch [86/90], lter [1681/1752] Loss: 1.8424\n",
      "Epoch [86/90], lter [1691/1752] Loss: 1.7291\n",
      "Epoch [86/90], lter [1701/1752] Loss: 1.2588\n",
      "Epoch [86/90], lter [1711/1752] Loss: 1.3952\n",
      "Epoch [86/90], lter [1721/1752] Loss: 1.5572\n",
      "Epoch [86/90], lter [1731/1752] Loss: 1.9879\n",
      "Epoch [86/90], lter [1741/1752] Loss: 1.7010\n",
      "Epoch [86/90], lter [1751/1752] Loss: 1.3818\n",
      "Epoch:  86 | train loss : 1.6617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 86/90 [88:48:20<1:32:09, 1382.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  85 | test loss : 1.5691\n",
      "Epoch [87/90], lter [1/1752] Loss: 2.0283\n",
      "Epoch [87/90], lter [11/1752] Loss: 1.6596\n",
      "Epoch [87/90], lter [21/1752] Loss: 1.8526\n",
      "Epoch [87/90], lter [31/1752] Loss: 1.7150\n",
      "Epoch [87/90], lter [41/1752] Loss: 1.8624\n",
      "Epoch [87/90], lter [51/1752] Loss: 1.4084\n",
      "Epoch [87/90], lter [61/1752] Loss: 1.3574\n",
      "Epoch [87/90], lter [71/1752] Loss: 1.8966\n",
      "Epoch [87/90], lter [81/1752] Loss: 1.5533\n",
      "Epoch [87/90], lter [91/1752] Loss: 1.8515\n",
      "Epoch [87/90], lter [101/1752] Loss: 1.8250\n",
      "Epoch [87/90], lter [111/1752] Loss: 1.5335\n",
      "Epoch [87/90], lter [121/1752] Loss: 1.6300\n",
      "Epoch [87/90], lter [131/1752] Loss: 1.5543\n",
      "Epoch [87/90], lter [141/1752] Loss: 1.2884\n",
      "Epoch [87/90], lter [151/1752] Loss: 1.9213\n",
      "Epoch [87/90], lter [161/1752] Loss: 1.8737\n",
      "Epoch [87/90], lter [171/1752] Loss: 1.5937\n",
      "Epoch [87/90], lter [181/1752] Loss: 1.3359\n",
      "Epoch [87/90], lter [191/1752] Loss: 1.8109\n",
      "Epoch [87/90], lter [201/1752] Loss: 1.3014\n",
      "Epoch [87/90], lter [211/1752] Loss: 1.5732\n",
      "Epoch [87/90], lter [221/1752] Loss: 1.5677\n",
      "Epoch [87/90], lter [231/1752] Loss: 1.4195\n",
      "Epoch [87/90], lter [241/1752] Loss: 1.5835\n",
      "Epoch [87/90], lter [251/1752] Loss: 1.6635\n",
      "Epoch [87/90], lter [261/1752] Loss: 1.6726\n",
      "Epoch [87/90], lter [271/1752] Loss: 1.7486\n",
      "Epoch [87/90], lter [281/1752] Loss: 1.7513\n",
      "Epoch [87/90], lter [291/1752] Loss: 1.7019\n",
      "Epoch [87/90], lter [301/1752] Loss: 1.3327\n",
      "Epoch [87/90], lter [311/1752] Loss: 1.8745\n",
      "Epoch [87/90], lter [321/1752] Loss: 1.3726\n",
      "Epoch [87/90], lter [331/1752] Loss: 1.8810\n",
      "Epoch [87/90], lter [341/1752] Loss: 1.7616\n",
      "Epoch [87/90], lter [351/1752] Loss: 1.6851\n",
      "Epoch [87/90], lter [361/1752] Loss: 1.5784\n",
      "Epoch [87/90], lter [371/1752] Loss: 1.8077\n",
      "Epoch [87/90], lter [381/1752] Loss: 1.5606\n",
      "Epoch [87/90], lter [391/1752] Loss: 1.3282\n",
      "Epoch [87/90], lter [401/1752] Loss: 1.3769\n",
      "Epoch [87/90], lter [411/1752] Loss: 1.4827\n",
      "Epoch [87/90], lter [421/1752] Loss: 1.7007\n",
      "Epoch [87/90], lter [431/1752] Loss: 1.6908\n",
      "Epoch [87/90], lter [441/1752] Loss: 1.6489\n",
      "Epoch [87/90], lter [451/1752] Loss: 2.0135\n",
      "Epoch [87/90], lter [461/1752] Loss: 1.3696\n",
      "Epoch [87/90], lter [471/1752] Loss: 1.8059\n",
      "Epoch [87/90], lter [481/1752] Loss: 1.8319\n",
      "Epoch [87/90], lter [491/1752] Loss: 1.6014\n",
      "Epoch [87/90], lter [501/1752] Loss: 1.3582\n",
      "Epoch [87/90], lter [511/1752] Loss: 1.6637\n",
      "Epoch [87/90], lter [521/1752] Loss: 1.4346\n",
      "Epoch [87/90], lter [531/1752] Loss: 1.9782\n",
      "Epoch [87/90], lter [541/1752] Loss: 1.6981\n",
      "Epoch [87/90], lter [551/1752] Loss: 1.5686\n",
      "Epoch [87/90], lter [561/1752] Loss: 1.3513\n",
      "Epoch [87/90], lter [571/1752] Loss: 1.8324\n",
      "Epoch [87/90], lter [581/1752] Loss: 1.9108\n",
      "Epoch [87/90], lter [591/1752] Loss: 2.1346\n",
      "Epoch [87/90], lter [601/1752] Loss: 1.7359\n",
      "Epoch [87/90], lter [611/1752] Loss: 2.0444\n",
      "Epoch [87/90], lter [621/1752] Loss: 1.9924\n",
      "Epoch [87/90], lter [631/1752] Loss: 1.7133\n",
      "Epoch [87/90], lter [641/1752] Loss: 1.5743\n",
      "Epoch [87/90], lter [651/1752] Loss: 1.8787\n",
      "Epoch [87/90], lter [661/1752] Loss: 1.5731\n",
      "Epoch [87/90], lter [671/1752] Loss: 1.9112\n",
      "Epoch [87/90], lter [681/1752] Loss: 1.2785\n",
      "Epoch [87/90], lter [691/1752] Loss: 1.3519\n",
      "Epoch [87/90], lter [701/1752] Loss: 1.8337\n",
      "Epoch [87/90], lter [711/1752] Loss: 1.3997\n",
      "Epoch [87/90], lter [721/1752] Loss: 1.8726\n",
      "Epoch [87/90], lter [731/1752] Loss: 1.4944\n",
      "Epoch [87/90], lter [741/1752] Loss: 2.4091\n",
      "Epoch [87/90], lter [751/1752] Loss: 1.8448\n",
      "Epoch [87/90], lter [761/1752] Loss: 1.9067\n",
      "Epoch [87/90], lter [771/1752] Loss: 1.6711\n",
      "Epoch [87/90], lter [781/1752] Loss: 1.5770\n",
      "Epoch [87/90], lter [791/1752] Loss: 1.7807\n",
      "Epoch [87/90], lter [801/1752] Loss: 1.4870\n",
      "Epoch [87/90], lter [811/1752] Loss: 1.9716\n",
      "Epoch [87/90], lter [821/1752] Loss: 1.8153\n",
      "Epoch [87/90], lter [831/1752] Loss: 2.0116\n",
      "Epoch [87/90], lter [841/1752] Loss: 1.5081\n",
      "Epoch [87/90], lter [851/1752] Loss: 1.9969\n",
      "Epoch [87/90], lter [861/1752] Loss: 2.0578\n",
      "Epoch [87/90], lter [871/1752] Loss: 1.4174\n",
      "Epoch [87/90], lter [881/1752] Loss: 1.2607\n",
      "Epoch [87/90], lter [891/1752] Loss: 1.7389\n",
      "Epoch [87/90], lter [901/1752] Loss: 2.0661\n",
      "Epoch [87/90], lter [911/1752] Loss: 1.5712\n",
      "Epoch [87/90], lter [921/1752] Loss: 1.6705\n",
      "Epoch [87/90], lter [931/1752] Loss: 1.5214\n",
      "Epoch [87/90], lter [941/1752] Loss: 1.1883\n",
      "Epoch [87/90], lter [951/1752] Loss: 1.9187\n",
      "Epoch [87/90], lter [961/1752] Loss: 1.5404\n",
      "Epoch [87/90], lter [971/1752] Loss: 1.7499\n",
      "Epoch [87/90], lter [981/1752] Loss: 1.8533\n",
      "Epoch [87/90], lter [991/1752] Loss: 1.6707\n",
      "Epoch [87/90], lter [1001/1752] Loss: 1.7285\n",
      "Epoch [87/90], lter [1011/1752] Loss: 1.3466\n",
      "Epoch [87/90], lter [1021/1752] Loss: 2.0354\n",
      "Epoch [87/90], lter [1031/1752] Loss: 1.8078\n",
      "Epoch [87/90], lter [1041/1752] Loss: 1.4492\n",
      "Epoch [87/90], lter [1051/1752] Loss: 2.2923\n",
      "Epoch [87/90], lter [1061/1752] Loss: 1.7874\n",
      "Epoch [87/90], lter [1071/1752] Loss: 1.6324\n",
      "Epoch [87/90], lter [1081/1752] Loss: 1.8237\n",
      "Epoch [87/90], lter [1091/1752] Loss: 1.9056\n",
      "Epoch [87/90], lter [1101/1752] Loss: 1.5587\n",
      "Epoch [87/90], lter [1111/1752] Loss: 1.9405\n",
      "Epoch [87/90], lter [1121/1752] Loss: 1.6921\n",
      "Epoch [87/90], lter [1131/1752] Loss: 1.4278\n",
      "Epoch [87/90], lter [1141/1752] Loss: 1.3923\n",
      "Epoch [87/90], lter [1151/1752] Loss: 1.8517\n",
      "Epoch [87/90], lter [1161/1752] Loss: 1.8818\n",
      "Epoch [87/90], lter [1171/1752] Loss: 1.2416\n",
      "Epoch [87/90], lter [1181/1752] Loss: 1.5114\n",
      "Epoch [87/90], lter [1191/1752] Loss: 1.7701\n",
      "Epoch [87/90], lter [1201/1752] Loss: 1.5187\n",
      "Epoch [87/90], lter [1211/1752] Loss: 1.8406\n",
      "Epoch [87/90], lter [1221/1752] Loss: 1.6989\n",
      "Epoch [87/90], lter [1231/1752] Loss: 1.4845\n",
      "Epoch [87/90], lter [1241/1752] Loss: 1.8825\n",
      "Epoch [87/90], lter [1251/1752] Loss: 1.5454\n",
      "Epoch [87/90], lter [1261/1752] Loss: 1.6456\n",
      "Epoch [87/90], lter [1271/1752] Loss: 1.7762\n",
      "Epoch [87/90], lter [1281/1752] Loss: 1.3380\n",
      "Epoch [87/90], lter [1291/1752] Loss: 1.8603\n",
      "Epoch [87/90], lter [1301/1752] Loss: 1.4480\n",
      "Epoch [87/90], lter [1311/1752] Loss: 1.0939\n",
      "Epoch [87/90], lter [1321/1752] Loss: 1.5825\n",
      "Epoch [87/90], lter [1331/1752] Loss: 1.4225\n",
      "Epoch [87/90], lter [1341/1752] Loss: 1.4078\n",
      "Epoch [87/90], lter [1351/1752] Loss: 1.4456\n",
      "Epoch [87/90], lter [1361/1752] Loss: 1.5875\n",
      "Epoch [87/90], lter [1371/1752] Loss: 1.4699\n",
      "Epoch [87/90], lter [1381/1752] Loss: 1.3205\n",
      "Epoch [87/90], lter [1391/1752] Loss: 1.7765\n",
      "Epoch [87/90], lter [1401/1752] Loss: 1.8115\n",
      "Epoch [87/90], lter [1411/1752] Loss: 1.8713\n",
      "Epoch [87/90], lter [1421/1752] Loss: 1.5167\n",
      "Epoch [87/90], lter [1431/1752] Loss: 1.7620\n",
      "Epoch [87/90], lter [1441/1752] Loss: 1.4331\n",
      "Epoch [87/90], lter [1451/1752] Loss: 1.8396\n",
      "Epoch [87/90], lter [1461/1752] Loss: 1.5420\n",
      "Epoch [87/90], lter [1471/1752] Loss: 1.7094\n",
      "Epoch [87/90], lter [1481/1752] Loss: 1.7518\n",
      "Epoch [87/90], lter [1491/1752] Loss: 1.3729\n",
      "Epoch [87/90], lter [1501/1752] Loss: 2.1415\n",
      "Epoch [87/90], lter [1511/1752] Loss: 1.6242\n",
      "Epoch [87/90], lter [1521/1752] Loss: 1.5903\n",
      "Epoch [87/90], lter [1531/1752] Loss: 1.7677\n",
      "Epoch [87/90], lter [1541/1752] Loss: 1.7064\n",
      "Epoch [87/90], lter [1551/1752] Loss: 1.3611\n",
      "Epoch [87/90], lter [1561/1752] Loss: 1.6479\n",
      "Epoch [87/90], lter [1571/1752] Loss: 1.7376\n",
      "Epoch [87/90], lter [1581/1752] Loss: 1.6010\n",
      "Epoch [87/90], lter [1591/1752] Loss: 1.3974\n",
      "Epoch [87/90], lter [1601/1752] Loss: 1.8122\n",
      "Epoch [87/90], lter [1611/1752] Loss: 1.5955\n",
      "Epoch [87/90], lter [1621/1752] Loss: 1.5606\n",
      "Epoch [87/90], lter [1631/1752] Loss: 1.6318\n",
      "Epoch [87/90], lter [1641/1752] Loss: 1.6315\n",
      "Epoch [87/90], lter [1651/1752] Loss: 1.3603\n",
      "Epoch [87/90], lter [1661/1752] Loss: 1.9083\n",
      "Epoch [87/90], lter [1671/1752] Loss: 1.9832\n",
      "Epoch [87/90], lter [1681/1752] Loss: 1.7752\n",
      "Epoch [87/90], lter [1691/1752] Loss: 1.7086\n",
      "Epoch [87/90], lter [1701/1752] Loss: 1.4473\n",
      "Epoch [87/90], lter [1711/1752] Loss: 1.7496\n",
      "Epoch [87/90], lter [1721/1752] Loss: 1.8163\n",
      "Epoch [87/90], lter [1731/1752] Loss: 1.6654\n",
      "Epoch [87/90], lter [1741/1752] Loss: 1.3562\n",
      "Epoch [87/90], lter [1751/1752] Loss: 1.8000\n",
      "Epoch:  87 | train loss : 1.6627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 87/90 [89:11:22<1:09:06, 1382.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  86 | test loss : 1.5578\n",
      "Epoch [88/90], lter [1/1752] Loss: 1.9949\n",
      "Epoch [88/90], lter [11/1752] Loss: 1.6491\n",
      "Epoch [88/90], lter [21/1752] Loss: 1.6052\n",
      "Epoch [88/90], lter [31/1752] Loss: 1.7900\n",
      "Epoch [88/90], lter [41/1752] Loss: 1.7803\n",
      "Epoch [88/90], lter [51/1752] Loss: 1.4424\n",
      "Epoch [88/90], lter [61/1752] Loss: 2.1601\n",
      "Epoch [88/90], lter [71/1752] Loss: 1.4464\n",
      "Epoch [88/90], lter [81/1752] Loss: 1.8339\n",
      "Epoch [88/90], lter [91/1752] Loss: 2.0578\n",
      "Epoch [88/90], lter [101/1752] Loss: 1.9337\n",
      "Epoch [88/90], lter [111/1752] Loss: 2.0953\n",
      "Epoch [88/90], lter [121/1752] Loss: 1.9068\n",
      "Epoch [88/90], lter [131/1752] Loss: 1.7383\n",
      "Epoch [88/90], lter [141/1752] Loss: 1.6743\n",
      "Epoch [88/90], lter [151/1752] Loss: 1.5406\n",
      "Epoch [88/90], lter [161/1752] Loss: 2.2444\n",
      "Epoch [88/90], lter [171/1752] Loss: 1.5824\n",
      "Epoch [88/90], lter [181/1752] Loss: 1.7565\n",
      "Epoch [88/90], lter [191/1752] Loss: 1.4395\n",
      "Epoch [88/90], lter [201/1752] Loss: 1.9034\n",
      "Epoch [88/90], lter [211/1752] Loss: 1.4636\n",
      "Epoch [88/90], lter [221/1752] Loss: 1.4873\n",
      "Epoch [88/90], lter [231/1752] Loss: 1.5859\n",
      "Epoch [88/90], lter [241/1752] Loss: 1.9265\n",
      "Epoch [88/90], lter [251/1752] Loss: 1.7955\n",
      "Epoch [88/90], lter [261/1752] Loss: 1.7051\n",
      "Epoch [88/90], lter [271/1752] Loss: 1.4065\n",
      "Epoch [88/90], lter [281/1752] Loss: 1.4350\n",
      "Epoch [88/90], lter [291/1752] Loss: 1.6188\n",
      "Epoch [88/90], lter [301/1752] Loss: 1.6405\n",
      "Epoch [88/90], lter [311/1752] Loss: 2.2279\n",
      "Epoch [88/90], lter [321/1752] Loss: 1.8490\n",
      "Epoch [88/90], lter [331/1752] Loss: 1.5995\n",
      "Epoch [88/90], lter [341/1752] Loss: 1.4500\n",
      "Epoch [88/90], lter [351/1752] Loss: 1.7984\n",
      "Epoch [88/90], lter [361/1752] Loss: 1.4558\n",
      "Epoch [88/90], lter [371/1752] Loss: 1.6188\n",
      "Epoch [88/90], lter [381/1752] Loss: 1.6289\n",
      "Epoch [88/90], lter [391/1752] Loss: 2.2039\n",
      "Epoch [88/90], lter [401/1752] Loss: 1.4953\n",
      "Epoch [88/90], lter [411/1752] Loss: 1.9095\n",
      "Epoch [88/90], lter [421/1752] Loss: 1.7507\n",
      "Epoch [88/90], lter [431/1752] Loss: 2.0638\n",
      "Epoch [88/90], lter [441/1752] Loss: 1.7223\n",
      "Epoch [88/90], lter [451/1752] Loss: 1.8983\n",
      "Epoch [88/90], lter [461/1752] Loss: 1.6522\n",
      "Epoch [88/90], lter [471/1752] Loss: 1.5339\n",
      "Epoch [88/90], lter [481/1752] Loss: 1.2635\n",
      "Epoch [88/90], lter [491/1752] Loss: 1.9660\n",
      "Epoch [88/90], lter [501/1752] Loss: 1.4668\n",
      "Epoch [88/90], lter [511/1752] Loss: 1.5257\n",
      "Epoch [88/90], lter [521/1752] Loss: 1.8259\n",
      "Epoch [88/90], lter [531/1752] Loss: 1.8627\n",
      "Epoch [88/90], lter [541/1752] Loss: 1.4832\n",
      "Epoch [88/90], lter [551/1752] Loss: 1.7895\n",
      "Epoch [88/90], lter [561/1752] Loss: 1.8474\n",
      "Epoch [88/90], lter [571/1752] Loss: 1.6364\n",
      "Epoch [88/90], lter [581/1752] Loss: 1.7433\n",
      "Epoch [88/90], lter [591/1752] Loss: 1.6896\n",
      "Epoch [88/90], lter [601/1752] Loss: 2.1081\n",
      "Epoch [88/90], lter [611/1752] Loss: 1.5361\n",
      "Epoch [88/90], lter [621/1752] Loss: 1.6010\n",
      "Epoch [88/90], lter [631/1752] Loss: 1.3952\n",
      "Epoch [88/90], lter [641/1752] Loss: 2.0125\n",
      "Epoch [88/90], lter [651/1752] Loss: 1.7934\n",
      "Epoch [88/90], lter [661/1752] Loss: 1.5580\n",
      "Epoch [88/90], lter [671/1752] Loss: 1.8489\n",
      "Epoch [88/90], lter [681/1752] Loss: 1.8376\n",
      "Epoch [88/90], lter [691/1752] Loss: 1.6818\n",
      "Epoch [88/90], lter [701/1752] Loss: 1.5572\n",
      "Epoch [88/90], lter [711/1752] Loss: 1.7186\n",
      "Epoch [88/90], lter [721/1752] Loss: 1.8703\n",
      "Epoch [88/90], lter [731/1752] Loss: 1.7820\n",
      "Epoch [88/90], lter [741/1752] Loss: 1.2108\n",
      "Epoch [88/90], lter [751/1752] Loss: 1.3371\n",
      "Epoch [88/90], lter [761/1752] Loss: 1.6826\n",
      "Epoch [88/90], lter [771/1752] Loss: 1.5522\n",
      "Epoch [88/90], lter [781/1752] Loss: 1.4780\n",
      "Epoch [88/90], lter [791/1752] Loss: 1.7650\n",
      "Epoch [88/90], lter [801/1752] Loss: 1.6626\n",
      "Epoch [88/90], lter [811/1752] Loss: 1.4541\n",
      "Epoch [88/90], lter [821/1752] Loss: 1.8558\n",
      "Epoch [88/90], lter [831/1752] Loss: 2.0187\n",
      "Epoch [88/90], lter [841/1752] Loss: 1.5379\n",
      "Epoch [88/90], lter [851/1752] Loss: 1.6175\n",
      "Epoch [88/90], lter [861/1752] Loss: 1.3065\n",
      "Epoch [88/90], lter [871/1752] Loss: 1.3281\n",
      "Epoch [88/90], lter [881/1752] Loss: 1.7334\n",
      "Epoch [88/90], lter [891/1752] Loss: 1.7920\n",
      "Epoch [88/90], lter [901/1752] Loss: 1.2854\n",
      "Epoch [88/90], lter [911/1752] Loss: 1.7360\n",
      "Epoch [88/90], lter [921/1752] Loss: 1.7431\n",
      "Epoch [88/90], lter [931/1752] Loss: 1.5142\n",
      "Epoch [88/90], lter [941/1752] Loss: 1.5608\n",
      "Epoch [88/90], lter [951/1752] Loss: 1.6631\n",
      "Epoch [88/90], lter [961/1752] Loss: 1.8182\n",
      "Epoch [88/90], lter [971/1752] Loss: 1.4547\n",
      "Epoch [88/90], lter [981/1752] Loss: 1.7809\n",
      "Epoch [88/90], lter [991/1752] Loss: 1.6164\n",
      "Epoch [88/90], lter [1001/1752] Loss: 1.6088\n",
      "Epoch [88/90], lter [1011/1752] Loss: 1.6467\n",
      "Epoch [88/90], lter [1021/1752] Loss: 1.4917\n",
      "Epoch [88/90], lter [1031/1752] Loss: 1.8331\n",
      "Epoch [88/90], lter [1041/1752] Loss: 1.6712\n",
      "Epoch [88/90], lter [1051/1752] Loss: 1.5955\n",
      "Epoch [88/90], lter [1061/1752] Loss: 1.6421\n",
      "Epoch [88/90], lter [1071/1752] Loss: 1.6450\n",
      "Epoch [88/90], lter [1081/1752] Loss: 1.4611\n",
      "Epoch [88/90], lter [1091/1752] Loss: 1.8901\n",
      "Epoch [88/90], lter [1101/1752] Loss: 1.6875\n",
      "Epoch [88/90], lter [1111/1752] Loss: 1.7576\n",
      "Epoch [88/90], lter [1121/1752] Loss: 1.2431\n",
      "Epoch [88/90], lter [1131/1752] Loss: 1.6168\n",
      "Epoch [88/90], lter [1141/1752] Loss: 1.6085\n",
      "Epoch [88/90], lter [1151/1752] Loss: 1.9199\n",
      "Epoch [88/90], lter [1161/1752] Loss: 2.0360\n",
      "Epoch [88/90], lter [1171/1752] Loss: 1.7240\n",
      "Epoch [88/90], lter [1181/1752] Loss: 1.6887\n",
      "Epoch [88/90], lter [1191/1752] Loss: 1.6984\n",
      "Epoch [88/90], lter [1201/1752] Loss: 1.6697\n",
      "Epoch [88/90], lter [1211/1752] Loss: 1.9627\n",
      "Epoch [88/90], lter [1221/1752] Loss: 1.4604\n",
      "Epoch [88/90], lter [1231/1752] Loss: 1.6357\n",
      "Epoch [88/90], lter [1241/1752] Loss: 1.8154\n",
      "Epoch [88/90], lter [1251/1752] Loss: 1.5993\n",
      "Epoch [88/90], lter [1261/1752] Loss: 1.3359\n",
      "Epoch [88/90], lter [1271/1752] Loss: 1.6204\n",
      "Epoch [88/90], lter [1281/1752] Loss: 1.4237\n",
      "Epoch [88/90], lter [1291/1752] Loss: 1.7761\n",
      "Epoch [88/90], lter [1301/1752] Loss: 1.9166\n",
      "Epoch [88/90], lter [1311/1752] Loss: 1.7915\n",
      "Epoch [88/90], lter [1321/1752] Loss: 1.7823\n",
      "Epoch [88/90], lter [1331/1752] Loss: 1.9344\n",
      "Epoch [88/90], lter [1341/1752] Loss: 2.0474\n",
      "Epoch [88/90], lter [1351/1752] Loss: 2.1508\n",
      "Epoch [88/90], lter [1361/1752] Loss: 1.8529\n",
      "Epoch [88/90], lter [1371/1752] Loss: 2.0282\n",
      "Epoch [88/90], lter [1381/1752] Loss: 2.0983\n",
      "Epoch [88/90], lter [1391/1752] Loss: 1.2078\n",
      "Epoch [88/90], lter [1401/1752] Loss: 1.7361\n",
      "Epoch [88/90], lter [1411/1752] Loss: 1.3859\n",
      "Epoch [88/90], lter [1421/1752] Loss: 1.3588\n",
      "Epoch [88/90], lter [1431/1752] Loss: 1.7361\n",
      "Epoch [88/90], lter [1441/1752] Loss: 1.4924\n",
      "Epoch [88/90], lter [1451/1752] Loss: 1.3305\n",
      "Epoch [88/90], lter [1461/1752] Loss: 1.9796\n",
      "Epoch [88/90], lter [1471/1752] Loss: 1.4750\n",
      "Epoch [88/90], lter [1481/1752] Loss: 1.5912\n",
      "Epoch [88/90], lter [1491/1752] Loss: 2.0072\n",
      "Epoch [88/90], lter [1501/1752] Loss: 1.7650\n",
      "Epoch [88/90], lter [1511/1752] Loss: 1.6079\n",
      "Epoch [88/90], lter [1521/1752] Loss: 1.2726\n",
      "Epoch [88/90], lter [1531/1752] Loss: 1.5946\n",
      "Epoch [88/90], lter [1541/1752] Loss: 1.5933\n",
      "Epoch [88/90], lter [1551/1752] Loss: 1.6303\n",
      "Epoch [88/90], lter [1561/1752] Loss: 1.7104\n",
      "Epoch [88/90], lter [1571/1752] Loss: 1.5064\n",
      "Epoch [88/90], lter [1581/1752] Loss: 1.9259\n",
      "Epoch [88/90], lter [1591/1752] Loss: 1.9840\n",
      "Epoch [88/90], lter [1601/1752] Loss: 1.3177\n",
      "Epoch [88/90], lter [1611/1752] Loss: 1.8989\n",
      "Epoch [88/90], lter [1621/1752] Loss: 1.7094\n",
      "Epoch [88/90], lter [1631/1752] Loss: 2.0460\n",
      "Epoch [88/90], lter [1641/1752] Loss: 1.6910\n",
      "Epoch [88/90], lter [1651/1752] Loss: 1.7107\n",
      "Epoch [88/90], lter [1661/1752] Loss: 1.5773\n",
      "Epoch [88/90], lter [1671/1752] Loss: 1.3380\n",
      "Epoch [88/90], lter [1681/1752] Loss: 1.8209\n",
      "Epoch [88/90], lter [1691/1752] Loss: 1.6845\n",
      "Epoch [88/90], lter [1701/1752] Loss: 1.8320\n",
      "Epoch [88/90], lter [1711/1752] Loss: 1.5617\n",
      "Epoch [88/90], lter [1721/1752] Loss: 1.9505\n",
      "Epoch [88/90], lter [1731/1752] Loss: 1.8888\n",
      "Epoch [88/90], lter [1741/1752] Loss: 2.0466\n",
      "Epoch [88/90], lter [1751/1752] Loss: 1.2827\n",
      "Epoch:  88 | train loss : 1.6674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 88/90 [89:34:11<45:56, 1378.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  87 | test loss : 1.5497\n",
      "Epoch [89/90], lter [1/1752] Loss: 1.6617\n",
      "Epoch [89/90], lter [11/1752] Loss: 1.4659\n",
      "Epoch [89/90], lter [21/1752] Loss: 1.4132\n",
      "Epoch [89/90], lter [31/1752] Loss: 1.9695\n",
      "Epoch [89/90], lter [41/1752] Loss: 1.7819\n",
      "Epoch [89/90], lter [51/1752] Loss: 1.7256\n",
      "Epoch [89/90], lter [61/1752] Loss: 1.2862\n",
      "Epoch [89/90], lter [71/1752] Loss: 2.2512\n",
      "Epoch [89/90], lter [81/1752] Loss: 1.4509\n",
      "Epoch [89/90], lter [91/1752] Loss: 1.6939\n",
      "Epoch [89/90], lter [101/1752] Loss: 1.6550\n",
      "Epoch [89/90], lter [111/1752] Loss: 1.5661\n",
      "Epoch [89/90], lter [121/1752] Loss: 1.3326\n",
      "Epoch [89/90], lter [131/1752] Loss: 1.2978\n",
      "Epoch [89/90], lter [141/1752] Loss: 1.5035\n",
      "Epoch [89/90], lter [151/1752] Loss: 1.5776\n",
      "Epoch [89/90], lter [161/1752] Loss: 1.7118\n",
      "Epoch [89/90], lter [171/1752] Loss: 1.4298\n",
      "Epoch [89/90], lter [181/1752] Loss: 1.5735\n",
      "Epoch [89/90], lter [191/1752] Loss: 1.3604\n",
      "Epoch [89/90], lter [201/1752] Loss: 1.6622\n",
      "Epoch [89/90], lter [211/1752] Loss: 2.1149\n",
      "Epoch [89/90], lter [221/1752] Loss: 1.7223\n",
      "Epoch [89/90], lter [231/1752] Loss: 1.5471\n",
      "Epoch [89/90], lter [241/1752] Loss: 1.6280\n",
      "Epoch [89/90], lter [251/1752] Loss: 1.7703\n",
      "Epoch [89/90], lter [261/1752] Loss: 1.7352\n",
      "Epoch [89/90], lter [271/1752] Loss: 1.5365\n",
      "Epoch [89/90], lter [281/1752] Loss: 1.9014\n",
      "Epoch [89/90], lter [291/1752] Loss: 1.5067\n",
      "Epoch [89/90], lter [301/1752] Loss: 1.8317\n",
      "Epoch [89/90], lter [311/1752] Loss: 1.9381\n",
      "Epoch [89/90], lter [321/1752] Loss: 1.6845\n",
      "Epoch [89/90], lter [331/1752] Loss: 1.5567\n",
      "Epoch [89/90], lter [341/1752] Loss: 2.0185\n",
      "Epoch [89/90], lter [351/1752] Loss: 1.5348\n",
      "Epoch [89/90], lter [361/1752] Loss: 1.8974\n",
      "Epoch [89/90], lter [371/1752] Loss: 1.4389\n",
      "Epoch [89/90], lter [381/1752] Loss: 1.7168\n",
      "Epoch [89/90], lter [391/1752] Loss: 1.4968\n",
      "Epoch [89/90], lter [401/1752] Loss: 1.7358\n",
      "Epoch [89/90], lter [411/1752] Loss: 1.7734\n",
      "Epoch [89/90], lter [421/1752] Loss: 1.5199\n",
      "Epoch [89/90], lter [431/1752] Loss: 1.6330\n",
      "Epoch [89/90], lter [441/1752] Loss: 1.3355\n",
      "Epoch [89/90], lter [451/1752] Loss: 1.4845\n",
      "Epoch [89/90], lter [461/1752] Loss: 1.4213\n",
      "Epoch [89/90], lter [471/1752] Loss: 1.6554\n",
      "Epoch [89/90], lter [481/1752] Loss: 1.8505\n",
      "Epoch [89/90], lter [491/1752] Loss: 1.3974\n",
      "Epoch [89/90], lter [501/1752] Loss: 1.6238\n",
      "Epoch [89/90], lter [511/1752] Loss: 1.7662\n",
      "Epoch [89/90], lter [521/1752] Loss: 1.7698\n",
      "Epoch [89/90], lter [531/1752] Loss: 1.4359\n",
      "Epoch [89/90], lter [541/1752] Loss: 1.7961\n",
      "Epoch [89/90], lter [551/1752] Loss: 1.5344\n",
      "Epoch [89/90], lter [561/1752] Loss: 1.7767\n",
      "Epoch [89/90], lter [571/1752] Loss: 1.3518\n",
      "Epoch [89/90], lter [581/1752] Loss: 1.5218\n",
      "Epoch [89/90], lter [591/1752] Loss: 1.4462\n",
      "Epoch [89/90], lter [601/1752] Loss: 1.2961\n",
      "Epoch [89/90], lter [611/1752] Loss: 1.6490\n",
      "Epoch [89/90], lter [621/1752] Loss: 1.6030\n",
      "Epoch [89/90], lter [631/1752] Loss: 1.7634\n",
      "Epoch [89/90], lter [641/1752] Loss: 1.8201\n",
      "Epoch [89/90], lter [651/1752] Loss: 1.8999\n",
      "Epoch [89/90], lter [661/1752] Loss: 1.5881\n",
      "Epoch [89/90], lter [671/1752] Loss: 2.0774\n",
      "Epoch [89/90], lter [681/1752] Loss: 1.6015\n",
      "Epoch [89/90], lter [691/1752] Loss: 1.6945\n",
      "Epoch [89/90], lter [701/1752] Loss: 1.7687\n",
      "Epoch [89/90], lter [711/1752] Loss: 1.7481\n",
      "Epoch [89/90], lter [721/1752] Loss: 1.6002\n",
      "Epoch [89/90], lter [731/1752] Loss: 1.5520\n",
      "Epoch [89/90], lter [741/1752] Loss: 1.7171\n",
      "Epoch [89/90], lter [751/1752] Loss: 1.7199\n",
      "Epoch [89/90], lter [761/1752] Loss: 1.8136\n",
      "Epoch [89/90], lter [771/1752] Loss: 1.7365\n",
      "Epoch [89/90], lter [781/1752] Loss: 1.7949\n",
      "Epoch [89/90], lter [791/1752] Loss: 1.5506\n",
      "Epoch [89/90], lter [801/1752] Loss: 1.5174\n",
      "Epoch [89/90], lter [811/1752] Loss: 1.8998\n",
      "Epoch [89/90], lter [821/1752] Loss: 1.5404\n",
      "Epoch [89/90], lter [831/1752] Loss: 1.5763\n",
      "Epoch [89/90], lter [841/1752] Loss: 1.6437\n",
      "Epoch [89/90], lter [851/1752] Loss: 1.8573\n",
      "Epoch [89/90], lter [861/1752] Loss: 1.7430\n",
      "Epoch [89/90], lter [871/1752] Loss: 2.0022\n",
      "Epoch [89/90], lter [881/1752] Loss: 2.0762\n",
      "Epoch [89/90], lter [891/1752] Loss: 1.8271\n",
      "Epoch [89/90], lter [901/1752] Loss: 1.2714\n",
      "Epoch [89/90], lter [911/1752] Loss: 1.6896\n",
      "Epoch [89/90], lter [921/1752] Loss: 1.9185\n",
      "Epoch [89/90], lter [931/1752] Loss: 1.8693\n",
      "Epoch [89/90], lter [941/1752] Loss: 1.5427\n",
      "Epoch [89/90], lter [951/1752] Loss: 1.9444\n",
      "Epoch [89/90], lter [961/1752] Loss: 1.5689\n",
      "Epoch [89/90], lter [971/1752] Loss: 1.1791\n",
      "Epoch [89/90], lter [981/1752] Loss: 1.6201\n",
      "Epoch [89/90], lter [991/1752] Loss: 2.1491\n",
      "Epoch [89/90], lter [1001/1752] Loss: 2.0667\n",
      "Epoch [89/90], lter [1011/1752] Loss: 1.4983\n",
      "Epoch [89/90], lter [1021/1752] Loss: 1.6753\n",
      "Epoch [89/90], lter [1031/1752] Loss: 1.3105\n",
      "Epoch [89/90], lter [1041/1752] Loss: 1.6188\n",
      "Epoch [89/90], lter [1051/1752] Loss: 1.4305\n",
      "Epoch [89/90], lter [1061/1752] Loss: 1.6647\n",
      "Epoch [89/90], lter [1071/1752] Loss: 1.6989\n",
      "Epoch [89/90], lter [1081/1752] Loss: 1.8847\n",
      "Epoch [89/90], lter [1091/1752] Loss: 1.7086\n",
      "Epoch [89/90], lter [1101/1752] Loss: 1.9180\n",
      "Epoch [89/90], lter [1111/1752] Loss: 1.8019\n",
      "Epoch [89/90], lter [1121/1752] Loss: 2.0639\n",
      "Epoch [89/90], lter [1131/1752] Loss: 1.6634\n",
      "Epoch [89/90], lter [1141/1752] Loss: 1.2433\n",
      "Epoch [89/90], lter [1151/1752] Loss: 1.3241\n",
      "Epoch [89/90], lter [1161/1752] Loss: 1.8216\n",
      "Epoch [89/90], lter [1171/1752] Loss: 1.8379\n",
      "Epoch [89/90], lter [1181/1752] Loss: 1.8254\n",
      "Epoch [89/90], lter [1191/1752] Loss: 1.7180\n",
      "Epoch [89/90], lter [1201/1752] Loss: 1.2762\n",
      "Epoch [89/90], lter [1211/1752] Loss: 2.0278\n",
      "Epoch [89/90], lter [1221/1752] Loss: 1.5761\n",
      "Epoch [89/90], lter [1231/1752] Loss: 1.3106\n",
      "Epoch [89/90], lter [1241/1752] Loss: 1.8753\n",
      "Epoch [89/90], lter [1251/1752] Loss: 2.0700\n",
      "Epoch [89/90], lter [1261/1752] Loss: 1.7146\n",
      "Epoch [89/90], lter [1271/1752] Loss: 1.5985\n",
      "Epoch [89/90], lter [1281/1752] Loss: 1.6666\n",
      "Epoch [89/90], lter [1291/1752] Loss: 1.5264\n",
      "Epoch [89/90], lter [1301/1752] Loss: 1.8118\n",
      "Epoch [89/90], lter [1311/1752] Loss: 1.1463\n",
      "Epoch [89/90], lter [1321/1752] Loss: 1.6674\n",
      "Epoch [89/90], lter [1331/1752] Loss: 1.5958\n",
      "Epoch [89/90], lter [1341/1752] Loss: 1.4562\n",
      "Epoch [89/90], lter [1351/1752] Loss: 1.8795\n",
      "Epoch [89/90], lter [1361/1752] Loss: 1.4499\n",
      "Epoch [89/90], lter [1371/1752] Loss: 1.5683\n",
      "Epoch [89/90], lter [1381/1752] Loss: 1.3645\n",
      "Epoch [89/90], lter [1391/1752] Loss: 1.5477\n",
      "Epoch [89/90], lter [1401/1752] Loss: 1.3136\n",
      "Epoch [89/90], lter [1411/1752] Loss: 1.6257\n",
      "Epoch [89/90], lter [1421/1752] Loss: 1.7292\n",
      "Epoch [89/90], lter [1431/1752] Loss: 1.6912\n",
      "Epoch [89/90], lter [1441/1752] Loss: 1.4097\n",
      "Epoch [89/90], lter [1451/1752] Loss: 1.5047\n",
      "Epoch [89/90], lter [1461/1752] Loss: 1.7155\n",
      "Epoch [89/90], lter [1471/1752] Loss: 1.9397\n",
      "Epoch [89/90], lter [1481/1752] Loss: 1.8277\n",
      "Epoch [89/90], lter [1491/1752] Loss: 1.6904\n",
      "Epoch [89/90], lter [1501/1752] Loss: 1.5360\n",
      "Epoch [89/90], lter [1511/1752] Loss: 1.4309\n",
      "Epoch [89/90], lter [1521/1752] Loss: 1.4557\n",
      "Epoch [89/90], lter [1531/1752] Loss: 1.7071\n",
      "Epoch [89/90], lter [1541/1752] Loss: 1.7583\n",
      "Epoch [89/90], lter [1551/1752] Loss: 1.5795\n",
      "Epoch [89/90], lter [1561/1752] Loss: 1.5299\n",
      "Epoch [89/90], lter [1571/1752] Loss: 1.6264\n",
      "Epoch [89/90], lter [1581/1752] Loss: 2.0973\n",
      "Epoch [89/90], lter [1591/1752] Loss: 1.3755\n",
      "Epoch [89/90], lter [1601/1752] Loss: 1.4064\n",
      "Epoch [89/90], lter [1611/1752] Loss: 1.5088\n",
      "Epoch [89/90], lter [1621/1752] Loss: 1.7892\n",
      "Epoch [89/90], lter [1631/1752] Loss: 1.7844\n",
      "Epoch [89/90], lter [1641/1752] Loss: 1.7336\n",
      "Epoch [89/90], lter [1651/1752] Loss: 1.7107\n",
      "Epoch [89/90], lter [1661/1752] Loss: 1.8569\n",
      "Epoch [89/90], lter [1671/1752] Loss: 1.6227\n",
      "Epoch [89/90], lter [1681/1752] Loss: 1.7139\n",
      "Epoch [89/90], lter [1691/1752] Loss: 1.8192\n",
      "Epoch [89/90], lter [1701/1752] Loss: 1.7690\n",
      "Epoch [89/90], lter [1711/1752] Loss: 1.6651\n",
      "Epoch [89/90], lter [1721/1752] Loss: 1.4545\n",
      "Epoch [89/90], lter [1731/1752] Loss: 1.4001\n",
      "Epoch [89/90], lter [1741/1752] Loss: 1.3433\n",
      "Epoch [89/90], lter [1751/1752] Loss: 1.6685\n",
      "Epoch:  89 | train loss : 1.6528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 89/90 [89:57:26<23:03, 1383.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  88 | test loss : 1.5536\n",
      "Epoch [90/90], lter [1/1752] Loss: 1.7849\n",
      "Epoch [90/90], lter [11/1752] Loss: 1.3901\n",
      "Epoch [90/90], lter [21/1752] Loss: 1.5012\n",
      "Epoch [90/90], lter [31/1752] Loss: 1.5726\n",
      "Epoch [90/90], lter [41/1752] Loss: 1.8749\n",
      "Epoch [90/90], lter [51/1752] Loss: 1.4391\n",
      "Epoch [90/90], lter [61/1752] Loss: 1.3837\n",
      "Epoch [90/90], lter [71/1752] Loss: 1.3433\n",
      "Epoch [90/90], lter [81/1752] Loss: 1.8068\n",
      "Epoch [90/90], lter [91/1752] Loss: 1.7876\n",
      "Epoch [90/90], lter [101/1752] Loss: 1.8290\n",
      "Epoch [90/90], lter [111/1752] Loss: 1.7019\n",
      "Epoch [90/90], lter [121/1752] Loss: 1.9735\n",
      "Epoch [90/90], lter [131/1752] Loss: 1.7826\n",
      "Epoch [90/90], lter [141/1752] Loss: 1.3876\n",
      "Epoch [90/90], lter [151/1752] Loss: 2.0638\n",
      "Epoch [90/90], lter [161/1752] Loss: 1.8288\n",
      "Epoch [90/90], lter [171/1752] Loss: 1.3867\n",
      "Epoch [90/90], lter [181/1752] Loss: 1.5316\n",
      "Epoch [90/90], lter [191/1752] Loss: 1.6545\n",
      "Epoch [90/90], lter [201/1752] Loss: 1.6986\n",
      "Epoch [90/90], lter [211/1752] Loss: 1.8110\n",
      "Epoch [90/90], lter [221/1752] Loss: 1.5685\n",
      "Epoch [90/90], lter [231/1752] Loss: 1.4189\n",
      "Epoch [90/90], lter [241/1752] Loss: 1.6513\n",
      "Epoch [90/90], lter [251/1752] Loss: 1.5678\n",
      "Epoch [90/90], lter [261/1752] Loss: 1.8186\n",
      "Epoch [90/90], lter [271/1752] Loss: 1.7180\n",
      "Epoch [90/90], lter [281/1752] Loss: 1.5181\n",
      "Epoch [90/90], lter [291/1752] Loss: 1.6316\n",
      "Epoch [90/90], lter [301/1752] Loss: 1.2930\n",
      "Epoch [90/90], lter [311/1752] Loss: 1.7792\n",
      "Epoch [90/90], lter [321/1752] Loss: 2.0761\n",
      "Epoch [90/90], lter [331/1752] Loss: 1.4953\n",
      "Epoch [90/90], lter [341/1752] Loss: 1.5438\n",
      "Epoch [90/90], lter [351/1752] Loss: 1.7250\n",
      "Epoch [90/90], lter [361/1752] Loss: 1.7834\n",
      "Epoch [90/90], lter [371/1752] Loss: 1.3839\n",
      "Epoch [90/90], lter [381/1752] Loss: 2.0818\n",
      "Epoch [90/90], lter [391/1752] Loss: 1.9010\n",
      "Epoch [90/90], lter [401/1752] Loss: 1.5212\n",
      "Epoch [90/90], lter [411/1752] Loss: 1.7134\n",
      "Epoch [90/90], lter [421/1752] Loss: 1.9788\n",
      "Epoch [90/90], lter [431/1752] Loss: 1.9670\n",
      "Epoch [90/90], lter [441/1752] Loss: 1.5361\n",
      "Epoch [90/90], lter [451/1752] Loss: 1.4208\n",
      "Epoch [90/90], lter [461/1752] Loss: 1.7612\n",
      "Epoch [90/90], lter [471/1752] Loss: 1.8072\n",
      "Epoch [90/90], lter [481/1752] Loss: 1.4946\n",
      "Epoch [90/90], lter [491/1752] Loss: 1.6130\n",
      "Epoch [90/90], lter [501/1752] Loss: 1.4283\n",
      "Epoch [90/90], lter [511/1752] Loss: 1.4911\n",
      "Epoch [90/90], lter [521/1752] Loss: 1.8218\n",
      "Epoch [90/90], lter [531/1752] Loss: 1.8541\n",
      "Epoch [90/90], lter [541/1752] Loss: 1.8048\n",
      "Epoch [90/90], lter [551/1752] Loss: 1.8901\n",
      "Epoch [90/90], lter [561/1752] Loss: 1.9643\n",
      "Epoch [90/90], lter [571/1752] Loss: 1.7091\n",
      "Epoch [90/90], lter [581/1752] Loss: 2.0649\n",
      "Epoch [90/90], lter [591/1752] Loss: 1.7072\n",
      "Epoch [90/90], lter [601/1752] Loss: 1.8813\n",
      "Epoch [90/90], lter [611/1752] Loss: 1.4025\n",
      "Epoch [90/90], lter [621/1752] Loss: 1.5772\n",
      "Epoch [90/90], lter [631/1752] Loss: 1.6675\n",
      "Epoch [90/90], lter [641/1752] Loss: 1.6961\n",
      "Epoch [90/90], lter [651/1752] Loss: 1.7913\n",
      "Epoch [90/90], lter [661/1752] Loss: 1.6781\n",
      "Epoch [90/90], lter [671/1752] Loss: 1.7397\n",
      "Epoch [90/90], lter [681/1752] Loss: 2.0276\n",
      "Epoch [90/90], lter [691/1752] Loss: 1.3492\n",
      "Epoch [90/90], lter [701/1752] Loss: 1.4780\n",
      "Epoch [90/90], lter [711/1752] Loss: 1.5258\n",
      "Epoch [90/90], lter [721/1752] Loss: 1.6322\n",
      "Epoch [90/90], lter [731/1752] Loss: 1.6494\n",
      "Epoch [90/90], lter [741/1752] Loss: 1.7667\n",
      "Epoch [90/90], lter [751/1752] Loss: 1.7682\n",
      "Epoch [90/90], lter [761/1752] Loss: 1.4204\n",
      "Epoch [90/90], lter [771/1752] Loss: 1.7765\n",
      "Epoch [90/90], lter [781/1752] Loss: 1.6059\n",
      "Epoch [90/90], lter [791/1752] Loss: 1.8693\n",
      "Epoch [90/90], lter [801/1752] Loss: 1.6623\n",
      "Epoch [90/90], lter [811/1752] Loss: 1.7246\n",
      "Epoch [90/90], lter [821/1752] Loss: 1.8215\n",
      "Epoch [90/90], lter [831/1752] Loss: 1.4182\n",
      "Epoch [90/90], lter [841/1752] Loss: 1.5460\n",
      "Epoch [90/90], lter [851/1752] Loss: 1.6086\n",
      "Epoch [90/90], lter [861/1752] Loss: 1.8368\n",
      "Epoch [90/90], lter [871/1752] Loss: 1.8583\n",
      "Epoch [90/90], lter [881/1752] Loss: 1.5596\n",
      "Epoch [90/90], lter [891/1752] Loss: 1.7461\n",
      "Epoch [90/90], lter [901/1752] Loss: 1.8442\n",
      "Epoch [90/90], lter [911/1752] Loss: 1.7630\n",
      "Epoch [90/90], lter [921/1752] Loss: 1.6882\n",
      "Epoch [90/90], lter [931/1752] Loss: 1.5644\n",
      "Epoch [90/90], lter [941/1752] Loss: 1.3620\n",
      "Epoch [90/90], lter [951/1752] Loss: 1.6577\n",
      "Epoch [90/90], lter [961/1752] Loss: 1.6289\n",
      "Epoch [90/90], lter [971/1752] Loss: 1.2174\n",
      "Epoch [90/90], lter [981/1752] Loss: 1.9013\n",
      "Epoch [90/90], lter [991/1752] Loss: 1.4703\n",
      "Epoch [90/90], lter [1001/1752] Loss: 1.5390\n",
      "Epoch [90/90], lter [1011/1752] Loss: 1.7253\n",
      "Epoch [90/90], lter [1021/1752] Loss: 1.5335\n",
      "Epoch [90/90], lter [1031/1752] Loss: 1.7804\n",
      "Epoch [90/90], lter [1041/1752] Loss: 1.9508\n",
      "Epoch [90/90], lter [1051/1752] Loss: 1.8081\n",
      "Epoch [90/90], lter [1061/1752] Loss: 1.7356\n",
      "Epoch [90/90], lter [1071/1752] Loss: 1.7120\n",
      "Epoch [90/90], lter [1081/1752] Loss: 2.1253\n",
      "Epoch [90/90], lter [1091/1752] Loss: 1.6307\n",
      "Epoch [90/90], lter [1101/1752] Loss: 1.8764\n",
      "Epoch [90/90], lter [1111/1752] Loss: 1.8152\n",
      "Epoch [90/90], lter [1121/1752] Loss: 2.1116\n",
      "Epoch [90/90], lter [1131/1752] Loss: 1.5695\n",
      "Epoch [90/90], lter [1141/1752] Loss: 1.6729\n",
      "Epoch [90/90], lter [1151/1752] Loss: 1.7055\n",
      "Epoch [90/90], lter [1161/1752] Loss: 1.8925\n",
      "Epoch [90/90], lter [1171/1752] Loss: 1.8350\n",
      "Epoch [90/90], lter [1181/1752] Loss: 1.6604\n",
      "Epoch [90/90], lter [1191/1752] Loss: 1.7350\n",
      "Epoch [90/90], lter [1201/1752] Loss: 1.7837\n",
      "Epoch [90/90], lter [1211/1752] Loss: 1.7200\n",
      "Epoch [90/90], lter [1221/1752] Loss: 1.4024\n",
      "Epoch [90/90], lter [1231/1752] Loss: 1.4581\n",
      "Epoch [90/90], lter [1241/1752] Loss: 1.2251\n",
      "Epoch [90/90], lter [1251/1752] Loss: 1.4374\n",
      "Epoch [90/90], lter [1261/1752] Loss: 1.7719\n",
      "Epoch [90/90], lter [1271/1752] Loss: 1.9569\n",
      "Epoch [90/90], lter [1281/1752] Loss: 1.6959\n",
      "Epoch [90/90], lter [1291/1752] Loss: 1.6039\n",
      "Epoch [90/90], lter [1301/1752] Loss: 1.7524\n",
      "Epoch [90/90], lter [1311/1752] Loss: 1.9219\n",
      "Epoch [90/90], lter [1321/1752] Loss: 1.7385\n",
      "Epoch [90/90], lter [1331/1752] Loss: 1.4912\n",
      "Epoch [90/90], lter [1341/1752] Loss: 1.8997\n",
      "Epoch [90/90], lter [1351/1752] Loss: 1.7245\n",
      "Epoch [90/90], lter [1361/1752] Loss: 1.6440\n",
      "Epoch [90/90], lter [1371/1752] Loss: 1.6975\n",
      "Epoch [90/90], lter [1381/1752] Loss: 1.3563\n",
      "Epoch [90/90], lter [1391/1752] Loss: 1.7497\n",
      "Epoch [90/90], lter [1401/1752] Loss: 1.8824\n",
      "Epoch [90/90], lter [1411/1752] Loss: 1.6651\n",
      "Epoch [90/90], lter [1421/1752] Loss: 1.5618\n",
      "Epoch [90/90], lter [1431/1752] Loss: 1.4504\n",
      "Epoch [90/90], lter [1441/1752] Loss: 1.4946\n",
      "Epoch [90/90], lter [1451/1752] Loss: 1.8474\n",
      "Epoch [90/90], lter [1461/1752] Loss: 1.5015\n",
      "Epoch [90/90], lter [1471/1752] Loss: 1.7800\n",
      "Epoch [90/90], lter [1481/1752] Loss: 1.6305\n",
      "Epoch [90/90], lter [1491/1752] Loss: 1.6779\n",
      "Epoch [90/90], lter [1501/1752] Loss: 1.3136\n",
      "Epoch [90/90], lter [1511/1752] Loss: 1.6113\n",
      "Epoch [90/90], lter [1521/1752] Loss: 1.7961\n",
      "Epoch [90/90], lter [1531/1752] Loss: 1.5917\n",
      "Epoch [90/90], lter [1541/1752] Loss: 1.9673\n",
      "Epoch [90/90], lter [1551/1752] Loss: 1.6949\n",
      "Epoch [90/90], lter [1561/1752] Loss: 1.7479\n",
      "Epoch [90/90], lter [1571/1752] Loss: 1.7873\n",
      "Epoch [90/90], lter [1581/1752] Loss: 1.5422\n",
      "Epoch [90/90], lter [1591/1752] Loss: 1.3294\n",
      "Epoch [90/90], lter [1601/1752] Loss: 1.5613\n",
      "Epoch [90/90], lter [1611/1752] Loss: 1.6500\n",
      "Epoch [90/90], lter [1621/1752] Loss: 1.7898\n",
      "Epoch [90/90], lter [1631/1752] Loss: 1.4930\n",
      "Epoch [90/90], lter [1641/1752] Loss: 1.8446\n",
      "Epoch [90/90], lter [1651/1752] Loss: 1.4998\n",
      "Epoch [90/90], lter [1661/1752] Loss: 1.4870\n",
      "Epoch [90/90], lter [1671/1752] Loss: 1.5820\n",
      "Epoch [90/90], lter [1681/1752] Loss: 1.7366\n",
      "Epoch [90/90], lter [1691/1752] Loss: 1.7273\n",
      "Epoch [90/90], lter [1701/1752] Loss: 1.7323\n",
      "Epoch [90/90], lter [1711/1752] Loss: 1.6126\n",
      "Epoch [90/90], lter [1721/1752] Loss: 1.6469\n",
      "Epoch [90/90], lter [1731/1752] Loss: 1.6932\n",
      "Epoch [90/90], lter [1741/1752] Loss: 1.5168\n",
      "Epoch [90/90], lter [1751/1752] Loss: 1.4747\n",
      "Epoch:  90 | train loss : 1.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [90:20:34<00:00, 3613.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  89 | test loss : 1.5466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Start training\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    total_batch = len(train_data_loader.dataset)//BATCH_SIZE\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()  # Set the model to train mode\n",
    "\n",
    "    # Get statistics\n",
    "    epoch_loss = 0\n",
    "    len_dataset = 0\n",
    "    \n",
    "    for step, (batch_images, batch_labels) in enumerate(train_data_loader):\n",
    "        X, Y = batch_images.to(device), batch_labels.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += pred.shape[0] * loss.item()\n",
    "        len_dataset += pred.shape[0]\n",
    "        if (step) % 10 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, EPOCHS, step+1, total_batch, loss.item()))\n",
    "\n",
    "    epoch_loss = epoch_loss/ len_dataset\n",
    "    print('Epoch: ', epoch+1, '| train loss : %0.4f' % epoch_loss)\n",
    "\n",
    "    LR_SCHEDULER.step()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.inference_mode():  \n",
    "        running_loss = 0\n",
    "        for step, (images, labels) in enumerate(test_data_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    running_loss = running_loss / len(test_data_loader)\n",
    "    print('Epoch: ', epoch, '| test loss : %0.4f' % running_loss )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b84f01a-84ff-4a51-a83f-a2735e83abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), 'trained_inception_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a178ad-fd25-4c31-ad14-05daf5bd57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\kh2197vi\\downloads\\facial_emotion_detec-main\\facial_emotion_detec-main\\.env\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('trained_inception_v3.pt', map_location=torch.device('cpu'), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64a2ad3-5a4e-431f-80af-6d57cdd1e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\kh2197vi\\downloads\\facial_emotion_detec-main\\facial_emotion_detec-main\\.env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\users\\kh2197vi\\downloads\\facial_emotion_detec-main\\facial_emotion_detec-main\\.env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): None\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=2048, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to the saved model\n",
    "PATH = 'trained_inception_v3.pt'\n",
    "\n",
    "# Load the model\n",
    "model = models.inception_v3(pretrained=True)\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer with a new one that outputs 7 classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 7)  # Output layer with 7 classes\n",
    "model.aux_logits = False\n",
    "model.AuxLogits = None\n",
    "\n",
    "# Load the saved weights\n",
    "model_state_dict = torch.load(PATH)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb52e2f-3e74-4173-a1aa-fdced1c79f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.87%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the correct_predictions and total_predictions variables\n",
    "correct_predictions, total_predictions = 0, 0\n",
    "\n",
    "# Loop over the batches of the test set\n",
    "for batch_data in test_data_loader:\n",
    "    # Forward pass\n",
    "    inputs, target = batch_data\n",
    "    output = model(inputs)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    # Increase the total predictions counter\n",
    "    total_predictions += target.size(0)\n",
    "\n",
    "    # Check if the predicted class is equal to the true class\n",
    "    correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
