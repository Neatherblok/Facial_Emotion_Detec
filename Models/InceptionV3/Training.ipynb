{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "574f39d9-dab6-45c3-bfd3-78e3dfebfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# Specify where to find the data preparation class\n",
    "sys.path.append('../../Data_Preparation')\n",
    "from Preparation import CustomDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008ac5be-e4af-4ee9-bb44-2d8a137cf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3 training data (ImageNet) properties\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "DIMENSIONS = 3\n",
    "#SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7889fed-b144-4a60-852b-ef19a3d1fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Loader:\n",
      "Batch Index: 0\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([2, 6, 0, 3, 2])\n",
      "Batch Index: 1\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([6, 3, 2, 5, 6])\n",
      "Batch Index: 2\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([6, 5, 6, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CustomDataLoader class for training\n",
    "train_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=32, dataset_type=\"train\", mean=MEAN, std=STD, dimensions=DIMENSIONS).data_loader\n",
    "test_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=32, dataset_type=\"test\", mean=MEAN, std=STD, dimensions=DIMENSIONS).data_loader\n",
    "\n",
    "# Confirm correct data load\n",
    "print(\"Train Data Loader:\")\n",
    "for batch_idx, (inputs, labels) in enumerate(train_data_loader):\n",
    "    print(\"Batch Index:\", batch_idx)\n",
    "    print(\"Inputs Shape:\", inputs.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    # Print the first few labels in the batch\n",
    "    print(\"Labels:\", labels[:5])\n",
    "    # Break after printing a few batches\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dab3e7ce-6b24-499b-b5b4-4708ffcce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the InceptionV3 model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = models.inception_v3(pretrained=True)\n",
    "\n",
    "model.aux_logits = False\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model_ft.fc.in_features, 10),\n",
    "    nn.Linear(10, 7)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3c195-5ac7-4b5c-ac35-5ff58399b50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "Epoch [1/25], lter [5/876] Loss: 1.9459\n",
      "Epoch [1/25], lter [10/876] Loss: 1.9082\n",
      "Epoch [1/25], lter [15/876] Loss: 1.9744\n",
      "Epoch [1/25], lter [20/876] Loss: 2.0286\n",
      "Epoch [1/25], lter [25/876] Loss: 2.0308\n",
      "Epoch [1/25], lter [30/876] Loss: 1.9939\n",
      "Epoch [1/25], lter [35/876] Loss: 1.9843\n",
      "Epoch [1/25], lter [40/876] Loss: 1.9548\n",
      "Epoch [1/25], lter [45/876] Loss: 1.9837\n",
      "Epoch [1/25], lter [50/876] Loss: 1.9853\n",
      "Epoch [1/25], lter [55/876] Loss: 1.9757\n",
      "Epoch [1/25], lter [60/876] Loss: 1.9699\n",
      "Epoch [1/25], lter [65/876] Loss: 1.9789\n",
      "Epoch [1/25], lter [70/876] Loss: 1.9876\n",
      "Epoch [1/25], lter [75/876] Loss: 1.9915\n",
      "Epoch [1/25], lter [80/876] Loss: 1.9935\n",
      "Epoch [1/25], lter [85/876] Loss: 1.9770\n",
      "Epoch [1/25], lter [90/876] Loss: 1.9529\n",
      "Epoch [1/25], lter [95/876] Loss: 1.9444\n",
      "Epoch [1/25], lter [100/876] Loss: 1.9724\n",
      "Epoch [1/25], lter [105/876] Loss: 1.9823\n",
      "Epoch [1/25], lter [110/876] Loss: 1.9896\n",
      "Epoch [1/25], lter [115/876] Loss: 1.9575\n",
      "Epoch [1/25], lter [120/876] Loss: 1.9437\n",
      "Epoch [1/25], lter [125/876] Loss: 1.9547\n",
      "Epoch [1/25], lter [130/876] Loss: 2.0132\n",
      "Epoch [1/25], lter [135/876] Loss: 1.9820\n",
      "Epoch [1/25], lter [140/876] Loss: 2.0049\n",
      "Epoch [1/25], lter [145/876] Loss: 2.0358\n",
      "Epoch [1/25], lter [150/876] Loss: 1.9886\n",
      "Epoch [1/25], lter [155/876] Loss: 1.9903\n",
      "Epoch [1/25], lter [160/876] Loss: 1.9613\n",
      "Epoch [1/25], lter [165/876] Loss: 1.9409\n",
      "Epoch [1/25], lter [170/876] Loss: 1.9670\n",
      "Epoch [1/25], lter [175/876] Loss: 1.9519\n",
      "Epoch [1/25], lter [180/876] Loss: 1.9771\n",
      "Epoch [1/25], lter [185/876] Loss: 1.9311\n",
      "Epoch [1/25], lter [190/876] Loss: 1.9991\n",
      "Epoch [1/25], lter [195/876] Loss: 1.9444\n",
      "Epoch [1/25], lter [200/876] Loss: 1.9132\n",
      "Epoch [1/25], lter [205/876] Loss: 1.9623\n",
      "Epoch [1/25], lter [210/876] Loss: 1.9482\n",
      "Epoch [1/25], lter [215/876] Loss: 1.9349\n",
      "Epoch [1/25], lter [220/876] Loss: 1.9860\n",
      "Epoch [1/25], lter [225/876] Loss: 1.9838\n",
      "Epoch [1/25], lter [230/876] Loss: 1.9348\n",
      "Epoch [1/25], lter [235/876] Loss: 1.9692\n",
      "Epoch [1/25], lter [240/876] Loss: 1.9470\n",
      "Epoch [1/25], lter [245/876] Loss: 1.9499\n",
      "Epoch [1/25], lter [250/876] Loss: 1.9469\n",
      "Epoch [1/25], lter [255/876] Loss: 2.0430\n",
      "Epoch [1/25], lter [260/876] Loss: 2.0261\n",
      "Epoch [1/25], lter [265/876] Loss: 1.9471\n",
      "Epoch [1/25], lter [270/876] Loss: 1.9584\n",
      "Epoch [1/25], lter [275/876] Loss: 1.9795\n",
      "Epoch [1/25], lter [280/876] Loss: 1.9704\n",
      "Epoch [1/25], lter [285/876] Loss: 1.9989\n",
      "Epoch [1/25], lter [290/876] Loss: 1.9953\n",
      "Epoch [1/25], lter [295/876] Loss: 1.9671\n",
      "Epoch [1/25], lter [300/876] Loss: 1.9813\n",
      "Epoch [1/25], lter [305/876] Loss: 1.9679\n",
      "Epoch [1/25], lter [310/876] Loss: 1.9572\n",
      "Epoch [1/25], lter [315/876] Loss: 1.9615\n",
      "Epoch [1/25], lter [320/876] Loss: 1.9817\n",
      "Epoch [1/25], lter [325/876] Loss: 2.0018\n",
      "Epoch [1/25], lter [330/876] Loss: 1.9703\n",
      "Epoch [1/25], lter [335/876] Loss: 2.0268\n",
      "Epoch [1/25], lter [340/876] Loss: 1.9241\n",
      "Epoch [1/25], lter [345/876] Loss: 1.9644\n",
      "Epoch [1/25], lter [350/876] Loss: 2.0095\n",
      "Epoch [1/25], lter [355/876] Loss: 1.9636\n",
      "Epoch [1/25], lter [360/876] Loss: 1.9448\n",
      "Epoch [1/25], lter [365/876] Loss: 1.9680\n",
      "Epoch [1/25], lter [370/876] Loss: 2.0120\n",
      "Epoch [1/25], lter [375/876] Loss: 1.9716\n",
      "Epoch [1/25], lter [380/876] Loss: 1.9824\n",
      "Epoch [1/25], lter [385/876] Loss: 1.9730\n",
      "Epoch [1/25], lter [390/876] Loss: 1.9934\n",
      "Epoch [1/25], lter [395/876] Loss: 1.9294\n",
      "Epoch [1/25], lter [400/876] Loss: 1.9921\n",
      "Epoch [1/25], lter [405/876] Loss: 1.9958\n",
      "Epoch [1/25], lter [410/876] Loss: 1.9145\n",
      "Epoch [1/25], lter [415/876] Loss: 1.9545\n",
      "Epoch [1/25], lter [420/876] Loss: 1.9600\n",
      "Epoch [1/25], lter [425/876] Loss: 2.0326\n",
      "Epoch [1/25], lter [430/876] Loss: 1.9987\n",
      "Epoch [1/25], lter [435/876] Loss: 1.9231\n",
      "Epoch [1/25], lter [440/876] Loss: 1.9860\n",
      "Epoch [1/25], lter [445/876] Loss: 2.0143\n",
      "Epoch [1/25], lter [450/876] Loss: 1.9137\n",
      "Epoch [1/25], lter [455/876] Loss: 1.9609\n",
      "Epoch [1/25], lter [460/876] Loss: 2.0003\n",
      "Epoch [1/25], lter [465/876] Loss: 1.9819\n",
      "Epoch [1/25], lter [470/876] Loss: 1.9504\n",
      "Epoch [1/25], lter [475/876] Loss: 2.0479\n",
      "Epoch [1/25], lter [480/876] Loss: 1.9946\n",
      "Epoch [1/25], lter [485/876] Loss: 1.9595\n",
      "Epoch [1/25], lter [490/876] Loss: 1.9818\n",
      "Epoch [1/25], lter [495/876] Loss: 2.0104\n",
      "Epoch [1/25], lter [500/876] Loss: 2.0211\n",
      "Epoch [1/25], lter [505/876] Loss: 1.9888\n",
      "Epoch [1/25], lter [510/876] Loss: 1.9688\n",
      "Epoch [1/25], lter [515/876] Loss: 1.9521\n",
      "Epoch [1/25], lter [520/876] Loss: 1.9814\n",
      "Epoch [1/25], lter [525/876] Loss: 1.9351\n",
      "Epoch [1/25], lter [530/876] Loss: 2.0239\n",
      "Epoch [1/25], lter [535/876] Loss: 1.9666\n",
      "Epoch [1/25], lter [540/876] Loss: 1.9955\n",
      "Epoch [1/25], lter [545/876] Loss: 2.0234\n",
      "Epoch [1/25], lter [550/876] Loss: 1.9897\n",
      "Epoch [1/25], lter [555/876] Loss: 1.9923\n",
      "Epoch [1/25], lter [560/876] Loss: 1.9017\n",
      "Epoch [1/25], lter [565/876] Loss: 1.9625\n",
      "Epoch [1/25], lter [570/876] Loss: 1.9631\n",
      "Epoch [1/25], lter [575/876] Loss: 1.9822\n",
      "Epoch [1/25], lter [580/876] Loss: 1.9959\n",
      "Epoch [1/25], lter [585/876] Loss: 1.9802\n",
      "Epoch [1/25], lter [590/876] Loss: 1.9843\n",
      "Epoch [1/25], lter [595/876] Loss: 1.9925\n",
      "Epoch [1/25], lter [600/876] Loss: 1.9609\n",
      "Epoch [1/25], lter [605/876] Loss: 1.9451\n",
      "Epoch [1/25], lter [610/876] Loss: 2.0204\n",
      "Epoch [1/25], lter [615/876] Loss: 1.9890\n",
      "Epoch [1/25], lter [620/876] Loss: 1.9476\n",
      "Epoch [1/25], lter [625/876] Loss: 1.9967\n",
      "Epoch [1/25], lter [630/876] Loss: 1.9442\n",
      "Epoch [1/25], lter [635/876] Loss: 1.9809\n",
      "Epoch [1/25], lter [640/876] Loss: 1.9752\n",
      "Epoch [1/25], lter [645/876] Loss: 1.9678\n",
      "Epoch [1/25], lter [650/876] Loss: 1.9705\n",
      "Epoch [1/25], lter [655/876] Loss: 1.9035\n",
      "Epoch [1/25], lter [660/876] Loss: 1.9951\n",
      "Epoch [1/25], lter [665/876] Loss: 1.9803\n",
      "Epoch [1/25], lter [670/876] Loss: 1.9792\n",
      "Epoch [1/25], lter [675/876] Loss: 2.0119\n",
      "Epoch [1/25], lter [680/876] Loss: 2.0023\n",
      "Epoch [1/25], lter [685/876] Loss: 1.9184\n",
      "Epoch [1/25], lter [690/876] Loss: 1.9552\n",
      "Epoch [1/25], lter [695/876] Loss: 1.9448\n",
      "Epoch [1/25], lter [700/876] Loss: 2.0013\n",
      "Epoch [1/25], lter [705/876] Loss: 2.0323\n",
      "Epoch [1/25], lter [710/876] Loss: 2.0204\n",
      "Epoch [1/25], lter [715/876] Loss: 1.9713\n",
      "Epoch [1/25], lter [720/876] Loss: 1.9886\n",
      "Epoch [1/25], lter [725/876] Loss: 1.9337\n",
      "Epoch [1/25], lter [730/876] Loss: 1.9539\n",
      "Epoch [1/25], lter [735/876] Loss: 1.9168\n",
      "Epoch [1/25], lter [740/876] Loss: 1.9596\n",
      "Epoch [1/25], lter [745/876] Loss: 1.9352\n",
      "Epoch [1/25], lter [750/876] Loss: 1.9862\n",
      "Epoch [1/25], lter [755/876] Loss: 1.9957\n",
      "Epoch [1/25], lter [760/876] Loss: 1.9575\n",
      "Epoch [1/25], lter [765/876] Loss: 1.9774\n",
      "Epoch [1/25], lter [770/876] Loss: 1.9647\n",
      "Epoch [1/25], lter [775/876] Loss: 1.9909\n",
      "Epoch [1/25], lter [780/876] Loss: 1.9976\n",
      "Epoch [1/25], lter [785/876] Loss: 1.9583\n",
      "Epoch [1/25], lter [790/876] Loss: 1.9580\n",
      "Epoch [1/25], lter [795/876] Loss: 1.9726\n",
      "Epoch [1/25], lter [800/876] Loss: 1.9601\n",
      "Epoch [1/25], lter [805/876] Loss: 1.9623\n",
      "Epoch [1/25], lter [810/876] Loss: 1.9430\n",
      "Epoch [1/25], lter [815/876] Loss: 1.9749\n",
      "Epoch [1/25], lter [820/876] Loss: 1.9976\n",
      "Epoch [1/25], lter [825/876] Loss: 1.9930\n",
      "Epoch [1/25], lter [830/876] Loss: 1.9839\n",
      "Epoch [1/25], lter [835/876] Loss: 1.9959\n",
      "Epoch [1/25], lter [840/876] Loss: 1.9737\n",
      "Epoch [1/25], lter [845/876] Loss: 1.9870\n",
      "Epoch [1/25], lter [850/876] Loss: 2.0004\n",
      "Epoch [1/25], lter [855/876] Loss: 1.9553\n",
      "Epoch [1/25], lter [860/876] Loss: 2.0028\n",
      "Epoch [1/25], lter [865/876] Loss: 1.9784\n",
      "Epoch [1/25], lter [870/876] Loss: 1.9486\n",
      "Epoch [1/25], lter [875/876] Loss: 1.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [59:14<23:41:37, 3554.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "----------\n",
      "Epoch [2/25], lter [5/876] Loss: 1.9891\n",
      "Epoch [2/25], lter [10/876] Loss: 1.9390\n",
      "Epoch [2/25], lter [15/876] Loss: 1.9907\n",
      "Epoch [2/25], lter [20/876] Loss: 1.9649\n",
      "Epoch [2/25], lter [25/876] Loss: 1.9994\n",
      "Epoch [2/25], lter [30/876] Loss: 1.9478\n",
      "Epoch [2/25], lter [35/876] Loss: 1.9730\n",
      "Epoch [2/25], lter [40/876] Loss: 1.9958\n",
      "Epoch [2/25], lter [45/876] Loss: 2.0026\n",
      "Epoch [2/25], lter [50/876] Loss: 1.9470\n",
      "Epoch [2/25], lter [55/876] Loss: 1.9505\n",
      "Epoch [2/25], lter [60/876] Loss: 1.9981\n",
      "Epoch [2/25], lter [65/876] Loss: 1.9695\n",
      "Epoch [2/25], lter [70/876] Loss: 1.9718\n",
      "Epoch [2/25], lter [75/876] Loss: 1.9771\n",
      "Epoch [2/25], lter [80/876] Loss: 2.0006\n",
      "Epoch [2/25], lter [85/876] Loss: 1.9101\n",
      "Epoch [2/25], lter [90/876] Loss: 1.9532\n",
      "Epoch [2/25], lter [95/876] Loss: 1.8949\n",
      "Epoch [2/25], lter [100/876] Loss: 1.9529\n",
      "Epoch [2/25], lter [105/876] Loss: 1.9264\n",
      "Epoch [2/25], lter [110/876] Loss: 1.9295\n",
      "Epoch [2/25], lter [115/876] Loss: 1.9461\n",
      "Epoch [2/25], lter [120/876] Loss: 1.9794\n",
      "Epoch [2/25], lter [125/876] Loss: 2.0004\n",
      "Epoch [2/25], lter [130/876] Loss: 2.0002\n",
      "Epoch [2/25], lter [135/876] Loss: 1.9786\n",
      "Epoch [2/25], lter [140/876] Loss: 1.9909\n",
      "Epoch [2/25], lter [145/876] Loss: 1.9995\n",
      "Epoch [2/25], lter [150/876] Loss: 1.9849\n",
      "Epoch [2/25], lter [155/876] Loss: 2.0166\n",
      "Epoch [2/25], lter [160/876] Loss: 1.9723\n",
      "Epoch [2/25], lter [165/876] Loss: 2.0211\n",
      "Epoch [2/25], lter [170/876] Loss: 1.9385\n",
      "Epoch [2/25], lter [175/876] Loss: 1.9582\n",
      "Epoch [2/25], lter [180/876] Loss: 1.9393\n",
      "Epoch [2/25], lter [185/876] Loss: 1.9696\n",
      "Epoch [2/25], lter [190/876] Loss: 1.9969\n",
      "Epoch [2/25], lter [195/876] Loss: 1.9667\n",
      "Epoch [2/25], lter [200/876] Loss: 1.9575\n",
      "Epoch [2/25], lter [205/876] Loss: 1.9739\n",
      "Epoch [2/25], lter [210/876] Loss: 2.0342\n",
      "Epoch [2/25], lter [215/876] Loss: 1.9792\n",
      "Epoch [2/25], lter [220/876] Loss: 1.9674\n",
      "Epoch [2/25], lter [225/876] Loss: 1.9200\n",
      "Epoch [2/25], lter [230/876] Loss: 1.9949\n",
      "Epoch [2/25], lter [235/876] Loss: 2.0269\n",
      "Epoch [2/25], lter [240/876] Loss: 1.9246\n",
      "Epoch [2/25], lter [245/876] Loss: 2.0589\n",
      "Epoch [2/25], lter [250/876] Loss: 1.9837\n",
      "Epoch [2/25], lter [255/876] Loss: 1.9354\n",
      "Epoch [2/25], lter [260/876] Loss: 1.9717\n",
      "Epoch [2/25], lter [265/876] Loss: 1.9810\n",
      "Epoch [2/25], lter [270/876] Loss: 2.0213\n",
      "Epoch [2/25], lter [275/876] Loss: 1.9326\n",
      "Epoch [2/25], lter [280/876] Loss: 1.9877\n",
      "Epoch [2/25], lter [285/876] Loss: 1.9642\n",
      "Epoch [2/25], lter [290/876] Loss: 1.9716\n",
      "Epoch [2/25], lter [295/876] Loss: 1.9709\n",
      "Epoch [2/25], lter [300/876] Loss: 2.0184\n",
      "Epoch [2/25], lter [305/876] Loss: 2.0351\n",
      "Epoch [2/25], lter [310/876] Loss: 1.9874\n",
      "Epoch [2/25], lter [315/876] Loss: 2.0134\n",
      "Epoch [2/25], lter [320/876] Loss: 1.9698\n",
      "Epoch [2/25], lter [325/876] Loss: 1.9458\n",
      "Epoch [2/25], lter [330/876] Loss: 2.0150\n",
      "Epoch [2/25], lter [335/876] Loss: 1.9650\n",
      "Epoch [2/25], lter [340/876] Loss: 1.9929\n",
      "Epoch [2/25], lter [345/876] Loss: 1.9988\n",
      "Epoch [2/25], lter [350/876] Loss: 1.9633\n",
      "Epoch [2/25], lter [355/876] Loss: 1.9630\n",
      "Epoch [2/25], lter [360/876] Loss: 1.9222\n",
      "Epoch [2/25], lter [365/876] Loss: 1.9574\n",
      "Epoch [2/25], lter [370/876] Loss: 1.9729\n",
      "Epoch [2/25], lter [375/876] Loss: 1.9871\n",
      "Epoch [2/25], lter [380/876] Loss: 1.9952\n",
      "Epoch [2/25], lter [385/876] Loss: 1.9431\n",
      "Epoch [2/25], lter [390/876] Loss: 1.9291\n",
      "Epoch [2/25], lter [395/876] Loss: 1.9934\n",
      "Epoch [2/25], lter [400/876] Loss: 1.9497\n",
      "Epoch [2/25], lter [405/876] Loss: 1.9543\n",
      "Epoch [2/25], lter [410/876] Loss: 1.9343\n",
      "Epoch [2/25], lter [415/876] Loss: 1.9709\n",
      "Epoch [2/25], lter [420/876] Loss: 1.9597\n",
      "Epoch [2/25], lter [425/876] Loss: 1.9412\n",
      "Epoch [2/25], lter [430/876] Loss: 1.9585\n",
      "Epoch [2/25], lter [435/876] Loss: 1.9783\n",
      "Epoch [2/25], lter [440/876] Loss: 1.9722\n",
      "Epoch [2/25], lter [445/876] Loss: 2.0034\n",
      "Epoch [2/25], lter [450/876] Loss: 1.9739\n",
      "Epoch [2/25], lter [455/876] Loss: 1.9904\n",
      "Epoch [2/25], lter [460/876] Loss: 1.9662\n",
      "Epoch [2/25], lter [465/876] Loss: 1.9617\n",
      "Epoch [2/25], lter [470/876] Loss: 2.0049\n",
      "Epoch [2/25], lter [475/876] Loss: 1.9500\n",
      "Epoch [2/25], lter [480/876] Loss: 1.9974\n",
      "Epoch [2/25], lter [485/876] Loss: 1.9964\n",
      "Epoch [2/25], lter [490/876] Loss: 1.9715\n",
      "Epoch [2/25], lter [495/876] Loss: 1.9433\n",
      "Epoch [2/25], lter [500/876] Loss: 1.9554\n",
      "Epoch [2/25], lter [505/876] Loss: 1.9822\n",
      "Epoch [2/25], lter [510/876] Loss: 1.9906\n",
      "Epoch [2/25], lter [515/876] Loss: 2.0551\n",
      "Epoch [2/25], lter [520/876] Loss: 1.9874\n",
      "Epoch [2/25], lter [525/876] Loss: 2.0039\n",
      "Epoch [2/25], lter [530/876] Loss: 1.9603\n",
      "Epoch [2/25], lter [535/876] Loss: 1.9817\n",
      "Epoch [2/25], lter [540/876] Loss: 2.0140\n",
      "Epoch [2/25], lter [545/876] Loss: 1.9544\n",
      "Epoch [2/25], lter [550/876] Loss: 1.9773\n",
      "Epoch [2/25], lter [555/876] Loss: 1.9709\n",
      "Epoch [2/25], lter [560/876] Loss: 1.9414\n",
      "Epoch [2/25], lter [565/876] Loss: 1.9895\n",
      "Epoch [2/25], lter [570/876] Loss: 1.9840\n",
      "Epoch [2/25], lter [575/876] Loss: 1.9537\n",
      "Epoch [2/25], lter [580/876] Loss: 1.9440\n",
      "Epoch [2/25], lter [585/876] Loss: 2.0238\n",
      "Epoch [2/25], lter [590/876] Loss: 1.9817\n",
      "Epoch [2/25], lter [595/876] Loss: 1.9666\n",
      "Epoch [2/25], lter [600/876] Loss: 1.9748\n",
      "Epoch [2/25], lter [605/876] Loss: 1.9628\n",
      "Epoch [2/25], lter [610/876] Loss: 1.9793\n",
      "Epoch [2/25], lter [615/876] Loss: 1.9759\n",
      "Epoch [2/25], lter [620/876] Loss: 1.9789\n",
      "Epoch [2/25], lter [625/876] Loss: 1.9918\n",
      "Epoch [2/25], lter [630/876] Loss: 2.0137\n",
      "Epoch [2/25], lter [635/876] Loss: 1.9546\n",
      "Epoch [2/25], lter [640/876] Loss: 2.0183\n",
      "Epoch [2/25], lter [645/876] Loss: 1.9692\n",
      "Epoch [2/25], lter [650/876] Loss: 1.9788\n",
      "Epoch [2/25], lter [655/876] Loss: 1.9555\n",
      "Epoch [2/25], lter [660/876] Loss: 1.9782\n",
      "Epoch [2/25], lter [665/876] Loss: 1.9554\n",
      "Epoch [2/25], lter [670/876] Loss: 1.8930\n",
      "Epoch [2/25], lter [675/876] Loss: 1.9359\n",
      "Epoch [2/25], lter [680/876] Loss: 2.0403\n",
      "Epoch [2/25], lter [685/876] Loss: 1.9577\n",
      "Epoch [2/25], lter [690/876] Loss: 1.9426\n",
      "Epoch [2/25], lter [695/876] Loss: 1.9915\n",
      "Epoch [2/25], lter [700/876] Loss: 2.0253\n",
      "Epoch [2/25], lter [705/876] Loss: 1.9339\n",
      "Epoch [2/25], lter [710/876] Loss: 1.9823\n",
      "Epoch [2/25], lter [715/876] Loss: 1.9911\n",
      "Epoch [2/25], lter [720/876] Loss: 1.9566\n",
      "Epoch [2/25], lter [725/876] Loss: 1.9961\n",
      "Epoch [2/25], lter [730/876] Loss: 1.9539\n",
      "Epoch [2/25], lter [735/876] Loss: 1.9997\n",
      "Epoch [2/25], lter [740/876] Loss: 1.9588\n",
      "Epoch [2/25], lter [745/876] Loss: 1.9813\n",
      "Epoch [2/25], lter [750/876] Loss: 1.9468\n",
      "Epoch [2/25], lter [755/876] Loss: 1.9177\n",
      "Epoch [2/25], lter [760/876] Loss: 1.9583\n",
      "Epoch [2/25], lter [765/876] Loss: 2.0168\n",
      "Epoch [2/25], lter [770/876] Loss: 1.9791\n",
      "Epoch [2/25], lter [775/876] Loss: 2.0010\n",
      "Epoch [2/25], lter [780/876] Loss: 1.9805\n",
      "Epoch [2/25], lter [785/876] Loss: 1.9624\n",
      "Epoch [2/25], lter [790/876] Loss: 1.9813\n",
      "Epoch [2/25], lter [795/876] Loss: 1.9645\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    total_batch = len(train_data_loader.dataset)//32\n",
    "\n",
    "    for i, (batch_images, batch_labels) in enumerate(train_data_loader):\n",
    "        \n",
    "        X = batch_images.to(device)\n",
    "        Y = batch_labels.to(device)\n",
    "\n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_data_loader:\n",
    "    \n",
    "    images = images.to(device)\n",
    "    outputs = model(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.to(device)-1).sum()\n",
    "    \n",
    "print('Accuracy of test images: %f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec2bd5-6592-493c-b47f-88d834fde95f",
   "metadata": {},
   "source": [
    "# GARBAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932f170-89db-4555-aac4-fa85b3f90732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(5)):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(train_data_loader):\n",
    "\t\t# send the input to the device\n",
    "\t\t#(x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\t# calculate the gradients\n",
    "\t\tloss.backward()\n",
    "\t\t# check if we are updating the model parameters and if so\n",
    "\t\t# update them, and zero out the previously accumulated gradients\n",
    "\t\tif (i + 2) % 2 == 0:\n",
    "\t\t\topt.step()\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\n",
    "    \t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tmodel.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in test_data_loader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t#(x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = model(x)\n",
    "\t\t\ttotalValLoss += lossFunc(pred, y)\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\tvalCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\t\ttorch.float).sum().item()\n",
    "\n",
    "    \t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# calculate the training and validation accuracy\n",
    "\ttrainCorrect = trainCorrect / len(trainDS)\n",
    "\tvalCorrect = valCorrect / len(valDS)\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(trainCorrect)\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\tH[\"val_acc\"].append(valCorrect)\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, 5))\n",
    "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, trainCorrect))\n",
    "\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\".format(\n",
    "\t\tavgValLoss, valCorrect))\n",
    "\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))\n",
    "# plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "# plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "# plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "# plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "# plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(config.WARMUP_PLOT)\n",
    "# serialize the model to disk\n",
    "torch.save(model, config.WARMUP_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f22e0-565c-4aa5-8526-b97fcb30e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in train_data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss / dataset_sizes\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            # deep copy the model\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f227e47-2ac9-4ff3-95a2-25054dd201db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (InceptionOutputs, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_ft \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 26\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (InceptionOutputs, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
