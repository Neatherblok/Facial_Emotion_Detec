{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d305aca8",
   "metadata": {},
   "source": [
    "To be up-to-date on the most current version of this code. Check out our GitHub repository: https://github.com/Neatherblok/Facial_Emotion_Detec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291d0668-017f-4cb2-a6c5-bc1d010f287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install --upgrade --force-reinstall huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f64a5-f521-4b48-8625-0c94fe75e531",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec7944f-90dd-443a-aa4e-6f95db701bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms, Lambda, Resize\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''\n",
    "Creates a class object CustomDataLoader\n",
    "************\n",
    "__init__\n",
    "  This function initializes a CustomDataLoader object.\n",
    "  It loads a part of the ImageNet-1K dataset specified by the user.\n",
    "  Options for dataset_type are 'train' and 'test'.\n",
    "\n",
    "  Args:\n",
    "    data_path (str): Path to the dataset.\n",
    "    batch_size (int): Batch size for data loading.\n",
    "    dataset_type (str): Specifies whether the dataset is for training or testing.\n",
    "\n",
    "************\n",
    "__getitem__\n",
    "  This function returns two variables containing each a variable present \n",
    "  in the original ImageNet-1K dataset.\n",
    "  Before returning these variables, it splits up and transforms the image \n",
    "  into a fixed resolution of 48 by 48 pixels.\n",
    "  \n",
    "  Returns:\n",
    "    data: The transformed image data.\n",
    "    label: The label corresponding to the image.\n",
    "************\n",
    "__len__\n",
    "  Returns: the length of the loaded dataset.\n",
    "'''\n",
    "\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, data_path, batch_size, dataset_type):\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_type = dataset_type\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((48, 48)),  # Resize to size 48x48\n",
    "            transforms.Grayscale(),  # Convert all images to grayscale format\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485], std=[0.2295])\n",
    "        ])\n",
    "\n",
    "        self.dataset = torchvision.datasets.ImageFolder(root=f\"{self.data_path}/{dataset_type}\", transform=self.transform)\n",
    "\n",
    "        if self.dataset_type == 'train':\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle = False\n",
    "\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=shuffle, num_workers=4)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479b95d1-796a-47e7-92d9-e769a2712220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify image dataset path:\n",
    "DATA_PATH = \"../FER2013_Data\"\n",
    "\n",
    "# Specify image batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load and execute transformations on datasets\n",
    "train_data_loader = CustomDataLoader(DATA_PATH, BATCH_SIZE, dataset_type='train').data_loader\n",
    "test_data_loader = CustomDataLoader(DATA_PATH, BATCH_SIZE, dataset_type='test').data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adce8767-aca4-4231-83aa-6110a1043e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Loader:\n",
      "Batch Index: 0\n",
      "Inputs Shape: torch.Size([32, 1, 48, 48])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([6, 3, 0, 4, 5])\n",
      "Batch Index: 1\n",
      "Inputs Shape: torch.Size([32, 1, 48, 48])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([2, 0, 4, 6, 4])\n",
      "Batch Index: 2\n",
      "Inputs Shape: torch.Size([32, 1, 48, 48])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([5, 3, 3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data Loader:\")\n",
    "for batch_idx, (inputs, labels) in enumerate(train_data_loader):\n",
    "    print(\"Batch Index:\", batch_idx)\n",
    "    print(\"Inputs Shape:\", inputs.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    # Print the first few labels in the batch\n",
    "    print(\"Labels:\", labels[:5])\n",
    "    # Break after printing a few batches\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22fccde-2444-4c7d-9dcf-9f58ad769dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels associated with the dataset\n",
    "class_labels = train_data_loader.dataset.classes\n",
    "\n",
    "# Print out the class labels\n",
    "print(\"Class Labels:\", class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0140d2a2-e9de-419d-b83c-cbaf1306cdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train data loader: 28044\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of images in the train data loader\n",
    "total_train_images = len(train_data_loader.dataset)\n",
    "\n",
    "# Print out the total number of images\n",
    "print(\"Total number of images in train data loader:\", total_train_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
