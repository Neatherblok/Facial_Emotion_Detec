{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc4cf86-37fc-46e9-b0e9-760087384de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\grees\\documents\\facial_emotion_detec\\.env\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\grees\\documents\\facial_emotion_detec\\.env\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# Specify where to find the data preparation class\n",
    "sys.path.append('../../Data_Preparation')\n",
    "from Preparation import CustomDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b05fb3-e436-477f-b540-2be749ea7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 training data (ImageNet) properties\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "DIMENSIONS = 3\n",
    "#SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3450901f-b30e-4955-bd91-5ce9622da748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello\n",
      "Train Data Loader:\n",
      "Batch Index: 0\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([3, 4, 3, 2, 0])\n",
      "Batch Index: 1\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([2, 6, 3, 6, 5])\n",
      "Batch Index: 2\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Labels: tensor([3, 4, 2, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CustomDataLoader class for training\n",
    "train_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=32, dataset_type=\"train\", mean=MEAN, std=STD, dimensions=DIMENSIONS).data_loader\n",
    "test_data_loader = CustomDataLoader(data_path=\"../../FER2013_Data\", batch_size=32, dataset_type=\"test\", mean=MEAN, std=STD, dimensions=DIMENSIONS).data_loader\n",
    "\n",
    "# Confirm correct data load\n",
    "print(\"Train Data Loader:\")\n",
    "for batch_idx, (inputs, labels) in enumerate(train_data_loader):\n",
    "    print(\"Batch Index:\", batch_idx)\n",
    "    print(\"Inputs Shape:\", inputs.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    # Print the first few labels in the batch\n",
    "    print(\"Labels:\", labels[:5])\n",
    "    # Break after printing a few batches\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e5b086-b1fe-410e-a99b-84c20e5d8de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grees\\Documents\\Facial_Emotion_Detec\\.env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\grees\\Documents\\Facial_Emotion_Detec\\.env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "vgg19 = models.vgg19(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781713ff-a295-4017-8553-71d17397e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "dataloaders = {'train':train_data_loader, 'val':test_data_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2accc-42c3-44bf-82e5-b7a272dbc968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Epoch [1/10], lter [5/876] Loss: 2.0313\n",
      "Epoch [1/10], lter [10/876] Loss: 1.8300\n",
      "Epoch [1/10], lter [15/876] Loss: 1.7613\n",
      "Epoch [1/10], lter [20/876] Loss: 1.7250\n",
      "Epoch [1/10], lter [25/876] Loss: 1.8724\n",
      "Epoch [1/10], lter [30/876] Loss: 1.6033\n",
      "Epoch [1/10], lter [35/876] Loss: 1.6282\n",
      "Epoch [1/10], lter [40/876] Loss: 1.8389\n",
      "Epoch [1/10], lter [45/876] Loss: 1.7978\n",
      "Epoch [1/10], lter [50/876] Loss: 1.6006\n",
      "Epoch [1/10], lter [55/876] Loss: 1.5941\n",
      "Epoch [1/10], lter [60/876] Loss: 1.6591\n",
      "Epoch [1/10], lter [65/876] Loss: 1.6199\n",
      "Epoch [1/10], lter [70/876] Loss: 1.4504\n",
      "Epoch [1/10], lter [75/876] Loss: 1.6586\n",
      "Epoch [1/10], lter [80/876] Loss: 1.9720\n",
      "Epoch [1/10], lter [85/876] Loss: 1.4589\n",
      "Epoch [1/10], lter [90/876] Loss: 1.8038\n",
      "Epoch [1/10], lter [95/876] Loss: 1.7015\n",
      "Epoch [1/10], lter [100/876] Loss: 1.4595\n",
      "Epoch [1/10], lter [105/876] Loss: 1.6611\n",
      "Epoch [1/10], lter [110/876] Loss: 1.5001\n",
      "Epoch [1/10], lter [115/876] Loss: 1.5919\n",
      "Epoch [1/10], lter [120/876] Loss: 1.6013\n",
      "Epoch [1/10], lter [125/876] Loss: 1.6887\n",
      "Epoch [1/10], lter [130/876] Loss: 1.5947\n",
      "Epoch [1/10], lter [135/876] Loss: 1.5791\n",
      "Epoch [1/10], lter [140/876] Loss: 1.5944\n",
      "Epoch [1/10], lter [145/876] Loss: 1.6456\n",
      "Epoch [1/10], lter [150/876] Loss: 1.7092\n",
      "Epoch [1/10], lter [155/876] Loss: 1.5580\n",
      "Epoch [1/10], lter [160/876] Loss: 1.3699\n",
      "Epoch [1/10], lter [165/876] Loss: 1.4180\n",
      "Epoch [1/10], lter [170/876] Loss: 2.0131\n",
      "Epoch [1/10], lter [175/876] Loss: 1.4177\n",
      "Epoch [1/10], lter [180/876] Loss: 1.5987\n",
      "Epoch [1/10], lter [185/876] Loss: 1.4601\n",
      "Epoch [1/10], lter [190/876] Loss: 1.6765\n",
      "Epoch [1/10], lter [195/876] Loss: 1.6660\n",
      "Epoch [1/10], lter [200/876] Loss: 1.6894\n",
      "Epoch [1/10], lter [205/876] Loss: 1.8671\n",
      "Epoch [1/10], lter [210/876] Loss: 1.8092\n",
      "Epoch [1/10], lter [215/876] Loss: 1.8748\n",
      "Epoch [1/10], lter [220/876] Loss: 1.4908\n",
      "Epoch [1/10], lter [225/876] Loss: 1.4947\n",
      "Epoch [1/10], lter [230/876] Loss: 1.6661\n",
      "Epoch [1/10], lter [235/876] Loss: 1.7782\n",
      "Epoch [1/10], lter [240/876] Loss: 1.6895\n",
      "Epoch [1/10], lter [245/876] Loss: 1.6098\n",
      "Epoch [1/10], lter [250/876] Loss: 1.6231\n",
      "Epoch [1/10], lter [255/876] Loss: 1.5799\n",
      "Epoch [1/10], lter [260/876] Loss: 1.4880\n",
      "Epoch [1/10], lter [265/876] Loss: 1.6612\n",
      "Epoch [1/10], lter [270/876] Loss: 1.6110\n",
      "Epoch [1/10], lter [275/876] Loss: 1.5013\n",
      "Epoch [1/10], lter [280/876] Loss: 1.5334\n",
      "Epoch [1/10], lter [285/876] Loss: 1.6832\n",
      "Epoch [1/10], lter [290/876] Loss: 1.4079\n",
      "Epoch [1/10], lter [295/876] Loss: 1.8051\n",
      "Epoch [1/10], lter [300/876] Loss: 1.7267\n",
      "Epoch [1/10], lter [305/876] Loss: 1.7636\n",
      "Epoch [1/10], lter [310/876] Loss: 1.3912\n",
      "Epoch [1/10], lter [315/876] Loss: 1.6875\n",
      "Epoch [1/10], lter [320/876] Loss: 1.6710\n",
      "Epoch [1/10], lter [325/876] Loss: 1.8007\n",
      "Epoch [1/10], lter [330/876] Loss: 1.6871\n",
      "Epoch [1/10], lter [335/876] Loss: 1.7216\n",
      "Epoch [1/10], lter [340/876] Loss: 1.4751\n",
      "Epoch [1/10], lter [345/876] Loss: 1.5832\n",
      "Epoch [1/10], lter [350/876] Loss: 2.0402\n",
      "Epoch [1/10], lter [355/876] Loss: 1.2283\n",
      "Epoch [1/10], lter [360/876] Loss: 1.7369\n",
      "Epoch [1/10], lter [365/876] Loss: 1.4768\n",
      "Epoch [1/10], lter [370/876] Loss: 1.4913\n",
      "Epoch [1/10], lter [375/876] Loss: 1.6783\n",
      "Epoch [1/10], lter [380/876] Loss: 1.6061\n",
      "Epoch [1/10], lter [385/876] Loss: 1.7522\n",
      "Epoch [1/10], lter [390/876] Loss: 1.7867\n",
      "Epoch [1/10], lter [395/876] Loss: 1.3489\n",
      "Epoch [1/10], lter [400/876] Loss: 1.5172\n",
      "Epoch [1/10], lter [405/876] Loss: 1.3936\n",
      "Epoch [1/10], lter [410/876] Loss: 1.7567\n",
      "Epoch [1/10], lter [415/876] Loss: 1.8199\n",
      "Epoch [1/10], lter [420/876] Loss: 1.6738\n",
      "Epoch [1/10], lter [425/876] Loss: 1.5236\n",
      "Epoch [1/10], lter [430/876] Loss: 1.7778\n",
      "Epoch [1/10], lter [435/876] Loss: 1.9361\n",
      "Epoch [1/10], lter [440/876] Loss: 1.7739\n",
      "Epoch [1/10], lter [445/876] Loss: 1.4180\n",
      "Epoch [1/10], lter [450/876] Loss: 1.5985\n",
      "Epoch [1/10], lter [455/876] Loss: 1.7373\n",
      "Epoch [1/10], lter [460/876] Loss: 1.8074\n",
      "Epoch [1/10], lter [465/876] Loss: 1.6552\n",
      "Epoch [1/10], lter [470/876] Loss: 1.4983\n",
      "Epoch [1/10], lter [475/876] Loss: 1.3965\n",
      "Epoch [1/10], lter [480/876] Loss: 1.4630\n",
      "Epoch [1/10], lter [485/876] Loss: 1.5726\n",
      "Epoch [1/10], lter [490/876] Loss: 1.8019\n",
      "Epoch [1/10], lter [495/876] Loss: 1.5424\n",
      "Epoch [1/10], lter [500/876] Loss: 1.5420\n",
      "Epoch [1/10], lter [505/876] Loss: 1.5581\n",
      "Epoch [1/10], lter [510/876] Loss: 1.8965\n",
      "Epoch [1/10], lter [515/876] Loss: 1.4332\n",
      "Epoch [1/10], lter [520/876] Loss: 1.7612\n",
      "Epoch [1/10], lter [525/876] Loss: 1.5004\n",
      "Epoch [1/10], lter [530/876] Loss: 1.8357\n",
      "Epoch [1/10], lter [535/876] Loss: 1.5731\n",
      "Epoch [1/10], lter [540/876] Loss: 1.6096\n",
      "Epoch [1/10], lter [545/876] Loss: 1.6664\n",
      "Epoch [1/10], lter [550/876] Loss: 1.4179\n",
      "Epoch [1/10], lter [555/876] Loss: 1.4600\n",
      "Epoch [1/10], lter [560/876] Loss: 1.6865\n",
      "Epoch [1/10], lter [565/876] Loss: 1.5824\n",
      "Epoch [1/10], lter [570/876] Loss: 1.6842\n",
      "Epoch [1/10], lter [575/876] Loss: 1.5467\n",
      "Epoch [1/10], lter [580/876] Loss: 1.7082\n",
      "Epoch [1/10], lter [585/876] Loss: 1.5775\n",
      "Epoch [1/10], lter [590/876] Loss: 1.4669\n",
      "Epoch [1/10], lter [595/876] Loss: 1.4325\n",
      "Epoch [1/10], lter [600/876] Loss: 1.8144\n",
      "Epoch [1/10], lter [605/876] Loss: 1.7063\n",
      "Epoch [1/10], lter [610/876] Loss: 1.5527\n",
      "Epoch [1/10], lter [615/876] Loss: 1.5307\n",
      "Epoch [1/10], lter [620/876] Loss: 1.6848\n",
      "Epoch [1/10], lter [625/876] Loss: 1.3864\n",
      "Epoch [1/10], lter [630/876] Loss: 1.5071\n",
      "Epoch [1/10], lter [635/876] Loss: 1.6103\n",
      "Epoch [1/10], lter [640/876] Loss: 1.8382\n",
      "Epoch [1/10], lter [645/876] Loss: 1.8778\n",
      "Epoch [1/10], lter [650/876] Loss: 1.9144\n",
      "Epoch [1/10], lter [655/876] Loss: 1.7585\n",
      "Epoch [1/10], lter [660/876] Loss: 1.6647\n",
      "Epoch [1/10], lter [665/876] Loss: 1.5095\n",
      "Epoch [1/10], lter [670/876] Loss: 1.9069\n",
      "Epoch [1/10], lter [675/876] Loss: 1.7170\n",
      "Epoch [1/10], lter [680/876] Loss: 1.3962\n",
      "Epoch [1/10], lter [685/876] Loss: 1.5153\n",
      "Epoch [1/10], lter [690/876] Loss: 1.3837\n",
      "Epoch [1/10], lter [695/876] Loss: 1.7475\n",
      "Epoch [1/10], lter [700/876] Loss: 1.7357\n",
      "Epoch [1/10], lter [705/876] Loss: 1.8226\n",
      "Epoch [1/10], lter [710/876] Loss: 1.5115\n",
      "Epoch [1/10], lter [715/876] Loss: 1.9344\n",
      "Epoch [1/10], lter [720/876] Loss: 1.6729\n",
      "Epoch [1/10], lter [725/876] Loss: 1.6797\n",
      "Epoch [1/10], lter [730/876] Loss: 1.9818\n",
      "Epoch [1/10], lter [735/876] Loss: 1.6991\n",
      "Epoch [1/10], lter [740/876] Loss: 1.6811\n",
      "Epoch [1/10], lter [745/876] Loss: 1.3596\n",
      "Epoch [1/10], lter [750/876] Loss: 1.6558\n",
      "Epoch [1/10], lter [755/876] Loss: 1.5740\n",
      "Epoch [1/10], lter [760/876] Loss: 1.4904\n",
      "Epoch [1/10], lter [765/876] Loss: 1.3601\n",
      "Epoch [1/10], lter [770/876] Loss: 1.5313\n",
      "Epoch [1/10], lter [775/876] Loss: 1.6010\n",
      "Epoch [1/10], lter [780/876] Loss: 1.8695\n",
      "Epoch [1/10], lter [785/876] Loss: 1.8750\n",
      "Epoch [1/10], lter [790/876] Loss: 1.6983\n",
      "Epoch [1/10], lter [795/876] Loss: 1.8822\n",
      "Epoch [1/10], lter [800/876] Loss: 1.4549\n",
      "Epoch [1/10], lter [805/876] Loss: 1.5917\n",
      "Epoch [1/10], lter [810/876] Loss: 1.6839\n",
      "Epoch [1/10], lter [815/876] Loss: 1.6158\n",
      "Epoch [1/10], lter [820/876] Loss: 1.3125\n",
      "Epoch [1/10], lter [825/876] Loss: 1.6464\n",
      "Epoch [1/10], lter [830/876] Loss: 1.4471\n",
      "Epoch [1/10], lter [835/876] Loss: 1.7650\n",
      "Epoch [1/10], lter [840/876] Loss: 1.6845\n",
      "Epoch [1/10], lter [845/876] Loss: 1.6207\n",
      "Epoch [1/10], lter [850/876] Loss: 1.6627\n",
      "Epoch [1/10], lter [855/876] Loss: 1.6808\n",
      "Epoch [1/10], lter [860/876] Loss: 1.7601\n",
      "Epoch [1/10], lter [865/876] Loss: 1.6796\n",
      "Epoch [1/10], lter [870/876] Loss: 1.7993\n",
      "Epoch [1/10], lter [875/876] Loss: 1.4174\n",
      "train Loss: 52.8532 | Acc: 11.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [4:27:20<40:06:08, 16040.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 47.6075 | Acc: 13.9644\n",
      "Epoch 1/9\n",
      "----------\n",
      "Epoch [2/10], lter [5/876] Loss: 1.7264\n",
      "Epoch [2/10], lter [10/876] Loss: 1.5972\n",
      "Epoch [2/10], lter [15/876] Loss: 1.5614\n",
      "Epoch [2/10], lter [20/876] Loss: 1.5161\n",
      "Epoch [2/10], lter [25/876] Loss: 1.7202\n",
      "Epoch [2/10], lter [30/876] Loss: 1.6482\n",
      "Epoch [2/10], lter [35/876] Loss: 1.8560\n",
      "Epoch [2/10], lter [40/876] Loss: 1.6290\n",
      "Epoch [2/10], lter [45/876] Loss: 1.7729\n",
      "Epoch [2/10], lter [50/876] Loss: 1.4117\n",
      "Epoch [2/10], lter [55/876] Loss: 1.2398\n",
      "Epoch [2/10], lter [60/876] Loss: 2.1684\n",
      "Epoch [2/10], lter [65/876] Loss: 1.4664\n",
      "Epoch [2/10], lter [70/876] Loss: 1.7224\n",
      "Epoch [2/10], lter [75/876] Loss: 1.6248\n",
      "Epoch [2/10], lter [80/876] Loss: 1.9701\n",
      "Epoch [2/10], lter [85/876] Loss: 1.6965\n",
      "Epoch [2/10], lter [90/876] Loss: 1.4579\n",
      "Epoch [2/10], lter [95/876] Loss: 1.4958\n",
      "Epoch [2/10], lter [100/876] Loss: 1.3143\n",
      "Epoch [2/10], lter [105/876] Loss: 1.5987\n",
      "Epoch [2/10], lter [110/876] Loss: 1.6043\n",
      "Epoch [2/10], lter [115/876] Loss: 1.9991\n",
      "Epoch [2/10], lter [120/876] Loss: 1.5536\n",
      "Epoch [2/10], lter [125/876] Loss: 1.3707\n",
      "Epoch [2/10], lter [130/876] Loss: 1.6744\n",
      "Epoch [2/10], lter [135/876] Loss: 1.4812\n",
      "Epoch [2/10], lter [140/876] Loss: 1.5305\n",
      "Epoch [2/10], lter [145/876] Loss: 1.4336\n",
      "Epoch [2/10], lter [150/876] Loss: 1.8743\n",
      "Epoch [2/10], lter [155/876] Loss: 1.4319\n",
      "Epoch [2/10], lter [160/876] Loss: 1.6796\n",
      "Epoch [2/10], lter [165/876] Loss: 1.6579\n",
      "Epoch [2/10], lter [170/876] Loss: 1.9395\n",
      "Epoch [2/10], lter [175/876] Loss: 1.8892\n",
      "Epoch [2/10], lter [180/876] Loss: 1.6841\n",
      "Epoch [2/10], lter [185/876] Loss: 1.5691\n",
      "Epoch [2/10], lter [190/876] Loss: 1.5032\n",
      "Epoch [2/10], lter [195/876] Loss: 1.3951\n",
      "Epoch [2/10], lter [200/876] Loss: 1.6646\n",
      "Epoch [2/10], lter [205/876] Loss: 1.2489\n",
      "Epoch [2/10], lter [210/876] Loss: 1.5941\n",
      "Epoch [2/10], lter [215/876] Loss: 1.7262\n",
      "Epoch [2/10], lter [220/876] Loss: 1.5902\n",
      "Epoch [2/10], lter [225/876] Loss: 1.4032\n",
      "Epoch [2/10], lter [230/876] Loss: 1.5840\n",
      "Epoch [2/10], lter [235/876] Loss: 1.9962\n",
      "Epoch [2/10], lter [240/876] Loss: 1.6685\n",
      "Epoch [2/10], lter [245/876] Loss: 1.4089\n",
      "Epoch [2/10], lter [250/876] Loss: 1.4914\n",
      "Epoch [2/10], lter [255/876] Loss: 1.4781\n",
      "Epoch [2/10], lter [260/876] Loss: 1.7122\n",
      "Epoch [2/10], lter [265/876] Loss: 1.4363\n",
      "Epoch [2/10], lter [270/876] Loss: 1.5474\n",
      "Epoch [2/10], lter [275/876] Loss: 1.9403\n",
      "Epoch [2/10], lter [280/876] Loss: 1.6989\n",
      "Epoch [2/10], lter [285/876] Loss: 1.6803\n",
      "Epoch [2/10], lter [290/876] Loss: 1.5991\n",
      "Epoch [2/10], lter [295/876] Loss: 1.4878\n",
      "Epoch [2/10], lter [300/876] Loss: 1.4703\n",
      "Epoch [2/10], lter [305/876] Loss: 1.5113\n",
      "Epoch [2/10], lter [310/876] Loss: 1.8552\n",
      "Epoch [2/10], lter [315/876] Loss: 1.6252\n",
      "Epoch [2/10], lter [320/876] Loss: 1.6357\n",
      "Epoch [2/10], lter [325/876] Loss: 1.6149\n",
      "Epoch [2/10], lter [330/876] Loss: 1.5268\n",
      "Epoch [2/10], lter [335/876] Loss: 1.8809\n",
      "Epoch [2/10], lter [340/876] Loss: 1.2365\n",
      "Epoch [2/10], lter [345/876] Loss: 1.6379\n",
      "Epoch [2/10], lter [350/876] Loss: 1.4552\n",
      "Epoch [2/10], lter [355/876] Loss: 1.5696\n",
      "Epoch [2/10], lter [360/876] Loss: 1.6433\n",
      "Epoch [2/10], lter [365/876] Loss: 1.9186\n",
      "Epoch [2/10], lter [370/876] Loss: 1.5714\n",
      "Epoch [2/10], lter [375/876] Loss: 1.7107\n",
      "Epoch [2/10], lter [380/876] Loss: 1.2145\n",
      "Epoch [2/10], lter [385/876] Loss: 1.4306\n",
      "Epoch [2/10], lter [390/876] Loss: 1.7716\n",
      "Epoch [2/10], lter [395/876] Loss: 1.7576\n",
      "Epoch [2/10], lter [400/876] Loss: 1.4231\n",
      "Epoch [2/10], lter [405/876] Loss: 1.5153\n",
      "Epoch [2/10], lter [410/876] Loss: 1.9008\n",
      "Epoch [2/10], lter [415/876] Loss: 1.6369\n",
      "Epoch [2/10], lter [420/876] Loss: 1.4691\n",
      "Epoch [2/10], lter [425/876] Loss: 1.5004\n",
      "Epoch [2/10], lter [430/876] Loss: 1.9481\n",
      "Epoch [2/10], lter [435/876] Loss: 1.4550\n",
      "Epoch [2/10], lter [440/876] Loss: 1.7815\n",
      "Epoch [2/10], lter [445/876] Loss: 1.4340\n",
      "Epoch [2/10], lter [450/876] Loss: 1.5278\n",
      "Epoch [2/10], lter [455/876] Loss: 1.6397\n",
      "Epoch [2/10], lter [460/876] Loss: 1.4420\n",
      "Epoch [2/10], lter [465/876] Loss: 1.6535\n",
      "Epoch [2/10], lter [470/876] Loss: 2.1245\n",
      "Epoch [2/10], lter [475/876] Loss: 1.5944\n",
      "Epoch [2/10], lter [480/876] Loss: 1.7215\n",
      "Epoch [2/10], lter [485/876] Loss: 1.7079\n",
      "Epoch [2/10], lter [490/876] Loss: 1.3064\n",
      "Epoch [2/10], lter [495/876] Loss: 1.6332\n",
      "Epoch [2/10], lter [500/876] Loss: 1.7596\n",
      "Epoch [2/10], lter [505/876] Loss: 1.9171\n",
      "Epoch [2/10], lter [510/876] Loss: 1.5167\n",
      "Epoch [2/10], lter [515/876] Loss: 1.3684\n",
      "Epoch [2/10], lter [520/876] Loss: 1.6114\n",
      "Epoch [2/10], lter [525/876] Loss: 1.3674\n",
      "Epoch [2/10], lter [530/876] Loss: 1.9966\n",
      "Epoch [2/10], lter [535/876] Loss: 2.0064\n",
      "Epoch [2/10], lter [540/876] Loss: 1.6122\n",
      "Epoch [2/10], lter [545/876] Loss: 1.7434\n",
      "Epoch [2/10], lter [550/876] Loss: 1.5559\n",
      "Epoch [2/10], lter [555/876] Loss: 1.5866\n",
      "Epoch [2/10], lter [560/876] Loss: 1.4169\n",
      "Epoch [2/10], lter [565/876] Loss: 1.6439\n",
      "Epoch [2/10], lter [570/876] Loss: 1.5087\n",
      "Epoch [2/10], lter [575/876] Loss: 1.7229\n",
      "Epoch [2/10], lter [580/876] Loss: 1.9117\n",
      "Epoch [2/10], lter [585/876] Loss: 1.6227\n",
      "Epoch [2/10], lter [590/876] Loss: 1.4704\n",
      "Epoch [2/10], lter [595/876] Loss: 1.5672\n",
      "Epoch [2/10], lter [600/876] Loss: 1.7920\n",
      "Epoch [2/10], lter [605/876] Loss: 1.8395\n",
      "Epoch [2/10], lter [610/876] Loss: 1.5267\n",
      "Epoch [2/10], lter [615/876] Loss: 1.5649\n",
      "Epoch [2/10], lter [620/876] Loss: 1.9694\n",
      "Epoch [2/10], lter [625/876] Loss: 1.6717\n",
      "Epoch [2/10], lter [630/876] Loss: 1.6255\n",
      "Epoch [2/10], lter [635/876] Loss: 1.5176\n",
      "Epoch [2/10], lter [640/876] Loss: 1.6488\n",
      "Epoch [2/10], lter [645/876] Loss: 1.8234\n",
      "Epoch [2/10], lter [650/876] Loss: 1.6712\n",
      "Epoch [2/10], lter [655/876] Loss: 1.7726\n",
      "Epoch [2/10], lter [660/876] Loss: 1.5795\n",
      "Epoch [2/10], lter [665/876] Loss: 1.5304\n",
      "Epoch [2/10], lter [670/876] Loss: 1.4002\n",
      "Epoch [2/10], lter [675/876] Loss: 1.7507\n",
      "Epoch [2/10], lter [680/876] Loss: 1.6433\n",
      "Epoch [2/10], lter [685/876] Loss: 1.4629\n",
      "Epoch [2/10], lter [690/876] Loss: 1.6319\n",
      "Epoch [2/10], lter [695/876] Loss: 1.8844\n",
      "Epoch [2/10], lter [700/876] Loss: 1.9090\n",
      "Epoch [2/10], lter [705/876] Loss: 1.5752\n",
      "Epoch [2/10], lter [710/876] Loss: 1.5745\n",
      "Epoch [2/10], lter [715/876] Loss: 1.4604\n",
      "Epoch [2/10], lter [720/876] Loss: 1.5864\n",
      "Epoch [2/10], lter [725/876] Loss: 1.9829\n",
      "Epoch [2/10], lter [730/876] Loss: 1.5517\n",
      "Epoch [2/10], lter [735/876] Loss: 1.5987\n",
      "Epoch [2/10], lter [740/876] Loss: 1.8533\n",
      "Epoch [2/10], lter [745/876] Loss: 1.3939\n",
      "Epoch [2/10], lter [750/876] Loss: 1.5707\n",
      "Epoch [2/10], lter [755/876] Loss: 1.7193\n",
      "Epoch [2/10], lter [760/876] Loss: 1.5293\n",
      "Epoch [2/10], lter [765/876] Loss: 1.6370\n",
      "Epoch [2/10], lter [770/876] Loss: 1.4406\n",
      "Epoch [2/10], lter [775/876] Loss: 1.6790\n",
      "Epoch [2/10], lter [780/876] Loss: 1.7358\n",
      "Epoch [2/10], lter [785/876] Loss: 1.4166\n",
      "Epoch [2/10], lter [790/876] Loss: 1.4505\n",
      "Epoch [2/10], lter [795/876] Loss: 1.5859\n",
      "Epoch [2/10], lter [800/876] Loss: 1.6715\n",
      "Epoch [2/10], lter [805/876] Loss: 1.8666\n",
      "Epoch [2/10], lter [810/876] Loss: 1.7400\n",
      "Epoch [2/10], lter [815/876] Loss: 1.5119\n",
      "Epoch [2/10], lter [820/876] Loss: 1.7633\n",
      "Epoch [2/10], lter [825/876] Loss: 1.4755\n",
      "Epoch [2/10], lter [830/876] Loss: 1.5447\n",
      "Epoch [2/10], lter [835/876] Loss: 1.8622\n",
      "Epoch [2/10], lter [840/876] Loss: 1.2635\n",
      "Epoch [2/10], lter [845/876] Loss: 1.7300\n",
      "Epoch [2/10], lter [850/876] Loss: 1.7622\n",
      "Epoch [2/10], lter [855/876] Loss: 1.6255\n",
      "Epoch [2/10], lter [860/876] Loss: 1.7736\n",
      "Epoch [2/10], lter [865/876] Loss: 1.8029\n",
      "Epoch [2/10], lter [870/876] Loss: 1.8684\n",
      "Epoch [2/10], lter [875/876] Loss: 1.6352\n",
      "train Loss: 51.8168 | Acc: 12.2098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [8:55:11<35:41:05, 16058.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 47.7350 | Acc: 13.9822\n",
      "Epoch 2/9\n",
      "----------\n",
      "Epoch [3/10], lter [5/876] Loss: 1.4781\n",
      "Epoch [3/10], lter [10/876] Loss: 1.5630\n",
      "Epoch [3/10], lter [15/876] Loss: 1.2099\n",
      "Epoch [3/10], lter [20/876] Loss: 1.3290\n",
      "Epoch [3/10], lter [25/876] Loss: 1.9095\n",
      "Epoch [3/10], lter [30/876] Loss: 1.5042\n",
      "Epoch [3/10], lter [35/876] Loss: 1.5836\n",
      "Epoch [3/10], lter [40/876] Loss: 1.5146\n",
      "Epoch [3/10], lter [45/876] Loss: 1.6388\n",
      "Epoch [3/10], lter [50/876] Loss: 1.8807\n",
      "Epoch [3/10], lter [55/876] Loss: 1.7844\n",
      "Epoch [3/10], lter [60/876] Loss: 1.6065\n",
      "Epoch [3/10], lter [65/876] Loss: 1.4541\n",
      "Epoch [3/10], lter [70/876] Loss: 1.7482\n",
      "Epoch [3/10], lter [75/876] Loss: 1.4025\n",
      "Epoch [3/10], lter [80/876] Loss: 1.4136\n",
      "Epoch [3/10], lter [85/876] Loss: 1.6705\n",
      "Epoch [3/10], lter [90/876] Loss: 1.4838\n",
      "Epoch [3/10], lter [95/876] Loss: 1.8115\n",
      "Epoch [3/10], lter [100/876] Loss: 1.9058\n",
      "Epoch [3/10], lter [105/876] Loss: 1.6410\n",
      "Epoch [3/10], lter [110/876] Loss: 1.5350\n",
      "Epoch [3/10], lter [115/876] Loss: 1.3696\n",
      "Epoch [3/10], lter [120/876] Loss: 1.7303\n",
      "Epoch [3/10], lter [125/876] Loss: 1.7387\n",
      "Epoch [3/10], lter [130/876] Loss: 1.5680\n",
      "Epoch [3/10], lter [135/876] Loss: 1.5067\n",
      "Epoch [3/10], lter [140/876] Loss: 1.8824\n",
      "Epoch [3/10], lter [145/876] Loss: 1.5951\n",
      "Epoch [3/10], lter [150/876] Loss: 1.8768\n",
      "Epoch [3/10], lter [155/876] Loss: 1.4962\n",
      "Epoch [3/10], lter [160/876] Loss: 1.8003\n",
      "Epoch [3/10], lter [165/876] Loss: 1.9731\n",
      "Epoch [3/10], lter [170/876] Loss: 1.3504\n",
      "Epoch [3/10], lter [175/876] Loss: 1.4217\n",
      "Epoch [3/10], lter [180/876] Loss: 1.4984\n",
      "Epoch [3/10], lter [185/876] Loss: 1.9499\n",
      "Epoch [3/10], lter [190/876] Loss: 1.6960\n",
      "Epoch [3/10], lter [195/876] Loss: 1.3408\n",
      "Epoch [3/10], lter [200/876] Loss: 1.5455\n",
      "Epoch [3/10], lter [205/876] Loss: 1.6516\n",
      "Epoch [3/10], lter [210/876] Loss: 1.5378\n",
      "Epoch [3/10], lter [215/876] Loss: 1.9223\n",
      "Epoch [3/10], lter [220/876] Loss: 1.6168\n",
      "Epoch [3/10], lter [225/876] Loss: 1.6202\n",
      "Epoch [3/10], lter [230/876] Loss: 1.5072\n",
      "Epoch [3/10], lter [235/876] Loss: 1.7340\n",
      "Epoch [3/10], lter [240/876] Loss: 1.3582\n",
      "Epoch [3/10], lter [245/876] Loss: 1.7084\n",
      "Epoch [3/10], lter [250/876] Loss: 1.6700\n",
      "Epoch [3/10], lter [255/876] Loss: 1.6033\n",
      "Epoch [3/10], lter [260/876] Loss: 1.6593\n",
      "Epoch [3/10], lter [265/876] Loss: 1.2996\n",
      "Epoch [3/10], lter [270/876] Loss: 1.6005\n",
      "Epoch [3/10], lter [275/876] Loss: 1.9153\n",
      "Epoch [3/10], lter [280/876] Loss: 1.6359\n",
      "Epoch [3/10], lter [285/876] Loss: 1.5535\n",
      "Epoch [3/10], lter [290/876] Loss: 1.6751\n",
      "Epoch [3/10], lter [295/876] Loss: 1.8625\n",
      "Epoch [3/10], lter [300/876] Loss: 1.7413\n",
      "Epoch [3/10], lter [305/876] Loss: 1.9334\n",
      "Epoch [3/10], lter [310/876] Loss: 1.3078\n",
      "Epoch [3/10], lter [315/876] Loss: 1.7893\n",
      "Epoch [3/10], lter [320/876] Loss: 1.5921\n",
      "Epoch [3/10], lter [325/876] Loss: 1.5115\n",
      "Epoch [3/10], lter [330/876] Loss: 1.7318\n",
      "Epoch [3/10], lter [335/876] Loss: 1.7118\n",
      "Epoch [3/10], lter [340/876] Loss: 1.7848\n",
      "Epoch [3/10], lter [345/876] Loss: 1.5202\n",
      "Epoch [3/10], lter [350/876] Loss: 1.7165\n",
      "Epoch [3/10], lter [355/876] Loss: 1.5865\n",
      "Epoch [3/10], lter [360/876] Loss: 1.5866\n",
      "Epoch [3/10], lter [365/876] Loss: 1.3312\n",
      "Epoch [3/10], lter [370/876] Loss: 1.9094\n",
      "Epoch [3/10], lter [375/876] Loss: 1.7501\n",
      "Epoch [3/10], lter [380/876] Loss: 1.5869\n",
      "Epoch [3/10], lter [385/876] Loss: 1.8539\n",
      "Epoch [3/10], lter [390/876] Loss: 1.7555\n",
      "Epoch [3/10], lter [395/876] Loss: 1.5762\n",
      "Epoch [3/10], lter [400/876] Loss: 1.5351\n",
      "Epoch [3/10], lter [405/876] Loss: 1.5913\n",
      "Epoch [3/10], lter [410/876] Loss: 1.6626\n",
      "Epoch [3/10], lter [415/876] Loss: 1.4714\n",
      "Epoch [3/10], lter [420/876] Loss: 1.6004\n",
      "Epoch [3/10], lter [425/876] Loss: 1.4891\n",
      "Epoch [3/10], lter [430/876] Loss: 1.6193\n",
      "Epoch [3/10], lter [435/876] Loss: 1.6757\n",
      "Epoch [3/10], lter [440/876] Loss: 1.2762\n",
      "Epoch [3/10], lter [445/876] Loss: 1.6721\n",
      "Epoch [3/10], lter [450/876] Loss: 1.8706\n",
      "Epoch [3/10], lter [455/876] Loss: 1.6080\n",
      "Epoch [3/10], lter [460/876] Loss: 1.6071\n",
      "Epoch [3/10], lter [465/876] Loss: 1.6533\n",
      "Epoch [3/10], lter [470/876] Loss: 1.8199\n",
      "Epoch [3/10], lter [475/876] Loss: 1.7381\n",
      "Epoch [3/10], lter [480/876] Loss: 1.6389\n",
      "Epoch [3/10], lter [485/876] Loss: 1.4302\n",
      "Epoch [3/10], lter [490/876] Loss: 1.7479\n",
      "Epoch [3/10], lter [495/876] Loss: 1.5054\n",
      "Epoch [3/10], lter [500/876] Loss: 1.5305\n",
      "Epoch [3/10], lter [505/876] Loss: 1.8162\n",
      "Epoch [3/10], lter [510/876] Loss: 1.7199\n",
      "Epoch [3/10], lter [515/876] Loss: 1.4609\n",
      "Epoch [3/10], lter [520/876] Loss: 1.7041\n",
      "Epoch [3/10], lter [525/876] Loss: 1.2733\n",
      "Epoch [3/10], lter [530/876] Loss: 1.6764\n",
      "Epoch [3/10], lter [535/876] Loss: 1.5040\n",
      "Epoch [3/10], lter [540/876] Loss: 1.5069\n",
      "Epoch [3/10], lter [545/876] Loss: 1.5708\n",
      "Epoch [3/10], lter [550/876] Loss: 1.9597\n",
      "Epoch [3/10], lter [555/876] Loss: 1.4067\n",
      "Epoch [3/10], lter [560/876] Loss: 1.7599\n",
      "Epoch [3/10], lter [565/876] Loss: 1.6389\n",
      "Epoch [3/10], lter [570/876] Loss: 1.6750\n",
      "Epoch [3/10], lter [575/876] Loss: 1.9508\n",
      "Epoch [3/10], lter [580/876] Loss: 1.6298\n",
      "Epoch [3/10], lter [585/876] Loss: 1.5340\n",
      "Epoch [3/10], lter [590/876] Loss: 1.3003\n",
      "Epoch [3/10], lter [595/876] Loss: 2.0098\n",
      "Epoch [3/10], lter [600/876] Loss: 1.8100\n",
      "Epoch [3/10], lter [605/876] Loss: 1.6094\n",
      "Epoch [3/10], lter [610/876] Loss: 1.5947\n",
      "Epoch [3/10], lter [615/876] Loss: 1.7929\n",
      "Epoch [3/10], lter [620/876] Loss: 1.0712\n",
      "Epoch [3/10], lter [625/876] Loss: 1.6322\n",
      "Epoch [3/10], lter [630/876] Loss: 1.5038\n",
      "Epoch [3/10], lter [635/876] Loss: 1.4392\n",
      "Epoch [3/10], lter [640/876] Loss: 1.3601\n",
      "Epoch [3/10], lter [645/876] Loss: 1.5763\n",
      "Epoch [3/10], lter [650/876] Loss: 1.8202\n",
      "Epoch [3/10], lter [655/876] Loss: 1.5758\n",
      "Epoch [3/10], lter [660/876] Loss: 1.6056\n",
      "Epoch [3/10], lter [665/876] Loss: 1.6843\n",
      "Epoch [3/10], lter [670/876] Loss: 1.4230\n",
      "Epoch [3/10], lter [675/876] Loss: 1.4296\n",
      "Epoch [3/10], lter [680/876] Loss: 1.9579\n",
      "Epoch [3/10], lter [685/876] Loss: 1.6031\n",
      "Epoch [3/10], lter [690/876] Loss: 1.6067\n",
      "Epoch [3/10], lter [695/876] Loss: 1.6940\n",
      "Epoch [3/10], lter [700/876] Loss: 1.6001\n",
      "Epoch [3/10], lter [705/876] Loss: 1.5317\n",
      "Epoch [3/10], lter [710/876] Loss: 1.5629\n",
      "Epoch [3/10], lter [715/876] Loss: 1.5183\n",
      "Epoch [3/10], lter [720/876] Loss: 1.4124\n",
      "Epoch [3/10], lter [725/876] Loss: 1.2540\n",
      "Epoch [3/10], lter [730/876] Loss: 1.4999\n",
      "Epoch [3/10], lter [735/876] Loss: 1.4198\n",
      "Epoch [3/10], lter [740/876] Loss: 1.5885\n",
      "Epoch [3/10], lter [745/876] Loss: 1.8670\n",
      "Epoch [3/10], lter [750/876] Loss: 1.3185\n",
      "Epoch [3/10], lter [755/876] Loss: 1.8538\n",
      "Epoch [3/10], lter [760/876] Loss: 1.4751\n",
      "Epoch [3/10], lter [765/876] Loss: 1.7891\n",
      "Epoch [3/10], lter [770/876] Loss: 1.7227\n",
      "Epoch [3/10], lter [775/876] Loss: 1.5479\n",
      "Epoch [3/10], lter [780/876] Loss: 1.2727\n",
      "Epoch [3/10], lter [785/876] Loss: 1.7346\n",
      "Epoch [3/10], lter [790/876] Loss: 1.6553\n",
      "Epoch [3/10], lter [795/876] Loss: 1.5519\n",
      "Epoch [3/10], lter [800/876] Loss: 1.6571\n",
      "Epoch [3/10], lter [805/876] Loss: 1.7320\n",
      "Epoch [3/10], lter [810/876] Loss: 1.5175\n",
      "Epoch [3/10], lter [815/876] Loss: 1.5287\n",
      "Epoch [3/10], lter [820/876] Loss: 1.5597\n",
      "Epoch [3/10], lter [825/876] Loss: 1.9072\n",
      "Epoch [3/10], lter [830/876] Loss: 1.5930\n",
      "Epoch [3/10], lter [835/876] Loss: 1.4904\n",
      "Epoch [3/10], lter [840/876] Loss: 1.5140\n",
      "Epoch [3/10], lter [845/876] Loss: 1.5733\n",
      "Epoch [3/10], lter [850/876] Loss: 1.4072\n",
      "Epoch [3/10], lter [855/876] Loss: 1.9529\n",
      "Epoch [3/10], lter [860/876] Loss: 1.8782\n",
      "Epoch [3/10], lter [865/876] Loss: 1.6146\n",
      "Epoch [3/10], lter [870/876] Loss: 1.6881\n",
      "Epoch [3/10], lter [875/876] Loss: 1.4531\n",
      "train Loss: 51.5035 | Acc: 12.4937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [13:23:50<31:16:41, 16085.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 49.2387 | Acc: 12.8089\n",
      "Epoch 3/9\n",
      "----------\n",
      "Epoch [4/10], lter [5/876] Loss: 1.4990\n",
      "Epoch [4/10], lter [10/876] Loss: 1.7006\n",
      "Epoch [4/10], lter [15/876] Loss: 1.6006\n",
      "Epoch [4/10], lter [20/876] Loss: 1.6322\n",
      "Epoch [4/10], lter [25/876] Loss: 1.4858\n",
      "Epoch [4/10], lter [30/876] Loss: 1.4972\n",
      "Epoch [4/10], lter [35/876] Loss: 1.1943\n",
      "Epoch [4/10], lter [40/876] Loss: 1.5980\n",
      "Epoch [4/10], lter [45/876] Loss: 1.5749\n",
      "Epoch [4/10], lter [50/876] Loss: 1.5018\n",
      "Epoch [4/10], lter [55/876] Loss: 1.5118\n",
      "Epoch [4/10], lter [60/876] Loss: 1.5194\n",
      "Epoch [4/10], lter [65/876] Loss: 1.5545\n",
      "Epoch [4/10], lter [70/876] Loss: 1.5049\n",
      "Epoch [4/10], lter [75/876] Loss: 1.8206\n",
      "Epoch [4/10], lter [80/876] Loss: 1.5983\n",
      "Epoch [4/10], lter [85/876] Loss: 1.3065\n",
      "Epoch [4/10], lter [90/876] Loss: 1.5022\n",
      "Epoch [4/10], lter [95/876] Loss: 1.7148\n",
      "Epoch [4/10], lter [100/876] Loss: 1.5426\n",
      "Epoch [4/10], lter [105/876] Loss: 2.1643\n",
      "Epoch [4/10], lter [110/876] Loss: 1.5880\n",
      "Epoch [4/10], lter [115/876] Loss: 1.6776\n",
      "Epoch [4/10], lter [120/876] Loss: 1.5997\n",
      "Epoch [4/10], lter [125/876] Loss: 1.8385\n",
      "Epoch [4/10], lter [130/876] Loss: 1.4411\n",
      "Epoch [4/10], lter [135/876] Loss: 1.1831\n",
      "Epoch [4/10], lter [140/876] Loss: 1.7054\n",
      "Epoch [4/10], lter [145/876] Loss: 1.9166\n",
      "Epoch [4/10], lter [150/876] Loss: 1.5458\n",
      "Epoch [4/10], lter [155/876] Loss: 1.4887\n",
      "Epoch [4/10], lter [160/876] Loss: 1.3403\n",
      "Epoch [4/10], lter [165/876] Loss: 1.5298\n",
      "Epoch [4/10], lter [170/876] Loss: 1.7919\n",
      "Epoch [4/10], lter [175/876] Loss: 1.6301\n",
      "Epoch [4/10], lter [180/876] Loss: 2.1185\n",
      "Epoch [4/10], lter [185/876] Loss: 1.5701\n",
      "Epoch [4/10], lter [190/876] Loss: 1.6970\n",
      "Epoch [4/10], lter [195/876] Loss: 1.7461\n",
      "Epoch [4/10], lter [200/876] Loss: 2.0295\n",
      "Epoch [4/10], lter [205/876] Loss: 1.5725\n",
      "Epoch [4/10], lter [210/876] Loss: 1.7359\n",
      "Epoch [4/10], lter [215/876] Loss: 1.7403\n",
      "Epoch [4/10], lter [220/876] Loss: 1.6648\n",
      "Epoch [4/10], lter [225/876] Loss: 1.6631\n",
      "Epoch [4/10], lter [230/876] Loss: 1.5344\n",
      "Epoch [4/10], lter [235/876] Loss: 1.6237\n",
      "Epoch [4/10], lter [240/876] Loss: 1.4631\n",
      "Epoch [4/10], lter [245/876] Loss: 1.6583\n",
      "Epoch [4/10], lter [250/876] Loss: 1.4720\n",
      "Epoch [4/10], lter [255/876] Loss: 1.4571\n",
      "Epoch [4/10], lter [260/876] Loss: 1.5218\n",
      "Epoch [4/10], lter [265/876] Loss: 1.4719\n",
      "Epoch [4/10], lter [270/876] Loss: 1.3388\n",
      "Epoch [4/10], lter [275/876] Loss: 1.5113\n",
      "Epoch [4/10], lter [280/876] Loss: 1.3872\n",
      "Epoch [4/10], lter [285/876] Loss: 1.7601\n",
      "Epoch [4/10], lter [290/876] Loss: 1.7920\n",
      "Epoch [4/10], lter [295/876] Loss: 1.6041\n",
      "Epoch [4/10], lter [300/876] Loss: 1.6567\n",
      "Epoch [4/10], lter [305/876] Loss: 1.5480\n",
      "Epoch [4/10], lter [310/876] Loss: 1.7272\n",
      "Epoch [4/10], lter [315/876] Loss: 1.6268\n",
      "Epoch [4/10], lter [320/876] Loss: 1.7939\n",
      "Epoch [4/10], lter [325/876] Loss: 1.1686\n",
      "Epoch [4/10], lter [330/876] Loss: 1.7389\n",
      "Epoch [4/10], lter [335/876] Loss: 1.6511\n",
      "Epoch [4/10], lter [340/876] Loss: 1.5931\n",
      "Epoch [4/10], lter [345/876] Loss: 1.6697\n",
      "Epoch [4/10], lter [350/876] Loss: 1.8783\n",
      "Epoch [4/10], lter [355/876] Loss: 1.4084\n",
      "Epoch [4/10], lter [360/876] Loss: 1.5686\n",
      "Epoch [4/10], lter [365/876] Loss: 1.6590\n",
      "Epoch [4/10], lter [370/876] Loss: 1.1919\n",
      "Epoch [4/10], lter [375/876] Loss: 1.7921\n",
      "Epoch [4/10], lter [380/876] Loss: 1.9363\n",
      "Epoch [4/10], lter [385/876] Loss: 1.5585\n",
      "Epoch [4/10], lter [390/876] Loss: 1.7567\n",
      "Epoch [4/10], lter [395/876] Loss: 1.4686\n",
      "Epoch [4/10], lter [400/876] Loss: 1.6514\n",
      "Epoch [4/10], lter [405/876] Loss: 1.6814\n",
      "Epoch [4/10], lter [410/876] Loss: 1.7883\n",
      "Epoch [4/10], lter [415/876] Loss: 1.6155\n",
      "Epoch [4/10], lter [420/876] Loss: 1.5756\n",
      "Epoch [4/10], lter [425/876] Loss: 1.6712\n",
      "Epoch [4/10], lter [430/876] Loss: 1.8142\n",
      "Epoch [4/10], lter [435/876] Loss: 1.7687\n",
      "Epoch [4/10], lter [440/876] Loss: 1.9102\n",
      "Epoch [4/10], lter [445/876] Loss: 1.7125\n",
      "Epoch [4/10], lter [450/876] Loss: 1.6383\n",
      "Epoch [4/10], lter [455/876] Loss: 1.7071\n",
      "Epoch [4/10], lter [460/876] Loss: 1.5082\n",
      "Epoch [4/10], lter [465/876] Loss: 1.6893\n",
      "Epoch [4/10], lter [470/876] Loss: 1.6597\n",
      "Epoch [4/10], lter [475/876] Loss: 1.2702\n",
      "Epoch [4/10], lter [480/876] Loss: 1.6851\n",
      "Epoch [4/10], lter [485/876] Loss: 1.4039\n",
      "Epoch [4/10], lter [490/876] Loss: 1.6967\n",
      "Epoch [4/10], lter [495/876] Loss: 1.7103\n",
      "Epoch [4/10], lter [500/876] Loss: 1.4528\n",
      "Epoch [4/10], lter [505/876] Loss: 1.8517\n",
      "Epoch [4/10], lter [510/876] Loss: 1.7563\n",
      "Epoch [4/10], lter [515/876] Loss: 1.2539\n",
      "Epoch [4/10], lter [520/876] Loss: 1.5828\n",
      "Epoch [4/10], lter [525/876] Loss: 1.7003\n",
      "Epoch [4/10], lter [530/876] Loss: 1.5194\n",
      "Epoch [4/10], lter [535/876] Loss: 1.4193\n",
      "Epoch [4/10], lter [540/876] Loss: 1.6734\n",
      "Epoch [4/10], lter [545/876] Loss: 1.4598\n",
      "Epoch [4/10], lter [550/876] Loss: 1.6765\n",
      "Epoch [4/10], lter [555/876] Loss: 1.6991\n",
      "Epoch [4/10], lter [560/876] Loss: 1.4575\n",
      "Epoch [4/10], lter [565/876] Loss: 1.5280\n",
      "Epoch [4/10], lter [570/876] Loss: 1.4070\n",
      "Epoch [4/10], lter [575/876] Loss: 1.2039\n",
      "Epoch [4/10], lter [580/876] Loss: 1.6545\n",
      "Epoch [4/10], lter [585/876] Loss: 1.0700\n",
      "Epoch [4/10], lter [590/876] Loss: 1.4410\n",
      "Epoch [4/10], lter [595/876] Loss: 1.5426\n",
      "Epoch [4/10], lter [600/876] Loss: 1.6893\n",
      "Epoch [4/10], lter [605/876] Loss: 2.0692\n",
      "Epoch [4/10], lter [610/876] Loss: 2.0166\n",
      "Epoch [4/10], lter [615/876] Loss: 1.6625\n",
      "Epoch [4/10], lter [620/876] Loss: 1.5627\n",
      "Epoch [4/10], lter [625/876] Loss: 1.4161\n",
      "Epoch [4/10], lter [630/876] Loss: 1.6178\n",
      "Epoch [4/10], lter [635/876] Loss: 1.4526\n",
      "Epoch [4/10], lter [640/876] Loss: 1.6477\n",
      "Epoch [4/10], lter [645/876] Loss: 2.0085\n",
      "Epoch [4/10], lter [650/876] Loss: 1.3536\n",
      "Epoch [4/10], lter [655/876] Loss: 1.8676\n",
      "Epoch [4/10], lter [660/876] Loss: 1.7497\n",
      "Epoch [4/10], lter [665/876] Loss: 1.3998\n",
      "Epoch [4/10], lter [670/876] Loss: 1.4700\n",
      "Epoch [4/10], lter [675/876] Loss: 1.5807\n",
      "Epoch [4/10], lter [680/876] Loss: 1.8141\n",
      "Epoch [4/10], lter [685/876] Loss: 1.7119\n",
      "Epoch [4/10], lter [690/876] Loss: 1.6009\n",
      "Epoch [4/10], lter [695/876] Loss: 1.9296\n",
      "Epoch [4/10], lter [700/876] Loss: 1.9127\n",
      "Epoch [4/10], lter [705/876] Loss: 1.6741\n",
      "Epoch [4/10], lter [710/876] Loss: 1.5573\n",
      "Epoch [4/10], lter [715/876] Loss: 1.8668\n",
      "Epoch [4/10], lter [720/876] Loss: 1.6715\n",
      "Epoch [4/10], lter [725/876] Loss: 1.5620\n",
      "Epoch [4/10], lter [730/876] Loss: 1.5832\n",
      "Epoch [4/10], lter [735/876] Loss: 1.9611\n",
      "Epoch [4/10], lter [740/876] Loss: 1.6167\n",
      "Epoch [4/10], lter [745/876] Loss: 1.6833\n",
      "Epoch [4/10], lter [750/876] Loss: 1.5922\n",
      "Epoch [4/10], lter [755/876] Loss: 1.2404\n",
      "Epoch [4/10], lter [760/876] Loss: 1.5135\n",
      "Epoch [4/10], lter [765/876] Loss: 1.6726\n",
      "Epoch [4/10], lter [770/876] Loss: 1.5560\n",
      "Epoch [4/10], lter [775/876] Loss: 1.9144\n",
      "Epoch [4/10], lter [780/876] Loss: 1.6338\n",
      "Epoch [4/10], lter [785/876] Loss: 1.4588\n",
      "Epoch [4/10], lter [790/876] Loss: 2.0449\n",
      "Epoch [4/10], lter [795/876] Loss: 1.7152\n",
      "Epoch [4/10], lter [800/876] Loss: 1.5723\n",
      "Epoch [4/10], lter [805/876] Loss: 1.4804\n",
      "Epoch [4/10], lter [810/876] Loss: 1.7901\n",
      "Epoch [4/10], lter [815/876] Loss: 1.4758\n",
      "Epoch [4/10], lter [820/876] Loss: 1.8417\n",
      "Epoch [4/10], lter [825/876] Loss: 2.0643\n",
      "Epoch [4/10], lter [830/876] Loss: 1.6685\n",
      "Epoch [4/10], lter [835/876] Loss: 1.4824\n",
      "Epoch [4/10], lter [840/876] Loss: 1.6104\n",
      "Epoch [4/10], lter [845/876] Loss: 1.4857\n",
      "Epoch [4/10], lter [850/876] Loss: 1.5157\n",
      "Epoch [4/10], lter [855/876] Loss: 1.6774\n",
      "Epoch [4/10], lter [860/876] Loss: 1.7436\n",
      "Epoch [4/10], lter [865/876] Loss: 1.4595\n",
      "Epoch [4/10], lter [870/876] Loss: 1.5361\n",
      "Epoch [4/10], lter [875/876] Loss: 1.9734\n",
      "train Loss: 51.6408 | Acc: 12.4401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [17:52:05<26:48:58, 16089.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 47.6699 | Acc: 13.7244\n",
      "Epoch 4/9\n",
      "----------\n",
      "Epoch [5/10], lter [5/876] Loss: 1.8609\n",
      "Epoch [5/10], lter [10/876] Loss: 1.3374\n",
      "Epoch [5/10], lter [15/876] Loss: 1.3017\n",
      "Epoch [5/10], lter [20/876] Loss: 1.5059\n",
      "Epoch [5/10], lter [25/876] Loss: 1.6241\n",
      "Epoch [5/10], lter [30/876] Loss: 1.2874\n",
      "Epoch [5/10], lter [35/876] Loss: 1.4092\n",
      "Epoch [5/10], lter [40/876] Loss: 1.6149\n",
      "Epoch [5/10], lter [45/876] Loss: 1.9081\n",
      "Epoch [5/10], lter [50/876] Loss: 1.6423\n",
      "Epoch [5/10], lter [55/876] Loss: 1.5494\n",
      "Epoch [5/10], lter [60/876] Loss: 1.6035\n",
      "Epoch [5/10], lter [65/876] Loss: 1.4976\n",
      "Epoch [5/10], lter [70/876] Loss: 1.8183\n",
      "Epoch [5/10], lter [75/876] Loss: 1.7389\n",
      "Epoch [5/10], lter [80/876] Loss: 1.5943\n",
      "Epoch [5/10], lter [85/876] Loss: 1.6291\n",
      "Epoch [5/10], lter [90/876] Loss: 1.5410\n",
      "Epoch [5/10], lter [95/876] Loss: 1.4309\n",
      "Epoch [5/10], lter [100/876] Loss: 1.5424\n",
      "Epoch [5/10], lter [105/876] Loss: 1.6702\n",
      "Epoch [5/10], lter [110/876] Loss: 1.7711\n",
      "Epoch [5/10], lter [115/876] Loss: 1.4446\n",
      "Epoch [5/10], lter [120/876] Loss: 1.5612\n",
      "Epoch [5/10], lter [125/876] Loss: 1.8846\n",
      "Epoch [5/10], lter [130/876] Loss: 1.5574\n",
      "Epoch [5/10], lter [135/876] Loss: 1.6845\n",
      "Epoch [5/10], lter [140/876] Loss: 1.6694\n",
      "Epoch [5/10], lter [145/876] Loss: 1.5703\n",
      "Epoch [5/10], lter [150/876] Loss: 1.1807\n",
      "Epoch [5/10], lter [155/876] Loss: 1.7064\n",
      "Epoch [5/10], lter [160/876] Loss: 1.5004\n",
      "Epoch [5/10], lter [165/876] Loss: 1.5510\n",
      "Epoch [5/10], lter [170/876] Loss: 1.2651\n",
      "Epoch [5/10], lter [175/876] Loss: 1.7738\n",
      "Epoch [5/10], lter [180/876] Loss: 1.8898\n",
      "Epoch [5/10], lter [185/876] Loss: 1.3726\n",
      "Epoch [5/10], lter [190/876] Loss: 1.4874\n",
      "Epoch [5/10], lter [195/876] Loss: 1.6464\n",
      "Epoch [5/10], lter [200/876] Loss: 1.5675\n",
      "Epoch [5/10], lter [205/876] Loss: 1.9539\n",
      "Epoch [5/10], lter [210/876] Loss: 1.4401\n",
      "Epoch [5/10], lter [215/876] Loss: 1.4293\n",
      "Epoch [5/10], lter [220/876] Loss: 1.4011\n",
      "Epoch [5/10], lter [225/876] Loss: 1.6063\n",
      "Epoch [5/10], lter [230/876] Loss: 1.4258\n",
      "Epoch [5/10], lter [235/876] Loss: 1.6484\n",
      "Epoch [5/10], lter [240/876] Loss: 1.5619\n",
      "Epoch [5/10], lter [245/876] Loss: 1.6007\n",
      "Epoch [5/10], lter [250/876] Loss: 1.5656\n",
      "Epoch [5/10], lter [255/876] Loss: 1.6736\n",
      "Epoch [5/10], lter [260/876] Loss: 1.8629\n",
      "Epoch [5/10], lter [265/876] Loss: 1.5274\n",
      "Epoch [5/10], lter [270/876] Loss: 1.4652\n",
      "Epoch [5/10], lter [275/876] Loss: 1.3558\n",
      "Epoch [5/10], lter [280/876] Loss: 1.6143\n",
      "Epoch [5/10], lter [285/876] Loss: 1.2235\n",
      "Epoch [5/10], lter [290/876] Loss: 1.5029\n",
      "Epoch [5/10], lter [295/876] Loss: 1.3669\n",
      "Epoch [5/10], lter [300/876] Loss: 1.4439\n",
      "Epoch [5/10], lter [305/876] Loss: 1.5407\n",
      "Epoch [5/10], lter [310/876] Loss: 1.5482\n",
      "Epoch [5/10], lter [315/876] Loss: 1.7621\n",
      "Epoch [5/10], lter [320/876] Loss: 1.3285\n",
      "Epoch [5/10], lter [325/876] Loss: 1.4871\n",
      "Epoch [5/10], lter [330/876] Loss: 1.4505\n",
      "Epoch [5/10], lter [335/876] Loss: 1.5464\n",
      "Epoch [5/10], lter [340/876] Loss: 1.3332\n",
      "Epoch [5/10], lter [345/876] Loss: 1.3216\n",
      "Epoch [5/10], lter [350/876] Loss: 1.5708\n",
      "Epoch [5/10], lter [355/876] Loss: 1.5583\n",
      "Epoch [5/10], lter [360/876] Loss: 1.3667\n",
      "Epoch [5/10], lter [365/876] Loss: 1.6850\n",
      "Epoch [5/10], lter [370/876] Loss: 1.2570\n",
      "Epoch [5/10], lter [375/876] Loss: 1.6344\n",
      "Epoch [5/10], lter [380/876] Loss: 1.4911\n",
      "Epoch [5/10], lter [385/876] Loss: 1.7031\n",
      "Epoch [5/10], lter [390/876] Loss: 1.7150\n",
      "Epoch [5/10], lter [395/876] Loss: 1.5397\n",
      "Epoch [5/10], lter [400/876] Loss: 1.6323\n",
      "Epoch [5/10], lter [405/876] Loss: 1.3768\n",
      "Epoch [5/10], lter [410/876] Loss: 1.8770\n",
      "Epoch [5/10], lter [415/876] Loss: 1.6753\n",
      "Epoch [5/10], lter [420/876] Loss: 1.5629\n",
      "Epoch [5/10], lter [425/876] Loss: 1.7411\n",
      "Epoch [5/10], lter [430/876] Loss: 1.3427\n",
      "Epoch [5/10], lter [435/876] Loss: 1.7246\n",
      "Epoch [5/10], lter [440/876] Loss: 1.4624\n",
      "Epoch [5/10], lter [445/876] Loss: 1.5593\n",
      "Epoch [5/10], lter [450/876] Loss: 1.8199\n",
      "Epoch [5/10], lter [455/876] Loss: 1.7862\n",
      "Epoch [5/10], lter [460/876] Loss: 1.5647\n",
      "Epoch [5/10], lter [465/876] Loss: 1.8488\n",
      "Epoch [5/10], lter [470/876] Loss: 1.6797\n",
      "Epoch [5/10], lter [475/876] Loss: 1.4311\n",
      "Epoch [5/10], lter [480/876] Loss: 1.7067\n",
      "Epoch [5/10], lter [485/876] Loss: 1.9188\n",
      "Epoch [5/10], lter [490/876] Loss: 1.6332\n",
      "Epoch [5/10], lter [495/876] Loss: 1.6008\n",
      "Epoch [5/10], lter [500/876] Loss: 1.5780\n",
      "Epoch [5/10], lter [505/876] Loss: 1.6884\n",
      "Epoch [5/10], lter [510/876] Loss: 1.5075\n",
      "Epoch [5/10], lter [515/876] Loss: 1.6806\n",
      "Epoch [5/10], lter [520/876] Loss: 1.0380\n",
      "Epoch [5/10], lter [525/876] Loss: 1.5327\n",
      "Epoch [5/10], lter [530/876] Loss: 1.2303\n",
      "Epoch [5/10], lter [535/876] Loss: 1.7938\n",
      "Epoch [5/10], lter [540/876] Loss: 1.6777\n",
      "Epoch [5/10], lter [545/876] Loss: 1.5814\n",
      "Epoch [5/10], lter [550/876] Loss: 1.8002\n",
      "Epoch [5/10], lter [555/876] Loss: 1.5909\n",
      "Epoch [5/10], lter [560/876] Loss: 1.5238\n",
      "Epoch [5/10], lter [565/876] Loss: 1.2957\n",
      "Epoch [5/10], lter [570/876] Loss: 1.1554\n",
      "Epoch [5/10], lter [575/876] Loss: 1.2895\n",
      "Epoch [5/10], lter [580/876] Loss: 1.9928\n",
      "Epoch [5/10], lter [585/876] Loss: 1.4234\n",
      "Epoch [5/10], lter [590/876] Loss: 1.5879\n",
      "Epoch [5/10], lter [595/876] Loss: 1.8051\n",
      "Epoch [5/10], lter [600/876] Loss: 1.4497\n",
      "Epoch [5/10], lter [605/876] Loss: 1.3950\n",
      "Epoch [5/10], lter [610/876] Loss: 1.6609\n",
      "Epoch [5/10], lter [615/876] Loss: 1.5700\n",
      "Epoch [5/10], lter [620/876] Loss: 1.3440\n",
      "Epoch [5/10], lter [625/876] Loss: 1.6482\n",
      "Epoch [5/10], lter [630/876] Loss: 1.4179\n",
      "Epoch [5/10], lter [635/876] Loss: 1.5516\n",
      "Epoch [5/10], lter [640/876] Loss: 1.8960\n",
      "Epoch [5/10], lter [645/876] Loss: 1.6956\n",
      "Epoch [5/10], lter [650/876] Loss: 1.6630\n",
      "Epoch [5/10], lter [655/876] Loss: 1.7464\n",
      "Epoch [5/10], lter [660/876] Loss: 1.3836\n",
      "Epoch [5/10], lter [665/876] Loss: 1.6672\n",
      "Epoch [5/10], lter [670/876] Loss: 1.9843\n",
      "Epoch [5/10], lter [675/876] Loss: 1.5908\n",
      "Epoch [5/10], lter [680/876] Loss: 1.7323\n",
      "Epoch [5/10], lter [685/876] Loss: 1.8451\n",
      "Epoch [5/10], lter [690/876] Loss: 1.5716\n",
      "Epoch [5/10], lter [695/876] Loss: 1.7660\n",
      "Epoch [5/10], lter [700/876] Loss: 1.5214\n",
      "Epoch [5/10], lter [705/876] Loss: 1.8993\n",
      "Epoch [5/10], lter [710/876] Loss: 1.5482\n",
      "Epoch [5/10], lter [715/876] Loss: 1.8947\n",
      "Epoch [5/10], lter [720/876] Loss: 1.4952\n",
      "Epoch [5/10], lter [725/876] Loss: 1.6562\n",
      "Epoch [5/10], lter [730/876] Loss: 1.5605\n",
      "Epoch [5/10], lter [735/876] Loss: 1.3744\n",
      "Epoch [5/10], lter [740/876] Loss: 1.6151\n",
      "Epoch [5/10], lter [745/876] Loss: 1.7713\n",
      "Epoch [5/10], lter [750/876] Loss: 1.6240\n",
      "Epoch [5/10], lter [755/876] Loss: 1.4433\n",
      "Epoch [5/10], lter [760/876] Loss: 1.4415\n",
      "Epoch [5/10], lter [765/876] Loss: 1.7171\n",
      "Epoch [5/10], lter [770/876] Loss: 1.7102\n",
      "Epoch [5/10], lter [775/876] Loss: 1.5951\n",
      "Epoch [5/10], lter [780/876] Loss: 1.7112\n",
      "Epoch [5/10], lter [785/876] Loss: 1.2809\n",
      "Epoch [5/10], lter [790/876] Loss: 1.4820\n",
      "Epoch [5/10], lter [795/876] Loss: 1.7540\n",
      "Epoch [5/10], lter [800/876] Loss: 1.2742\n",
      "Epoch [5/10], lter [805/876] Loss: 1.1947\n",
      "Epoch [5/10], lter [810/876] Loss: 1.5018\n",
      "Epoch [5/10], lter [815/876] Loss: 1.2892\n",
      "Epoch [5/10], lter [820/876] Loss: 1.5959\n",
      "Epoch [5/10], lter [825/876] Loss: 1.6390\n",
      "Epoch [5/10], lter [830/876] Loss: 1.5503\n",
      "Epoch [5/10], lter [835/876] Loss: 1.8210\n",
      "Epoch [5/10], lter [840/876] Loss: 1.7272\n",
      "Epoch [5/10], lter [845/876] Loss: 1.6173\n",
      "Epoch [5/10], lter [850/876] Loss: 1.6968\n",
      "Epoch [5/10], lter [855/876] Loss: 1.6118\n",
      "Epoch [5/10], lter [860/876] Loss: 1.6692\n",
      "Epoch [5/10], lter [865/876] Loss: 1.4936\n",
      "Epoch [5/10], lter [870/876] Loss: 1.5189\n",
      "Epoch [5/10], lter [875/876] Loss: 1.3657\n",
      "train Loss: 51.1538 | Acc: 12.6283\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_data_loader.dataset.classes)\n",
    "vgg19.classifier[6] = nn.Linear(4096, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_vgg19 = optim.Adam(vgg19.parameters())\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch {}/{}'.format(epoch,num_epochs -1))\n",
    "    print('-'*10)\n",
    "    total_batch = len(train_data_loader.dataset) // 32\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            vgg19.train()\n",
    "        else:\n",
    "            vgg19.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        corrects = 0\n",
    "\n",
    "        for i,(inputs, labels) in enumerate(dataloaders[phase]):\n",
    "            optimizer_vgg19.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs_vgg19 = vgg19(inputs)\n",
    "                _, preds_vgg19 = torch.max(outputs_vgg19, 1)\n",
    "\n",
    "                loss_vgg19 = criterion(outputs_vgg19, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss_vgg19.backward()\n",
    "                    optimizer_vgg19.step()\n",
    "\n",
    "                    if (i+1) % 5 == 0:\n",
    "                        print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                             %(epoch+1, num_epochs, i+1, total_batch, loss_vgg19.item()))\n",
    "\n",
    "            running_loss += loss_vgg19.item() * inputs.size(0)\n",
    "            corrects += torch.sum(preds_vgg19 == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase])\n",
    "        epoch_acc = corrects.double() / len(dataloaders[phase])\n",
    "\n",
    "        print('{} Loss: {:.4f} | Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc68a84-0856-41d9-a5b2-eb83b376531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Loader:\n",
      "Batch Index: 0\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Unique Labels: tensor([0, 2, 3, 4, 5, 6])\n",
      "Batch Index: 1\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Unique Labels: tensor([0, 2, 3, 4, 5, 6])\n",
      "Batch Index: 2\n",
      "Inputs Shape: torch.Size([32, 3, 299, 299])\n",
      "Labels Shape: torch.Size([32])\n",
      "Unique Labels: tensor([0, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Confirm correct data load\n",
    "print(\"Train Data Loader:\")\n",
    "for batch_idx, (inputs, labels) in enumerate(train_data_loader):\n",
    "    print(\"Batch Index:\", batch_idx)\n",
    "    print(\"Inputs Shape:\", inputs.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    # Print the unique labels in the batch\n",
    "    print(\"Unique Labels:\", torch.unique(labels))\n",
    "    # Break after printing a few batches\n",
    "    if batch_idx == 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecce74-36b6-4062-be47-3f75d5606ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
